{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Team 15]   \n",
    "[Anu Yadav, Connor Stern, Erik Hou, Noah Pflaum]   \n",
    "Summer 2019, section [Hou, Stern, Yadav: Section 3; Pflaum: Section 5]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pyspark.ml.feature import FeatureHasher\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the variable to True if running spark locally, to False if running this notebook using week 08 environment\n",
    "local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if local:\n",
    "    # run if using local\n",
    "    app_name = \"final_project\"\n",
    "    master = \"local[*]\"\n",
    "    spark = SparkSession\\\n",
    "            .builder\\\n",
    "            .appName(app_name)\\\n",
    "            .master(master)\\\n",
    "            .getOrCreate()\n",
    "    # get the working directory\n",
    "    PWD = !pwd\n",
    "    PWD = PWD[0]\n",
    "    \n",
    "else:\n",
    "    # run if using wk08 environment\n",
    "    sqlContext = SQLContext(sc)\n",
    "    \n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '37483'),\n",
       " ('spark.app.name', 'final_project'),\n",
       " ('spark.app.id', 'local-1576099697938'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.host', 'docker.w261')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital advertising industry is booming. Every day, advertisers publish billions of ads to websites and other online platforms where potential customers can view and click on them. For example, Nike as an advertiser might post an ad for running shoes on the ESPN.com website or in a Google search for shoes. The position of the ad, the audience, and the time that the ad is displayed may all significantly influence a user's decision to click on it or not. \n",
    "\n",
    "Ad posts generate vast amount of user log data representing ad placement and whether or not a user clicked on an ad. This data is a treasure trove for companies seeking to more effectively target their ads to specific consumers. As such 'clickthrough rate' prediction (determining whether or not a customer will click on an ad) has become a core task in the field of digital advertising, and multiple clickthrough rate prediction challenges have been launched in recent years to build and continuously improve models that can predict given the features of the ad, whether or not a user will click on the ad. \n",
    "\n",
    "In this project, we seek to build our own homegrown model to complete such a task, inspired by previous successful models. The primary goal is to beat a baseline model defined by measuring the overall click-through rate (CTR, percent of ads clicked). A secondary goal is to match the model performance of competition-winning models.\n",
    "\n",
    "\n",
    "\n",
    "We therefore begin with the question:\n",
    "\n",
    "-  What are some common approaches to CTR prediction? Which have been the most successful in previous competitions?\n",
    "\n",
    "-  What metric is appropriate to measure model performance?\n",
    "\n",
    "    -  After some initial research, we find that CTR prediction is typically treated as a logistic regression problem. However, there are many different methods *within* a logistic regression framework that the problem can be handled. One such method is that of a Field-Aware Factorization Machine (FFM), which was the winning model for several of the previously mentioned CTR prediction competitions.\n",
    "    -  For several reasons, we consider log-loss to be the most appropriate performance metric in comparison to other metrics, such as accuracy. \n",
    "        -  When making inferences, we output a probability (as opposed to a label) for each input. Accuracy is a metric for predicted labels, while log-loss is a metric for predicted probabilities.\n",
    "        - Our training data is unbalanced, with 75% of our responses being non-clicks. As a result, we can get an accuracy of 75% for free (using a base-line of always predicting non-clicks) if accuracy is used as a metric. Log-loss is a better metric for handling imbalanced training data.\n",
    "    \n",
    "Inspired by the success of Juan, Zhuang, Chin, and Lin (the authors of several papers on FFMs and winners of the CTR challenges), we decide to implement a FFM model of our own. This leads us to several additional questions we seek to answer:\n",
    "\n",
    "-  What resource requirements are there for implementing an FFM?\n",
    "\n",
    "-  Can the FFM model be implemented in Pyspark using distributed computing using map-reduce patterns? What modifications (if any) need to be made to the algorithm/implementation outlined by Juan, Zhuang, Chin, and Lin for an FFM model to work in our framework?  \n",
    "\n",
    "- What level of performance can we expect with our implementation?\n",
    "\n",
    "    -  Juan, Zhuang, Chin, & Lin have built their own custom library called 'LibFFM'. Their papers outline an approach using the Hogwild algorithm that allows for concurrent updates to a sparse matrix of model coefficients, which eliminates some of the issues with the communication and synchronization barriers that generally exist when using stochastic gradient descent. Our goal is to explore how this algorithm might be implemented using the techniques we learned in this class (such as Pyspark and distributed computing using map-reduce patterns, as mentioned above). We do not expect our algorithm to be as performant as a customized C++ implmentation; instead, we highlight the application of the ideas learned in this class for creating predicitve models for big data.\n",
    "    \n",
    "To build our model, we use real-world training data provided for a clickthrough rate competion on Kaggle. The data can be found at:\n",
    "\n",
    "http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/\n",
    "\n",
    "From the `readme.txt` file, we have the following description of the data:\n",
    "\n",
    "> \"The training dataset consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo and the first column is indicates whether this ad has been clicked or not. The positive (clicked) and negatives (non-clicked) examples have both been subsampled (but at different rates) in order to reduce the dataset size. There are 13 features taking integer values (mostly count features) and 26 categorical features. The values of the categorical features have been hashed onto 32 bits for anonymization purposes. The semantic of these features is undisclosed. Some features may have missing values. The rows are chronologically ordered.\"\n",
    "\n",
    "As we build our own FFM model, we will likely need to follow the suggestion and take samples of this large real-world dataset. In this way we can ensure our algorithm is working correctly before we attempt to scale our model to run on the full dataset.\n",
    "\n",
    "We will next dive more deeply into our choice of implementing a FFM model, and a detailed description of the algorithm, complete with a toy example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because an online advertisement can either be clicked ($response = 1$) or not ($response = -1$) , Click-Through-Rate (CTR) Prediction is generally treated as a logistic regression problem. For any set of features, we calculate some value $s$, and perform the logit transformation to yield our CTR prediction:\n",
    "$$CTR = \\frac{1}{1+e^{-s}} $$\n",
    "\n",
    "The calculated CTR represents our predicted probability for a success($response=-1$). Therefore, we desire CTRs close to 1 of positive responses and CTRs close to 0 for negative responses. There are several methods we can use to estimate $s$, each with its own benefits and drawbacks. Typical implementations include a linear model, a degree-2 polynomial mapping, a factorization machine, and a field-aware factorization machine. \n",
    "\n",
    "We will consider an example with the following dataset as we discuss the different methods of estimating $s$:\n",
    "\n",
    "<table>\n",
    "<th>Response</th>\n",
    "<th>Publisher</th>\n",
    "<th>Advertiser</th>\n",
    "<th>Gender</th>\n",
    "<tr><td>1</td><td>Netflix</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>-1</td><td>Spotify</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>-1</td><td>Facebook</td><td>Gatorade</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Spotify</td><td>Coca-cola</td><td>Male</td></tr>\n",
    "<tr><td>1</td><td>Facebook</td><td>Coca-cola</td><td>Female</td></tr>\n",
    "<tr><td>-1</td><td>Facebook</td><td>Pepsi</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Netflix</td><td>Gatorade</td><td>Female</td></tr>\n",
    "</table>\n",
    "\n",
    "In this dataset, we refer to the categories Publisher, Advertiser, and Gender as \"fields\" and the labels within each field (Netflix, Spotify, Facebook, Pepsi, Gatorade, Coca-cola, Male, Female) as \"features.\"\n",
    "\n",
    "#### Linear Model\n",
    "In a linear model, the algorithm learns a weight for every given feature. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) =\\textbf{w}^T \\textbf{x}  =\\sum_{j \\epsilon C_1}w_jx_j$$\n",
    "where $\\textbf{w}$ is the learned model, $\\textbf{x}$ is the data observation, and $C_1$ is the non-zero elements in $\\textbf{x}$. \n",
    "\n",
    "In our toy example, our model would learn different weights for the different Publishers (Netflix, Spotify, and Pepsi), Advertisers (Pepsi, Gatorade, and Coca-cola), and Genders (Male, Female). The value $s$ would then be calculated for each impression using these different weights. Thus, for each impression we would have have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s =& w_{Netflix}\\cdot x_{Netflix} + w_{Spotify}\\cdot x_{Spotify} + w_{Facebook}\\cdot x_{Facebook} \\\\\n",
    "&+ w_{Coca-cola}\\cdot x_{Coca-cola} + w_{Pepsi}\\cdot x_{Pepsi} + w_{Gatorade}\\cdot x_{Gatorade} \\\\\n",
    "&+ + w_{Male}\\cdot x_{Male} + w_{Female}\\cdot x_{Female}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For our first impression of the dataset (Netflix, Pepsi, Male) this becomes:\n",
    "$$s = w_{Netflix} + w_{Coca-cola} + w_{Male}$$\n",
    "since $x_j = 1$ for Netflix, Pepsi, and Male while $x_j = 0$ for all other features. \n",
    "\n",
    "\n",
    "This model is simple and efficient, yet it does not allow for interactive effects between features. For example, Coca-cola may have a higher CTR with Netflix than another publisher. A linear model is unable to learn this type of information, as it essentially learns the \"average effect\" of each feature.\n",
    "\n",
    "#### Degree-2 Polynomial Mapping\n",
    "The simplest way to learn the effect of the \"feature conjunction\" described above (in the case where a particular advertiser may have a higher CTR with one publisher compared to others) is to use a degree-2 polynomial mapping. In this model, the algorithm learns an additional weight for each feature pair. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} w_{j_1,j_2} \\cdot x_{j_1}x_{j_2}$$\n",
    "where $C_2$ is the pairwise combination of non-zero elements in $\\textbf{x}$.\n",
    "\n",
    "\n",
    "Returning to our example dataset and the impression with the features Netflix, Pepsi, and Male the model would be:\n",
    "$$s = w_{Netflix} + w_{Pepsi} + w_{Male} + w_{Netflix,Pepsi} + w_{Netflix,Male} + w_{Pepsi,Male}$$\n",
    "\n",
    "While this model improves on the linear model by allowing us to account for interactions between features, it does not handle sparse datasets well. Since we have 0 impressions of the advertiser Gatorade with the publisher Spotify, the model prediction will be trivial as no weight was learned for this feature combination. The model is also susceptible to overfitting, as it generates unreliable predictions for feature combinations with a very small number of impressions.\n",
    "\n",
    "#### Factorization Machine\n",
    "Factorization Machines (FM) begin to provide us a solution to the problem of sparse datasets. Here the model learns a latent vector, rather than an explicit weight, for each feature. The model formulation is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1}, \\textbf{w}_{j_2} \\rangle x_{j_1}x_{j_2}$$\n",
    "where $\\textbf{w}_{j_1}$ and $\\textbf{w}_{j_2}$ are two learned latent vectors of length $k$ (some user-defined parameter).\n",
    "\n",
    "Returning to our example of an impression with the features of Netflix, Pepsi, and Male, the FM model would be:\n",
    "$$s = w_{Netflix} \\cdot w_{Pepsi} + w_{Netflix} \\cdot w_{Male} + w_{Pepsi} \\cdot w_{Male}$$\n",
    "where $w_{Netflix}, w_{Pepsi}, w_{Male} \\epsilon {\\rm I\\!R}^k$.\n",
    "\n",
    "This allows the model to learn the latent vectors for each feature based on all the data points for that feature, and these latent vectors can be used to predict the CTR for unobserved feature combinations (such as Spotify and Gatorade, as previously mentioned), something the degree-2 polynomial mapping method did not allow us to do.\n",
    "\n",
    "We notice in our example above, however, that the latent vector $w_{Netflix}$ is used twice: once to calculate the latent effect of the Publisher Netflix with the Advertiser Pepsi ($w_{Netflix} \\cdot w_{Pepsi}$), and once to calculate the latent effect of the Publisher Netflix with the Gender Male ($w_{Netflix} \\cdot w_{Male}$). Yet the latent effect for publisher with advertiser could be very different from the latent effect for publisher with gender, and as such the Factorization Machine model is too restrictive and unrealistic.\n",
    "\n",
    "\n",
    "#### Field-Aware Factorization Machine\n",
    "Field-Aware Factorization Machines (FFM) provide a solution to this problem by introducing the flexibility to learn multiple latent vectors for each feature. In the context of the above example, the FFM model will learn two separate latent vectors for Netflix: $w_{Netflix, A}$, which will be used for the calculation of $P \\times A$ (to learn the latent effect of Netflix with a given advertiser), and $w_{Netflix, G}$. $w_{Netflix, A}$, for the calculation of $P \\times G$ (to learn the latent effect of Netflix with a given gender).\n",
    "\n",
    "Specifically, the FFM model formulation with the (Netflix, Pepsi, Male) impression gives us:\n",
    "\n",
    "$$s = w_{Netflix, A} \\cdot w_{Pepsi, P} + w_{Netflix, G} \\cdot w_{Male, P} + w_{Pepsi, G} \\cdot w_{Male, A}$$\n",
    "\n",
    "Now, the latent effect of (Netflix, Pepsi), is learned by using the latent vector $w_{Netflix, A}$, since Pepsi belongs to the advertiser field (A). The latent effect of (Netflix, Male), by contrast, is learned using a different latent vector, $w_{Netflix, G}$, since Male belongs to the gender field (G). In this way, FFM splits the latent factors for $P \\times A$ and $P \\times G$, something the traditional Factorization Machine is unable to do. \n",
    "\n",
    "The full model formulation for a Field-Aware Factorization Machine is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$\n",
    "\n",
    "where $f_1$ and $f_2$ are the fields of $j_1$ and $j_2$, respectively. \n",
    "#### Optimization\n",
    "\n",
    "As part of a logistic regression problem, the goal is to find the set of parameters that minimize the log-loss function, defined by:\n",
    "\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i}) + (1-y_i) \\cdot log_e(1-\\hat{y_i})] $$\n",
    "\n",
    "\n",
    "where $n$ is the number of impressions, $y_i$ is the true CTR of impression $i$, and $\\hat{y_i}$ is the predicted CTR of impression $i$, $\\frac{1}{1+e^{-s_i}}$.\n",
    "\n",
    "For a given impression $x_i$, we have two cases:\n",
    "* If the true response, $y_i = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss &= -log_e(\\hat{y_i}) \\\\\n",
    "&= -log_e \\left( \\frac{1}{1+e^{-s}} \\right) \\\\\n",
    "&= log_e (1+e^{-s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* If the true response, $y_i = -1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss &= -log_e(1-\\hat{y_i}) \\\\\n",
    "&= -log_e \\left( 1 - \\frac{1}{1+e^{-s}} \\right)\\\\\n",
    "&= -log_e \\left( \\frac{1 + e^{-s} - 1}{1+e^{-s}} \\right) \\\\\n",
    "&= -log_e \\left( \\frac{e^-s}{1+e^{-s}} \\right) \\\\\n",
    "&= -log_e \\left( \\frac{1}{1+e^{s}} \\right) \\\\\n",
    "&= log_e(1+e^{s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As such, the loss for impression $x_i$ can be written as:\n",
    "$$log(1+exp(-y_i\\cdot s_i)$$\n",
    "\n",
    "Therefore, we can rewrite the log-loss function as:\n",
    "$$ Loss = \\frac{1}{n} \\sum_{i=1}^n log(1+exp(-y_i\\cdot s_i))$$\n",
    "\n",
    "\n",
    "and $s = \\phi(\\textbf{w},\\textbf{x}_i) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$\n",
    "\n",
    "\n",
    "Introducing regularization, the optimization problem we have is:\n",
    "\n",
    "$$ \\min_{\\textbf{w}} \\sum_{i=1}^m \\left( log(1+exp(-y_i\\phi(\\textbf{w},\\textbf{x}_i)) + \\frac{\\lambda}{2}\\|\\textbf{w}\\|^2 \\right)$$\n",
    "\n",
    "$m$ is the number of impressions, and  $\\lambda$ is our regularization parameter.\n",
    "\n",
    "We solve this optimization problem using gradient descent methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example: FFM in action\n",
    "\n",
    "We will now walk through an entire iteration of the FFM algorithm to learn a CTR prediction model.\n",
    "For our work here, we will utilize a hashed representation of our dataset. In this representation, each field is represented by the first integer in the hash, and the feature within that field is represented by the second integer. The Publisher field is represented by the integer 1, the Advertiser field by the integer 2, and the Gender field by the integer 3. Within the Publisher field, Netflix is represented by the integer 1, Spotify by 2, and Facebook by 3. Within the Advertiser field, Pepsi is represented by the integer 1, Gatorade by 2, and Coca-cola by 3. Within the Gender field, Male is represented by 1 and Female by 2. Using this hash representation, our first impression of (Netflix, Pepsi, Male) has the features 1:1, 2:1, 3:1.\n",
    "The full hashed representation of our dataset is shown below:\n",
    "\n",
    "<table>\n",
    "<th>Impression</th>\n",
    "<th>Response</th>\n",
    "<th>Features</th>\n",
    "<tr><td>1</td><td>1</td><td>1:1, 2:1, 3:1</td></tr>\n",
    "<tr><td>2</td><td>1</td><td>1:2, 2:1, 3:1</td></tr>\n",
    "<tr><td>3</td><td>-1</td><td>1:3, 2:2, 3:2</td></tr>\n",
    "<tr><td>4</td><td>-1</td><td>1:2, 2:3, 3:1</td></tr>\n",
    "<tr><td>5</td><td>1</td><td>1:3, 2:3, 3:2</td></tr>\n",
    "<tr><td>6</td><td>-1</td><td>1:3, 2:1, 3:2</td></tr>\n",
    "<tr><td>7</td><td>1</td><td>1:1, 2:2, 3:2</td></tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/toy_set.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/toy_set.txt\n",
    "1\t1\t1\t1\t1\n",
    "2\t1\t2\t1\t1\n",
    "3\t-1\t3\t2\t2\n",
    "4\t-1\t2\t3\t1\n",
    "5\t1\t3\t3\t2\n",
    "6\t-1\t3\t1\t2\n",
    "7\t1\t1\t2\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD = sc.textFile('data/toy_set.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ We define k (the length of our latent vectors) to be 3.\n",
    "\n",
    "__2)__ We randomly initialize our latent vectors $\\textbf{w}$. For the Publisher field, each feature will have two different latent vectors (represented by columns below)--one corresponding to Advertiser (A), and one corresponding to Gender (G):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9, -0.2, -0.2],\n",
       "       [-0.6, -0. , -0.3],\n",
       "       [-0.1, -0.5, -1.2],\n",
       "       [-1.1,  0.2,  1.2],\n",
       "       [ 1.4, -0.7,  0.7],\n",
       "       [ 0. ,  0.8,  0.1]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_P = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "#uncomment the line below to use weights that match all subsequent hand calculations/tables\n",
    "#W_P =np.array([0.9, -0.2, -0.2, -0.6, 0, -0.3, -0.1, -0.5, -1.2, -1.1, 0.2, 1.2, 1.4,-0.7, 0.7, 0, 0.8, 0.1]).reshape(6,3)\n",
    "W_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Netflix,A}$</th>\n",
    "<th>$\\textbf{w}_{Netflix,G}$</th>\n",
    "<th>$\\textbf{w}_{Spotify,A}$</th>\n",
    "<th>$\\textbf{w}_{Spotify,G}$</th>\n",
    "<th>$\\textbf{w}_{Facebook,A}$</th>\n",
    "<th>$\\textbf{w}_{Facebook,G}$</th>\n",
    "<tr><td>0.9</td><td>-0.6</td><td>-0.1</td><td>-1.1</td><td>1.4</td><td>0</td></tr>\n",
    "<tr><td>-0.2</td><td>0</td><td>-0.5</td><td>0.2</td><td>-0.7</td><td>0.8</td></tr>\n",
    "<tr><td>-0.2</td><td>-0.3</td><td>-1.2</td><td>1.2</td><td>0.7</td><td>0.1</td></tr>\n",
    "<table>\n",
    "The Advertiser field will similarly have two different latent vectors for each feature--one for Publisher (P), and one for Gender (G):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.3, -0.1],\n",
       "       [-0.4, -1. , -0. ],\n",
       "       [-0.2, -1. , -1.7],\n",
       "       [ 0.7, -0.3, -1.1],\n",
       "       [ 0.1,  1.3, -0.2],\n",
       "       [ 0.5,  0.2,  0.5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_A = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "#uncomment the line below to use weights that match all subsequent hand calculations/tables\n",
    "#W_A =np.array([1, -0.3, -0.1, -0.4, -1, 0, -0.2, -1, -1.7, 0.7, -0.3, -1.1, 0.1, 1.3, -0.2, 0.5, 0.2, 0.5]).reshape(6,3)\n",
    "W_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Pepsi,P}$</th>\n",
    "<th>$\\textbf{w}_{Pepsi,G}$</th>\n",
    "<th>$\\textbf{w}_{Gatorade,P}$</th>\n",
    "<th>$\\textbf{w}_{Gatorade,G}$</th>\n",
    "<th>$\\textbf{w}_{Coca-cola,P}$</th>\n",
    "<th>$\\textbf{w}_{Coca-cola,G}$</th>\n",
    "<tr><td>1</td><td>-0.4</td><td>-0.2</td><td>0.7</td><td>0.1</td><td>0.5</td></tr>\n",
    "<tr><td>-0.3</td><td>-1</td><td>-1</td><td>-0.3</td><td>-1.3</td><td>0.2</td></tr>\n",
    "<tr><td>-0.1</td><td>0</td><td>-1.7</td><td>-1.1</td><td>-0.2</td><td>0.5</td></tr>\n",
    "<table>\n",
    "\n",
    "The Gender field will also have two different latent vectors per feature (one for Publisher (P), one for Advertiser (A)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4,  0.8,  1.5],\n",
       "       [-0.9,  1.3, -1. ],\n",
       "       [ 0.8, -1.5,  0.3],\n",
       "       [ 0.4,  0.7,  0.5]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_G = np.random.uniform(-1.7,1.7, size=(4,3)).round(1)\n",
    "#uncomment the line below to use weights that match all subsequent hand calculations/tables\n",
    "#W_G =np.array([-0.4, 0.8, 1.5, -0.9, 1.3, -1, 0.8, -1.5, 0.3, 0.4, 0.7, 0.5]).reshape(4,3)\n",
    "W_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Male,P}$</th>\n",
    "<th>$\\textbf{w}_{Male,A}$</th>\n",
    "<th>$\\textbf{w}_{Female,P}$</th>\n",
    "<th>$\\textbf{w}_{Female,A}$</th>\n",
    "<tr><td>-0.4</td><td>-0.9</td><td>0.8</td><td>0.4</td></tr>\n",
    "<tr><td>0.8</td><td>1.3</td><td>-1.5</td><td>0.7</td></tr>\n",
    "<tr><td>1.5</td><td>-1</td><td>0.3</td><td>0.5</td></tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3)__ We calculate $s$ for each impression using these weight vectors.\n",
    "\n",
    "For each impression, we would use the formula\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$    \n",
    "to generate our value of $s$. Fully expanded, this would be:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "s =& \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Netflix}x_{Pepsi} + \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Netflix}x_{Coca-cola} + \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Netflix}x_{Gatorade} \\\\\n",
    "&+ \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Netflix}x_{Male} + \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Netflix}x_{Female} + \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Spotify}x_{Pepsi} \\\\\n",
    "&+ \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Spotify}x_{Coca-cola} + \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Spotify}x_{Gatorade} + \\langle\\textbf{w}_{Spotify,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Spotify}x_{Male} \\\\\n",
    "&+ \\langle\\textbf{w}_{Spotify,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Spotify}x_{Female} + \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Facebook}x_{Pepsi} + \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Facebook}x_{Coca-cola} \\\\\n",
    "&+ \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Facebook}x_{Gatorade} + \\langle\\textbf{w}_{Facebook,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Facebook}x_{Male} + \\langle\\textbf{w}_{Facebook,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Facebook}x_{Female} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For each impression, we are only concerned with the terms containing the features represented in the impression, since these $x$ values are equal to one, while all other terms become zero.\n",
    "    \n",
    "Our first impression (Netflix, Pepsi, Male) would therefore give us:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s &= \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle + \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Male,P}\\rangle + \\langle\\textbf{w}_{Pepsi,G} \\cdot \\textbf{w}_{Male,A}\\rangle\\\\\n",
    "&= (0.9*1 + -0.2*-0.3 + -0.2*-0.1) + (-0.6*-0.4 + 0*0.8 + -0.3*1.5) + (-0.4*-0.9 + -1*1.3 + 0*-1)  \\\\\n",
    "&= 0.98 + -0.21 + -0.94 \\\\\n",
    "&= -0.17\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Plugging this value into the logit transformation, we get a predicted CTR of:\n",
    "$$\\frac{1}{1+e^{-s}} = \\frac{1}{1+e^{0.17}} \\approx 0.46$$\n",
    "    \n",
    "Since this value is closer to 0 than 1, we would ultimately predict a non-click (incorrectly). However, this predicted value is vital for our log-loss calculation, which we will see momentarily.\n",
    "\n",
    "Below we calculate the value of $s$ and predicted CTR for each impression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_p = sc.broadcast(W_P)\n",
    "W_a = sc.broadcast(W_A)\n",
    "W_g = sc.broadcast(W_G)\n",
    "\n",
    "def predict(x):\n",
    "    imp = x[0]\n",
    "    res = x[2]\n",
    "    pub = x[4]\n",
    "    adv = x[6]\n",
    "    gen = x[8]\n",
    "    #define appropriate latent vectors to use in calculations\n",
    "    if pub == '1':\n",
    "        w_pa = W_p.value[0]\n",
    "        w_pg = W_p.value[1]\n",
    "    elif pub == '2':\n",
    "        w_pa = W_p.value[2]\n",
    "        w_pg = W_p.value[3]\n",
    "    else:\n",
    "        w_pa = W_p.value[4]\n",
    "        w_pg = W_p.value[5]\n",
    "    if adv == '1':\n",
    "        w_ap = W_a.value[0]\n",
    "        w_ag = W_a.value[1]\n",
    "    elif adv == '2':\n",
    "        w_ap = W_a.value[2]\n",
    "        w_ag = W_a.value[3]\n",
    "    else:\n",
    "        w_ap = W_a.value[4]\n",
    "        w_ag = W_a.value[5]\n",
    "    if gen == '1':\n",
    "        w_gp = W_g.value[0]\n",
    "        w_ga = W_g.value[1]\n",
    "    else:\n",
    "        w_gp = W_g.value[2]\n",
    "        w_ga = W_g.value[3]\n",
    "    #calculate s using latent vectors\n",
    "    s = (np.dot(w_pa, w_ap) + np.dot(w_pg, w_gp) + np.dot(w_ag, w_ga)).round(2)\n",
    "    #calculate CTR\n",
    "    CTR = np.divide(1,1+np.exp(-s)).round(2)\n",
    "    yield imp,res, pub, adv, gen, s, CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '1', '1', '1', -0.17, 0.46),\n",
       " ('2', '1', '2', '1', '1', 1.63, 0.84),\n",
       " ('3', '-1', '3', '2', '2', -2.42, 0.08),\n",
       " ('4', '-1', '2', '3', '1', 1.29, 0.78),\n",
       " ('5', '1', '3', '3', '2', -1.49, 0.18),\n",
       " ('6', '-1', '3', '1', '2', -0.49, 0.38),\n",
       " ('7', '1', '1', '2', '2', -0.69, 0.33)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD = toyRDD.flatMap(predict).cache()\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarized in a table, we have:\n",
    "\n",
    "<table>\n",
    "<th>Impression</th>\n",
    "<th>$s$</th>\n",
    "<th>Predicted CTR</th>\n",
    "<th>True Response</th>\n",
    "<tr><td>(Netflix, Pepsi, Male)</td><td>-0.17</td><td>$\\approx 0.46$</td><td>1</td></tr>\n",
    "<tr><td>(Spotify, Pepsi, Male)</td><td>1.63</td><td>$\\approx 0.84$</td><td>1</td></tr>\n",
    "<tr><td>(Facebook, Gatoriade, Female)</td><td>-2.42</td><td>$\\approx 0.08$</td><td>-1</td></tr>\n",
    "<tr><td>(Spotify, Coca-cola, Male)</td><td>1.29</td><td>$\\approx 0.78$</td><td>-1</td><td></tr>\n",
    "<tr><td>(Facebook, Coca-cola, Female)</td><td>-1.49</td><td>$\\approx 0.18$</td><td>1</td></tr>\n",
    "<tr><td>(Facebook, Pepsi, Female)</td><td>-0.49</td><td>$\\approx 0.38$</td><td>-1</td></tr>\n",
    "<tr><td>(Netflix, Gatorade, Female)</td><td>-0.69</td><td>$\\approx 0.33$</td><td>1</td></tr>\n",
    "<table>\n",
    "\n",
    "    \n",
    "__4)__ We can now calculate the log-loss of the model. While this is not necessary in order to find the optimum latent vectors for our model, we show the calculation here using the equation:\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i}) + (1-y_i) \\cdot log_e(1-\\hat{y_i})] $$\n",
    "\n",
    "Simply put, if our True Response ($y_i$) for an impression is -1, we add the log-value of `1-Predicted CTR` to our sum; if our True Response ($y_i$) for an impression is 1, we add the log-value of `CTR`.    \n",
    "\n",
    "Using our values from our table above, we have:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "LogLoss =& - \\frac{1}{7}[log_e(0.46) + log_e(0.84) + log_e(0.92) + log_e(0.22) + log_e(0.18) + log_e(0.62)+ log_e(0.33)]\\\\\n",
    "=& 0.8356\n",
    "\\end{aligned}\n",
    "$$\n",
    "    \n",
    "Our hand calculation is confirmed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8356983388241626]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_loss(imp):\n",
    "    #helper function to calculate log(CTR) or log(1-CTR) depending on true response\n",
    "    resp = imp[1]\n",
    "    pred = imp[6]\n",
    "    if resp == '-1':\n",
    "        yield 1, (np.log(1-pred), 1)\n",
    "    else:\n",
    "        yield 1, (np.log(pred), 1)\n",
    "\n",
    "def sum_loss(x, y):\n",
    "    #function to add log-losses, counts\n",
    "    return (x[0]+y[0],x[1]+y[1])\n",
    "\n",
    "loss = resultRDD.flatMap(log_loss).reduceByKey(lambda x, y: sum_loss(x, y))\\\n",
    "    .map(lambda x: -np.divide(x[1][0], x[1][1])).collect()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5)__ We then would optimize our weights using gradient descent. In their implementation outlined in their paper, Juan, Zhuang, Chin, & Lin update the weight matrix of latent vectors using stochastic gradient descent. We will replicate this method with our toy sample here.\n",
    "\n",
    "At each step, we choose a data point and update $\\textbf{w}_{j_1,f_2}$ and $\\textbf{w}_{j_2, f_1}$ for each non-zero pair of features $f_1 \\epsilon j_1$ and $f_2 \\epsilon j_2$. We first calculate the subgradients: \n",
    "$$\\textbf{g}_{j_1,f_2} = \\nabla \\textbf{w}_{j_1,f_2} f(\\textbf{w}) = \\lambda \\cdot \\textbf{w}_{j_1,f_2} + \\kappa \\cdot \\textbf{w}_{j_2,f_1}x_{j_1}x_{j_2} $$\n",
    "$$\\textbf{g}_{j_2,f_1} = \\nabla \\textbf{w}_{j_2,f_1} f(\\textbf{w}) = \\lambda \\cdot \\textbf{w}_{j_2,f_1} + \\kappa \\cdot \\textbf{w}_{j_1,f_2}x_{j_1}x_{j_2} $$\n",
    "where $$\\kappa = \\frac{-y}{1+exp(y\\phi(\\textbf{wx}))}$$\n",
    "\n",
    "For simplicity here, we ignore the regularization term and consider the first datapoint- the impression of (Netflix, Pepsi, Male). We calculate $\\kappa = -0.54$, and calculate the first pair of subgradients:\n",
    "    $$\\textbf{g}_{Pepsi, P} = \\kappa\\textbf{w}_{Netflix, A}$$ \n",
    "    $$\\textbf{g}_{Netflix, A} = \\kappa\\textbf{w}_{Pepsi, P}$$\n",
    "    \n",
    "(Note: below we only continue to demonstrate how to update the latent vector $\\textbf{w}_{Pepsi,P}$. Updating all other latent vectors is analagous.)\n",
    "    \n",
    "Plugging in our known and calculated values, we have:\n",
    "    \n",
    "$$\\textbf{g}_{Pepsi, P}= -0.54 \\times \\begin{bmatrix}0.9\\\\-0.2\\\\-0.2 \\end{bmatrix} = \\begin{bmatrix}-0.488\\\\0.108\\\\0.108 \\end{bmatrix} $$\n",
    "\n",
    "Next, for each coordinate $d = {1,...,k}$, the sum of squared gradient is accumulated:\n",
    "$$ (G_{j_1,f_2})_d \\leftarrow (G_{j_1,f_2})_d + (\\textbf{g}_{j_1,f_2})_d^2$$ where $(G_{j_1,f_2})_d$ are initialized as all 1s. Thus, we have:\n",
    "$$ (G_{Pepsi,P}) = \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix} + \\begin{bmatrix}(-0.488)^2\\\\(0.108)^2\\\\(0.108)^2\\end{bmatrix} = \\begin{bmatrix}1.2383\\\\1.0117\\\\1.0117\\end{bmatrix}  $$\n",
    "    \n",
    "Finally, we update our latent vector $\\textbf{w}_{j_1,f_2}$ by:\n",
    "    $$(\\textbf{w}_{j_1,f_2})_d \\leftarrow (\\textbf{w}_{j_1,f_2})_d - \\frac{\\eta}{\\sqrt{(G_{j_1,f_2})_d}} \\textbf{g}_{j_1,f_2} $$\n",
    "where $\\eta$ is a specified learning rate. For this example, we use $\\eta = 0.1$, and get the following update to our latent vector, $\\textbf{w}_{Pepsi,P}$:\n",
    "\n",
    "$$\\textbf{w}_{Pepsi,P} = \\begin{bmatrix}1\\\\-0.3\\\\-0.1 \\end{bmatrix} - \\begin{bmatrix}\\frac{0.1}{\\sqrt{1.2383}} \\cdot -0.488\\\\\\frac{0.1}{\\sqrt{1.0117}} \\cdot 0.108\\\\\\frac{0.1}{\\sqrt{1.0117}} \\cdot 0.108 \\end{bmatrix} = \\begin{bmatrix}1.04\\\\-0.31\\\\-0.11 \\end{bmatrix}$$\n",
    "\n",
    "Using the same impression, we similarly update the latent vectors $\\textbf{w}_{Netflix,A}$, $\\textbf{w}_{Netflix,G}$, $\\textbf{w}_{Male,P}$, $\\textbf{w}_{Male,A}$, and $\\textbf{w}_{Pepsi,G}$.\n",
    "These newly updated latent vectors are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  '1',\n",
       "  '1',\n",
       "  array([ 0.94767801, -0.2160607 , -0.20541602]),\n",
       "  array([-0.62120264,  0.03980592, -0.2368895 ]),\n",
       "  array([ 1.04386801, -0.31078469, -0.11078469]),\n",
       "  array([-0.44386801, -0.94237342, -0.04767801]),\n",
       "  array([-0.43094634,  0.8       ,  1.4839393 ]),\n",
       "  array([-0.92120264,  1.25232199, -1.        ]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kappa(x):\n",
    "    #function to calculate kappa value based on true response and calculated s value\n",
    "    y=int(x[1])\n",
    "    return -np.divide(y,1 + np.exp(y*x[5])) #kappa calculation\n",
    "\n",
    "def gradient(x):\n",
    "    #function to calculate gradient update for latent vectors of an impression\n",
    "    #need response variables to be +1 or -1\n",
    "    k = kappa(x)\n",
    "    pub = x[2]\n",
    "    adv = x[3]\n",
    "    gen = x[4]\n",
    "    #extract appropriate latent vectors to update, given the impression\n",
    "    if pub == '1':\n",
    "        w_pa = W_p.value[0]\n",
    "        w_pg = W_p.value[1]\n",
    "    elif pub == '2':\n",
    "        w_pa = W_p.value[2]\n",
    "        w_pg = W_p.value[3]\n",
    "    else:\n",
    "        w_pa = W_p.value[4]\n",
    "        w_pg = W_p.value[5]\n",
    "    if adv == '1':\n",
    "        w_ap = W_a.value[0]\n",
    "        w_ag = W_a.value[1]\n",
    "    elif adv == '2':\n",
    "        w_ap = W_a.value[2]\n",
    "        w_ag = W_a.value[3]\n",
    "    else:\n",
    "        w_ap = W_a.value[4]\n",
    "        w_ag = W_a.value[5]\n",
    "    if gen == '1':\n",
    "        w_gp = W_g.value[0]\n",
    "        w_ga = W_g.value[1]\n",
    "    else:\n",
    "        w_gp = W_g.value[2]\n",
    "        w_ga = W_g.value[3]\n",
    "    #calculate subgradients\n",
    "    g_pa = k*w_ap\n",
    "    g_pg = k*w_gp\n",
    "    g_ap = k*w_pa\n",
    "    g_ag = k*w_ga\n",
    "    g_gp = k*w_pg\n",
    "    g_ga = k*w_ag\n",
    "    #accumulate sum of squared gradients, add to G vector (initialized as ones)\n",
    "    G_pa = np.ones(3) + g_pa**2\n",
    "    G_pg = np.ones(3) + g_pg**2\n",
    "    G_ap = np.ones(3) + g_ap**2\n",
    "    G_ag = np.ones(3) + g_ag**2\n",
    "    G_gp = np.ones(3) + g_gp**2\n",
    "    G_ga = np.ones(3) + g_ga**2\n",
    "    \n",
    "    #update latent vectors\n",
    "    w_pa = w_pa - np.divide(0.1,np.sqrt(G_pa))*g_pa\n",
    "    w_pg = w_pg - np.divide(0.1,np.sqrt(G_pg))*g_pg\n",
    "    w_ap = w_ap - np.divide(0.1,np.sqrt(G_ap))*g_ap\n",
    "    w_ag = w_ag - np.divide(0.1,np.sqrt(G_ag))*g_ag\n",
    "    w_gp = w_gp - np.divide(0.1,np.sqrt(G_gp))*g_gp\n",
    "    w_ga = w_ga - np.divide(0.1,np.sqrt(G_ga))*g_ga\n",
    "    #yield features (to know which vectors to update in weight matrix) and updated latent vectors\n",
    "    yield  pub, adv, gen, w_pa, w_pg, w_ap, w_ag, w_gp, w_ga\n",
    "    \n",
    "# demonstrate, specifying first impression to check hand calculations\n",
    "update = sc.parallelize(resultRDD.takeOrdered(1)).flatMap(gradient).collect()\n",
    "update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the third vector here, representing the updated `PepsiP` latent vector,  matches our hand calculation (which rounded to the second decimal place). We must also pass along the features in order to ensure we update the correct latent vectors in the weight matrix. In the case of the first impression, we emit '1', '1', '1' for the features, so we know to update NetflixA (`W_P[0]`), Netflix G (`W_P[1]`), PepsiP(`W_A[0]`), PepsiG (`W_A[1]`), MaleP(`W_G[0]`), and MaleA (`W_G[1]`) in the weight matrix:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeights(W_P, W_A, W_G, update):\n",
    "# Function that takes current weight matrices, and updated vectors emitted after SGD\n",
    "# and updates Weight Matrices\n",
    "    #take first feature (Publisher), and update W_P latent vectors accordingly\n",
    "    if update[0][0] == '1':\n",
    "        W_P[0] = update[0][3]\n",
    "        W_P[1] = update[0][4]\n",
    "    elif update[0][0] == '2':\n",
    "        W_P[2] = update[0][3]\n",
    "        W_P[3] = update[0][4]\n",
    "    else:\n",
    "        W_P[4] = update[0][3]\n",
    "        W_P[5] = update[0][4]\n",
    "    #take second feature (Advertiser), and update W_A latent vectors accordingly\n",
    "    if update[0][1] == '1':\n",
    "        W_A[0] = update[0][5]\n",
    "        W_A[1] = update[0][6]\n",
    "    elif update[0][1] == '2':\n",
    "        W_A[2] = update[0][5]\n",
    "        W_A[3] = update[0][6]\n",
    "    else:\n",
    "        W_A[4] = update[0][5]\n",
    "        W_A[5] = update[0][6]\n",
    "    #take third feature (Gender), and update W_G latent vectors accordingly\n",
    "    if update[0][2] == '1':\n",
    "        W_G[0] = update[0][7]\n",
    "        W_G[1] = update[0][8]\n",
    "    else:\n",
    "        W_G[2] = update[0][7]\n",
    "        W_G[3] = update[0][8]\n",
    "    return W_P, W_A, W_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print the updated matrices of latent vectors that would be broadcast back out for the next iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.94767801, -0.2160607 , -0.20541602],\n",
       "        [-0.62120264,  0.03980592, -0.2368895 ],\n",
       "        [-0.1       , -0.5       , -1.2       ],\n",
       "        [-1.1       ,  0.2       ,  1.2       ],\n",
       "        [ 1.4       , -0.7       ,  0.7       ],\n",
       "        [ 0.        ,  0.8       ,  0.1       ]]),\n",
       " array([[ 1.04386801, -0.31078469, -0.11078469],\n",
       "        [-0.44386801, -0.94237342, -0.04767801],\n",
       "        [-0.2       , -1.        , -1.7       ],\n",
       "        [ 0.7       , -0.3       , -1.1       ],\n",
       "        [ 0.1       ,  1.3       , -0.2       ],\n",
       "        [ 0.5       ,  0.2       ,  0.5       ]]),\n",
       " array([[-0.43094634,  0.8       ,  1.4839393 ],\n",
       "        [-0.92120264,  1.25232199, -1.        ],\n",
       "        [ 0.8       , -1.5       ,  0.3       ],\n",
       "        [ 0.4       ,  0.7       ,  0.5       ]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateWeights(W_P, W_A, W_G, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6)__ We repeat 3-5 for a specified number of iterations or until some convergence criteria is met for the latent vectors. In each iteration we would use a different impression to update the latent vectors. Therefore, our code from above must be slightly modified (to choose an impression at random, and not explicitly the first impression. Below is the code we would run for 10 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Loss: 0.6674505942985505\n",
      "Iteration: 2\n",
      "Loss: 0.6494112620568574\n",
      "Iteration: 3\n",
      "Loss: 0.655285193107078\n",
      "Iteration: 4\n",
      "Loss: 0.6289630447180298\n",
      "Iteration: 5\n",
      "Loss: 0.6460073554561067\n",
      "Iteration: 6\n",
      "Loss: 0.6310762578752105\n",
      "Iteration: 7\n",
      "Loss: 0.6310762578752105\n",
      "Iteration: 8\n",
      "Loss: 0.6269998104577843\n",
      "Iteration: 9\n",
      "Loss: 0.6314429931312981\n",
      "Iteration: 10\n",
      "Loss: 0.6278931538743446\n",
      "Final Weight Matrices:\n",
      "W_P:\n",
      " [[ 0.43168523  0.74967893  0.34789988]\n",
      " [ 0.7024423   0.99953272  0.52493794]\n",
      " [-0.9         1.4        -1.5       ]\n",
      " [-1.6        -0.4        -1.4       ]\n",
      " [-0.04794223  1.54197082 -1.46481178]\n",
      " [-0.4055017   0.06581357  1.12199341]] \n",
      "W_A:\n",
      " [[ 0.40556162 -0.44751152  0.65177044]\n",
      " [ 0.89036241 -1.50447862  0.95611675]\n",
      " [ 1.45286369 -0.16237191  0.25864327]\n",
      " [ 1.27069659 -0.86144194  1.32753492]\n",
      " [-0.3         0.4         1.6       ]\n",
      " [-0.6        -0.7        -0.7       ]] \n",
      "W_G:\n",
      " [[-1.29278909 -0.38949718  1.60583298]\n",
      " [ 0.30907984 -1.41524401  1.00969589]\n",
      " [ 0.17304778 -0.42320007 -1.10260394]\n",
      " [ 1.08750227 -0.97697814 -0.2830387 ]]\n"
     ]
    }
   ],
   "source": [
    "W_P = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "W_A = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "W_G = np.random.uniform(-1.7,1.7, size=(4,3)).round(1)\n",
    "for i in range(10):\n",
    "    print('Iteration:', i+1)\n",
    "    W_p = sc.broadcast(W_P)\n",
    "    W_a = sc.broadcast(W_A)\n",
    "    W_g = sc.broadcast(W_G)\n",
    "    resultRDD = toyRDD.flatMap(predict).cache()\n",
    "    loss = resultRDD.flatMap(log_loss).reduceByKey(lambda x, y: sum_loss(x, y))\\\n",
    "        .map(lambda x: -np.divide(x[1][0], x[1][1])).collect()\n",
    "    print('Loss:', loss[0])\n",
    "    update = sc.parallelize(resultRDD.takeSample(True,1)).flatMap(gradient).collect()\n",
    "    updateWeights(W_P, W_A, W_G, update)\n",
    "    \n",
    "print('Final Weight Matrices:')\n",
    "print('W_P:\\n', W_P,'\\nW_A:\\n', W_A,'\\nW_G:\\n', W_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that stochastic gradient descent does not always improve the loss from iteration to iteration, as only a single impression is used to update after each iteration. For this and other reasons, we consider using other methods of gradient descent moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "- https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "- https://www.youtube.com/watch?v=1cRGpDXTJC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've had a discussion about the algorithm we are going to use for the project, in this section, we seek to understand the important characteristics of the training dataset and how we can use them to inform our implementation of the FFM algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "We get the data from link from provided in `w261Project2019.pdf`, which is also in this repository. The link to the original data source is:\n",
    "\n",
    "http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/\n",
    "\n",
    "Desciption about the data from quoting the from the `readme.txt`:\n",
    "\n",
    "> \"The training dataset consists of a portion of Criteo's traffic over a period of 7 days. Each row corresponds to a display ad served by Criteo and the first column is indicates whether this ad has been clicked or not. The positive (clicked) and negatives (non-clicked) examples have both been subsampled (but at different rates) in order to reduce the dataset size. There are 13 features taking integer values (mostly count features) and 26 categorical features. The values of the categorical features have been hashed onto 32 bits for anonymization purposes. The semantic of these features is undisclosed. Some features may have missing values. The rows are chronologically ordered.\"\n",
    "\n",
    "The format of the training data is:\n",
    "\n",
    "> `<label> \\t <integer feature 1> \\t...\\t <integer feature 13> \\t <categorical feature 1> \\t...\\t <categorical feature 26>`  \n",
    "> A field is left empty when the value is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data. By default, in local environment we only load the mediumTrain.parquet which contains 5000 randomly sample instances from train.txt  \n",
    "# because the full dataset is too slow to run in local environment\n",
    "\n",
    "if local:\n",
    "    train_parquet = spark.read.parquet(f\"{PWD}/data/mediumTrain.parquet\")\n",
    "    # train_parquet = spark.read.parquet(f\"{PWD}/data/train.parquet\") #uncomment if loading the entire dataset\n",
    "else: # if using wk08 environment load the file from my bucket\n",
    "    train_parquet = spark.read.parquet(\"gs://w261-bucket-hou/final-project/data/train.parquet\") #train.parquet is the parquet file of train.txt\n",
    "\n",
    "oldColNames = train_parquet.schema.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the label to integer type and rename the label column\n",
    "train_parquet = train_parquet.withColumn(\"label\", train_parquet[\"_c0\"].cast(types.IntegerType()))\n",
    "\n",
    "#rename files and recast integer types on the first 13 features\n",
    "for colNum in range(1,14): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"int_feature_\"+ str(colNum), train_parquet[colName].cast(types.IntegerType()))\n",
    "for colNum in range(14,40): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"cate_feature_\"+ str(colNum-13), train_parquet[colName])\n",
    "\n",
    "#drop the old columns\n",
    "train_parquet = train_parquet.drop(*oldColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record feature names by feature type\n",
    "intFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'int' and colName != 'label']\n",
    "cateFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'string' and colName != 'label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 1: Overview of the training data\n",
    "\n",
    "We will take a look at the general properties of the training data in this section, such as the shape of the dataset and the counts of null and distinct values of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "recCount = train_parquet.count() #get record count\n",
    "fieldCount = len(train_parquet.columns)-1 #get the count of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Get the counts of distinct values and null values in each columns in the data set\n",
    "\n",
    "distinctValues = []\n",
    "nullValues = []\n",
    "for colName in train_parquet.schema.names:\n",
    "    distinctValues.append(train_parquet.select(colName).distinct().count())\n",
    "    nullValues.append(train_parquet.filter((train_parquet[colName] == \"\") | train_parquet[colName].isNull() | isnan(train_parquet[colName])).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Overview:\n",
      "\n",
      "Record Count: 5000\n",
      "Feature Count: 39\n",
      "Total Distinct Categorical Values: 28063\n",
      "Null Value Percentage: 14.2%\n",
      "====================================================================== \n",
      "\n",
      "Column Level Information:\n",
      "\n",
      "Field Name     | Distinct Count | Null Count     | Null Percent   \n",
      "----------------------------------------------------------------------\n",
      "label          | 2              | 0              | 0.0%           \n",
      "int_feature_1  | 63             | 2212           | 44.2%          \n",
      "int_feature_2  | 570            | 0              | 0.0%           \n",
      "int_feature_3  | 208            | 1072           | 21.4%          \n",
      "int_feature_4  | 63             | 1034           | 20.7%          \n",
      "int_feature_5  | 3233           | 130            | 2.6%           \n",
      "int_feature_6  | 567            | 1099           | 22.0%          \n",
      "int_feature_7  | 207            | 215            | 4.3%           \n",
      "int_feature_8  | 52             | 3              | 0.1%           \n",
      "int_feature_9  | 582            | 215            | 4.3%           \n",
      "int_feature_10 | 6              | 2212           | 44.2%          \n",
      "int_feature_11 | 50             | 215            | 4.3%           \n",
      "int_feature_12 | 25             | 3826           | 76.5%          \n",
      "int_feature_13 | 77             | 1034           | 20.7%          \n",
      "cate_feature_1 | 115            | 0              | 0.0%           \n",
      "cate_feature_2 | 332            | 0              | 0.0%           \n",
      "cate_feature_3 | 3292           | 177            | 3.5%           \n",
      "cate_feature_4 | 2421           | 177            | 3.5%           \n",
      "cate_feature_5 | 48             | 0              | 0.0%           \n",
      "cate_feature_6 | 7              | 569            | 11.4%          \n",
      "cate_feature_7 | 2238           | 0              | 0.0%           \n",
      "cate_feature_8 | 63             | 0              | 0.0%           \n",
      "cate_feature_9 | 2              | 0              | 0.0%           \n",
      "cate_feature_10| 2085           | 0              | 0.0%           \n",
      "cate_feature_11| 1576           | 0              | 0.0%           \n",
      "cate_feature_12| 3111           | 177            | 3.5%           \n",
      "cate_feature_13| 1349           | 0              | 0.0%           \n",
      "cate_feature_14| 22             | 0              | 0.0%           \n",
      "cate_feature_15| 1486           | 0              | 0.0%           \n",
      "cate_feature_16| 2821           | 177            | 3.5%           \n",
      "cate_feature_17| 9              | 0              | 0.0%           \n",
      "cate_feature_18| 904            | 0              | 0.0%           \n",
      "cate_feature_19| 360            | 2227           | 44.5%          \n",
      "cate_feature_20| 4              | 2227           | 44.5%          \n",
      "cate_feature_21| 2960           | 177            | 3.5%           \n",
      "cate_feature_22| 7              | 3836           | 76.7%          \n",
      "cate_feature_23| 12             | 0              | 0.0%           \n",
      "cate_feature_24| 1594           | 177            | 3.5%           \n",
      "cate_feature_25| 35             | 2227           | 44.5%          \n",
      "cate_feature_26| 1210           | 2227           | 44.5%          \n"
     ]
    }
   ],
   "source": [
    "#print the results\n",
    "print(\"Training Dataset Overview:\\n\")\n",
    "print(\"Record Count:\", recCount)\n",
    "print(\"Feature Count:\", fieldCount)\n",
    "print(\"Total Distinct Categorical Values:\", np.sum(distinctValues[14:]))\n",
    "print(\"Null Value Percentage:\", str(round(float(sum(nullValues))/(recCount*fieldCount)*100,1)) + \"%\")\n",
    "\n",
    "print(\"=\"*70, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"Column Level Information:\\n\")\n",
    "print('%-15s| %-15s| %-15s| %-15s' %('Field Name','Distinct Count', 'Null Count', 'Null Percent'))\n",
    "print('-'*70)\n",
    "for colName, distinctVal, nullVal in zip(train_parquet.schema.names, distinctValues, nullValues):\n",
    "    print('%-15s| %-15s| %-15s| %-15s' %(colName, distinctVal, nullVal, str(round(float(nullVal)/recCount*100,1)) + \"%\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EDA 1 Findings:__\n",
    "\n",
    "There are a few things we see in our dataset that helps inform our implementation:\n",
    "\n",
    "> We have a total of $45,840,617$ records in the `train.txt`. We would still have $45,840,617 \\cdot 80\\% = 36,672,493$ records we can use to train our model even if we do a 80%, 20% split and save the 20% for validation. With this many records in our dataset and the difficulty in parallelize stochastic gradient descent, we might explore using mini-batch gradient descent to speed up training.\n",
    "\n",
    "> $14.2\\%$ of the data has null values, and some categorical features (see `cate_feature_3`, `cate_feature_16` and etc...) have a large number of distinct values. With these two properties, if we simply one-hot encode the categorial variable and stroe the entire vector, we will have a vary sparse dataset. To efficiently store the data in memory, we need to consider using a more dense representation of the data similar to stripes or pairs introduced in class.\n",
    "\n",
    "> We notice that categorical features (see `cate_feature_3`, `cate_feature_16` and etc...) with a large number of distinct values might imply very low counts of records per values which will make our model prone to overfitting and reduce the generalizability. For example, if we train many parameters on only a few records, would mean we are training our model based on some specific information only contained in this particular training set. Therefore, in the next EDA section, we will explore this topic further and suggest ways to deal with this issue if it exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 2: Categorical Features\n",
    "\n",
    "In this section of EDA, we will take a look at the categorical variables in the dataset with the main goal of exploring how many instances we have for each distinct value which is equivalent to a feature after one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct_summary(df, col, thresholds):\n",
    "    \"\"\"\n",
    "    Helper function which helps calculate the numbers of distinct values left after\n",
    "    exluding those whose count of instances is lower than given thresholds\n",
    "    \n",
    "    Args:\n",
    "        df         - a spark dataframe of the entire dataset\n",
    "        col        - the column name of the column used in the calculation\n",
    "        thresholds - a list of numbers used as the thresholds\n",
    "    Returns:\n",
    "        summary_values - a list: \n",
    "                         1. first element as the column name\n",
    "                         2. second element as the distinct value count\n",
    "                         3. the following elements as the counts of distinct values left \n",
    "                            after exluding the ones having counts lower than the threshold.\n",
    "    \"\"\"\n",
    "    \n",
    "    summary_values = [col]\n",
    "    summary_values.append(df.select(col).distinct().count())\n",
    "    \n",
    "    count_df = df.groupBy(col).count()\n",
    "    filtered_counts = [count_df.filter(f\"count >= {threshold}\").count() + 1 for threshold in thresholds]\n",
    "    summary_values.extend(filtered_counts)\n",
    "    \n",
    "    return [str(val) for val in summary_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the distinct value counts for all categorical variables for the thresholds below\n",
    "\n",
    "thresholds = [5, 10, 25, 50, 100, 500, 1000]\n",
    "distinct_summary = [get_distinct_summary(train_parquet, c, thresholds) for c in cateFieldNames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number and the percentage of feature preserved for a given threshold\n",
    "\n",
    "distinct_counts = np.asarray(distinct_summary)[:,1:].astype(int)\n",
    "feat_num = np.sum(distinct_counts, axis = 0)\n",
    "feat_percent_preserv = np.sum(distinct_counts, axis = 0)/np.sum(distinct_counts, axis = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Feature Counts by Threshold')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot feature counts by threshold\n",
    "\n",
    "plt.figure(figsize=(9, 5))\n",
    "\n",
    "\n",
    "Ts = ['None'] + [f\"{threshold}\" for threshold in thresholds]\n",
    "y_pos = np.arange(len(Ts))\n",
    "\n",
    "plt.bar(y_pos, feat_num, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, Ts)\n",
    "plt.ylabel('Feature Count')\n",
    "plt.xlabel('Threshold')\n",
    "plt.title('Feature Counts by Threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+----------+-----------+-----------+-----------+------------+------------+-------------+\n",
      "|          Feature|Distinct|count >= 5|count >= 10|count >= 25|count >= 50|count >= 100|count >= 500|count >= 1000|\n",
      "+-----------------+--------+----------+-----------+-----------+-----------+------------+------------+-------------+\n",
      "|   cate_feature_1|     115|        35|         22|         13|          9|           7|           3|            2|\n",
      "|   cate_feature_2|     332|       144|         95|         41|         20|          12|           2|            1|\n",
      "|   cate_feature_3|    3292|        93|         41|         14|          5|           3|           1|            1|\n",
      "|   cate_feature_4|    2421|       122|         55|         17|         10|           6|           1|            1|\n",
      "|   cate_feature_5|      48|        19|         13|         10|          7|           6|           3|            2|\n",
      "|   cate_feature_6|       7|         8|          8|          8|          8|           8|           5|            3|\n",
      "|   cate_feature_7|    2238|       189|         56|         14|          4|           2|           1|            1|\n",
      "|   cate_feature_8|      63|        23|         17|         12|          8|           6|           3|            2|\n",
      "|   cate_feature_9|       2|         3|          3|          3|          3|           3|           3|            2|\n",
      "|  cate_feature_10|    2085|       139|         34|          7|          3|           2|           2|            2|\n",
      "|  cate_feature_11|    1576|       228|         78|         21|          8|           3|           1|            1|\n",
      "|  cate_feature_12|    3111|       102|         43|         15|          6|           4|           1|            1|\n",
      "|  cate_feature_13|    1349|       245|         93|         24|          9|           3|           1|            1|\n",
      "|  cate_feature_14|      22|        17|         13|         11|          9|           7|           4|            3|\n",
      "|  cate_feature_15|    1486|       246|        106|         27|          5|           1|           1|            1|\n",
      "|  cate_feature_16|    2821|       117|         52|         18|          7|           4|           1|            1|\n",
      "|  cate_feature_17|       9|        10|         10|         10|         10|          10|           4|            2|\n",
      "|  cate_feature_18|     904|       216|        111|         37|         13|           7|           1|            1|\n",
      "|  cate_feature_19|     360|        53|         21|          9|          4|           4|           3|            3|\n",
      "|  cate_feature_20|       4|         5|          5|          5|          5|           5|           5|            2|\n",
      "|  cate_feature_21|    2960|       111|         47|         17|          7|           4|           1|            1|\n",
      "|  cate_feature_22|       7|         7|          7|          5|          4|           4|           3|            2|\n",
      "|  cate_feature_23|      12|        13|         12|         10|          9|           8|           4|            3|\n",
      "|  cate_feature_24|    1594|       114|         60|         22|         16|           9|           1|            1|\n",
      "|  cate_feature_25|      35|        26|         19|         13|         11|          10|           4|            2|\n",
      "|  cate_feature_26|    1210|        84|         33|         12|          8|           4|           2|            2|\n",
      "|      ***Total***|   28063|      2369|       1054|        395|        208|         142|          61|           44|\n",
      "|***% Preserved***|  100.0%|      8.4%|       3.8%|       1.4%|       0.7%|        0.5%|        0.2%|         0.2%|\n",
      "+-----------------+--------+----------+-----------+-----------+-----------+------------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show the distinct value count for all categorical variables for all thresholds and the summary at the bottom\n",
    "\n",
    "eda2_summary = distinct_summary + [[\"***Total***\"] + [str(num) for num in feat_num]]\n",
    "eda2_summary = eda2_summary + [[\"***% Preserved***\"] + [str(round(percent*100,1))+\"%\" for percent in feat_percent_preserv]]\n",
    "\n",
    "\n",
    "distinctCounts = spark.createDataFrame(eda2_summary, ['Feature', 'Distinct'] + [f\"count >= {threshold}\" for threshold in thresholds] )\n",
    "distinctCounts.show(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EDA 2 Findings:__\n",
    "\n",
    "> In this section, we found that most of the values of the categorical features have very few counts. Even a threshold of five occurance would cause a drastic drop in feature number (refer to the bar graph above). Looking at the row, `***% Preserved***` in the table above, we only have 6.2% of the values left if we set the threshold for minimum occurance to be 5 and 3.2% left when the threshold is set to 10. This piece of information helps inform the feature engineering. As part of our feature engineering, we will attempt to establish a threshold to exclude these values having very low occurances so we don't train our model with local trend that doesn't exist in the population. \n",
    "\n",
    "> Rare values does not equal missing value. Therefore, we will also explore to assign a special value to those rare values so the algorithm can distinguish them from missing data and observe this practice's impact on model performance.\n",
    "\n",
    "> Looking at the counts of features left after a given threshold is applied (row, `***Total***` in the table above), we can see that when the threshold is set to ten, we would still have around 1 million one-hot-encoded features, and we can guarantee that all the parameters in our models are trained as least on ten examples. Therefore, we will set the threshold at ten to filter our categorical values. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA 3: Integer Features\n",
    "\n",
    "In this section, we perform an EDA on the integer features (i.e., the first 13 features). The goal is to identify possible feature engineering that could potentially improve our algorithm's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|summary|              label|    int_feature_1|     int_feature_2|    int_feature_3|    int_feature_4|     int_feature_5|     int_feature_6|\n",
      "+-------+-------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "|  count|               5000|             2788|              5000|             3928|             3966|              4870|              3901|\n",
      "|   mean|              0.262|3.826757532281205|          110.9356| 43.9806517311609|7.389813414019163|17061.030184804928|118.69700076903358|\n",
      "| stddev|0.43976661871473055| 9.53160896120588|393.61348473815525|870.2330455018677|8.877438545143875| 58911.75067484051| 422.3105054811211|\n",
      "|    min|                  0|                0|                -2|                0|                0|                 0|                 0|\n",
      "|    max|                  1|              145|              4864|            37170|              107|           1077390|             17030|\n",
      "+-------+-------------------+-----------------+------------------+-----------------+-----------------+------------------+------------------+\n",
      "\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|     int_feature_6|     int_feature_7|     int_feature_8|     int_feature_9|    int_feature_10|    int_feature_11|    int_feature_12|   int_feature_13|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|              3901|              4785|              4997|              4785|              2788|              4785|              1174|             3966|\n",
      "|   mean|118.69700076903358| 16.61671891327064|12.236141685011006|109.79958202716823|0.6362984218077475|2.7839080459770114|0.8091993185689949| 8.20196671709531|\n",
      "| stddev| 422.3105054811211|62.123301344325135|12.688856379647316|237.26999987911375| 0.684862885900568| 5.190126031125586| 3.549342920033132|13.69169835883991|\n",
      "|    min|                 0|                 0|                 0|                 0|                 0|                 0|                 0|                0|\n",
      "|    max|             17030|              1777|                71|              4371|                 4|                79|                69|              471|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_parquet.select(intFieldNames[:7]).describe().show()\n",
    "train_parquet.select(intFieldNames[6:]).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 14 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 16))\n",
    "for i in range(len(intFieldNames)):\n",
    "    colName = intFieldNames[i]\n",
    "    ax = plt.subplot(5, 4, i+1)\n",
    "    ax.hist(train_parquet.select(colName).rdd.map(lambda r: r[0]).filter(lambda x: x!=None).collect())\n",
    "    ax.set_title(colName)\n",
    "\n",
    "st = fig.suptitle(\"Integer Feature Histogram\", fontsize=\"x-large\")\n",
    "fig.tight_layout()\n",
    "st.set_y(1.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__EDA 3 Findings:__\n",
    "\n",
    "From both the histograms and statistics summary, we can see that most of the integer features are very right-skewed with outliers. When fitting to the integers without any feature engineering, we might give the outliers more than appropriate weight in affecting our model's decision-making. Therefore, it is worth it to try transforming the integers by taking the natural log of its values. We will compare the performance of the model with and without the log transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "We start section 4 with our default feature engineering based on the EDA in section 3 that we think can improve the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "\n",
    "We replace the categorical values having counts less than 10 occurances with a special value `***` to prevent overfitting and distinguishing them from missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the default threshold at 10\n",
    "threshold = 10\n",
    "\n",
    "train_parquet_MD = train_parquet\n",
    "\n",
    "# transform data column by column\n",
    "for col in cateFieldNames:\n",
    "    valuesToKeep = train_parquet.groupBy(col).count().filter(f\"count >= {threshold}\").select(col)\n",
    "    valuesToKeep = valuesToKeep.withColumn(\"_\"+col, train_parquet[col])\n",
    "    valuesToKeep = valuesToKeep.drop(col)\n",
    "\n",
    "    train_parquet_MD = train_parquet_MD.join(F.broadcast(valuesToKeep), train_parquet_MD[col] == valuesToKeep[\"_\"+col], 'leftouter')\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.when(F.col(\"_\"+col).isNull(), \"***\").otherwise(F.col(\"_\"+col)))\n",
    "    train_parquet_MD = train_parquet_MD.drop(\"_\"+col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|         2015|         null|            2|        11727|         null|            0|           33|            2|          null|             0|             0|             2|      8cf07265|      2c16a946|           ***|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|            ***|       3516f6e6|       b28479f6|       3628a186|            ***|       3486227d|       e4ca448c|            ***|            ***|            ***|            ***|       bcdee96c|       9117a34a|            ***|            ***|\n",
      "|    0|         null|           -1|           26|            7|        96252|         null|            0|            7|           14|          null|             0|          null|             7|      05db9164|      80e26c9b|           ***|           ***|      25c83c98|           ***|           ***|      64523cfa|      7cc72ec2|            ***|            ***|            ***|            ***|       cfef1c29|            ***|            ***|       e5ba7672|       f54016b9|       21ddcdc9|       b1252a9d|            ***|            ***|       423fab69|            ***|       e8b83407|            ***|\n",
      "|    0|         null|            1|            1|           29|          100|         null|            0|           29|           30|          null|             0|             0|            30|      5a9ed9b0|      8084ee93|      d032c263|      c18be181|      25c83c98|      7e0ccccf|      dda1fed2|      5b392875|      a73ee510|       547c0ffe|       7f8ffe57|       dfbb09fb|       46f42a63|       07d13a8f|       f3996583|       84898b2a|       1e88c74f|       003d4f4f|            ***|            ***|       0014c32a|            ***|       3a171ecb|       3b183c5c|            ***|            ***|\n",
      "|    0|         null|           -1|         null|         null|        30147|            7|            2|            0|            3|          null|             1|          null|          null|           ***|      38a947a1|           ***|           ***|      25c83c98|      6f6d9be8|           ***|      5b392875|      7cc72ec2|            ***|       7a3651f5|            ***|       95bc260c|       07d13a8f|            ***|            ***|       07c540c4|            ***|            ***|            ***|            ***|            ***|       32c7478e|            ***|            ***|            ***|\n",
      "|    0|         null|            5|           11|            1|         1183|         null|            0|           44|          108|          null|             0|          null|             1|      39af2607|      287130e0|           ***|           ***|      30903e74|      fe6b92e5|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|       9efd8b77|            ***|       e5ba7672|       891589e7|            ***|       b1252a9d|            ***|       78e2e389|       bcdee96c|       3fdb382b|       ea9a246c|       49d68486|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view data after the replacement\n",
    "train_parquet_MD.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Variables\n",
    "\n",
    "By default, we take the floor of the log of the numerical value + 2. \n",
    "> Log transform is aimming to reduce the potentially drastic effect out outliers can have in the model.  \n",
    "> The +2 is to prevent 0 becoming undefined and to separate 0 from 1 because 0 might have a more special meaning than value >= 1 since it represents the total absence of something.  \n",
    "> ($\\lfloor log(0+2) \\rfloor = 0$, $\\lfloor log(i+2) \\rfloor = 0, where\\ i=\\{1,2,3,4,5\\}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform data column by column\n",
    "for col in intFieldNames:\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.floor(F.log(F.col(col)+F.lit(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|            7|         null|            1|            9|         null|            0|            3|            1|          null|             0|             0|             1|      8cf07265|      2c16a946|           ***|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|            ***|       3516f6e6|       b28479f6|       3628a186|            ***|       3486227d|       e4ca448c|            ***|            ***|            ***|            ***|       bcdee96c|       9117a34a|            ***|            ***|\n",
      "|    0|         null|         null|            3|            2|           11|         null|            0|            2|            2|          null|             0|          null|             2|      05db9164|      80e26c9b|           ***|           ***|      25c83c98|           ***|           ***|      64523cfa|      7cc72ec2|            ***|            ***|            ***|            ***|       cfef1c29|            ***|            ***|       e5ba7672|       f54016b9|       21ddcdc9|       b1252a9d|            ***|            ***|       423fab69|            ***|       e8b83407|            ***|\n",
      "|    0|         null|            0|            0|            3|            4|         null|            0|            3|            3|          null|             0|             0|             3|      5a9ed9b0|      8084ee93|      d032c263|      c18be181|      25c83c98|      7e0ccccf|      dda1fed2|      5b392875|      a73ee510|       547c0ffe|       7f8ffe57|       dfbb09fb|       46f42a63|       07d13a8f|       f3996583|       84898b2a|       1e88c74f|       003d4f4f|            ***|            ***|       0014c32a|            ***|       3a171ecb|       3b183c5c|            ***|            ***|\n",
      "|    0|         null|         null|         null|         null|           10|            2|            1|            0|            1|          null|             0|          null|          null|           ***|      38a947a1|           ***|           ***|      25c83c98|      6f6d9be8|           ***|      5b392875|      7cc72ec2|            ***|       7a3651f5|            ***|       95bc260c|       07d13a8f|            ***|            ***|       07c540c4|            ***|            ***|            ***|            ***|            ***|       32c7478e|            ***|            ***|            ***|\n",
      "|    0|         null|            1|            2|            0|            7|         null|            0|            3|            4|          null|             0|          null|             0|      39af2607|      287130e0|           ***|           ***|      30903e74|      fe6b92e5|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|       9efd8b77|            ***|       e5ba7672|       891589e7|            ***|       b1252a9d|            ***|       78e2e389|       bcdee96c|       3fdb382b|       ea9a246c|       49d68486|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view data after the transformation\n",
    "train_parquet_MD.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All four of the models we discussed in section 2 were parametric models. In order to train and use parametric models, we need to transform the text of categorical data so that they are represented by numbers. The most straight forward way to tackle this task is one-hot-encoding where for each category, a distinct value is represented by their position in the vector and a value of one in the position.\n",
    "\n",
    "In our project, we decided to use the hashing trick: https://en.wikipedia.org/wiki/Feature_hashing  \n",
    "The core idea of feature hash is simple: Instead of making a one-to-one mapping of a categorical value to a position and a number in the feature vector, we use a hashing function to decide the location of a categorical value in our feature vector, which usually has a lower dimension than the dimension of our features. \n",
    "\n",
    "Just like any other hashing methods, collision might occur (collision is bound to happen when hashing to a lower dimension), where two distinct features are hashed to the same location. Yet, empirical findings have suggusted that for very sparse feature space, it doesn't significantly impact performance. Since data is sparse, it is unlikely that a single feature can have great impact in the model performance. A collision is only catastrophic when two very important features collide but it is unlikely for sparse data since it's rare to have one deciding feature and the odds of two extremely features colliding in a large space is even lower. In some cases, collision brings in noise to our training data resulting in less overfitting and improved performance. \n",
    "\n",
    "In this project we consider to use feature hashing due to the following advantages it has over one-hot-encoding:\n",
    "1. It is faster than buiding the one-to-one mapping needed for one-hot-encoding\n",
    "2. We don't need to store that mapping in memory for lookup for transforming unseen data come inference time.\n",
    "3. We don't need to deal with unseen feature come inference time.\n",
    "\n",
    "_source: https://dzone.com/articles/feature-hashing-for-scalable-machine-learning_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# set the number of feature to 50000 about twice as much as the average feature count for each column. \n",
    "# This number is to be higher when using non-field-aware methods\n",
    "n_features = 50000 \n",
    "n_fields = len(intFieldNames) + len(cateFieldNames)\n",
    "\n",
    "# hash the features\n",
    "hasher = FeatureHasher()\n",
    "# the default is to hash integer features as categorical\n",
    "hasher.setCategoricalCols(intFieldNames)\n",
    "hasher.setNumFeatures(n_features)\n",
    "\n",
    "for col in intFieldNames + cateFieldNames:\n",
    "    hasher.setInputCols([col])\n",
    "    hasher.setOutputCol(col+\"_hashed\")\n",
    "    train_parquet_MD = hasher.transform(train_parquet_MD)\n",
    " \n",
    "\n",
    "hashed_columns = train_parquet_MD.schema.names[-n_fields:]\n",
    "\n",
    "# Convert values to hashes\n",
    "def parse_sparse_vectors(vector, field_ind):\n",
    "    \"\"\"\n",
    "    Helper function which takes in a sparse vector and field number and emit a tuple of (FIELD_NUMBER, FEATURE_NUMBER)\n",
    "    \n",
    "    Args:\n",
    "        vector  - a sparse vector with the hash\n",
    "        field_ind  - the field index of the feature\n",
    "    Returns:\n",
    "        a tuple of (FIELD_NUMBER, FEATURE_NUMBER)\n",
    "    \"\"\"\n",
    "    if vector.indices.size > 0:\n",
    "        return int(vector.indices[0])\n",
    "    else:\n",
    "        return None\n",
    "# build a udf with the function parse_sparse_vectors\n",
    "vector_parser = F.udf(parse_sparse_vectors, types.IntegerType())\n",
    "\n",
    "train_parquet_hashed = train_parquet_MD\n",
    "\n",
    "# convert the sparse vectors to tuples of (FIELD_NUMBER, FEATURE_NUMBER)\n",
    "\n",
    "for field_ind, col in enumerate(hashed_columns):\n",
    "    train_parquet_hashed = train_parquet_hashed.withColumn(col, vector_parser(col, F.lit(field_ind)))\n",
    "\n",
    "train_parquet_hashed = train_parquet_hashed.drop(*(intFieldNames + cateFieldNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|label|int_feature_1_hashed|int_feature_2_hashed|int_feature_3_hashed|int_feature_4_hashed|int_feature_5_hashed|int_feature_6_hashed|int_feature_7_hashed|int_feature_8_hashed|int_feature_9_hashed|int_feature_10_hashed|int_feature_11_hashed|int_feature_12_hashed|int_feature_13_hashed|cate_feature_1_hashed|cate_feature_2_hashed|cate_feature_3_hashed|cate_feature_4_hashed|cate_feature_5_hashed|cate_feature_6_hashed|cate_feature_7_hashed|cate_feature_8_hashed|cate_feature_9_hashed|cate_feature_10_hashed|cate_feature_11_hashed|cate_feature_12_hashed|cate_feature_13_hashed|cate_feature_14_hashed|cate_feature_15_hashed|cate_feature_16_hashed|cate_feature_17_hashed|cate_feature_18_hashed|cate_feature_19_hashed|cate_feature_20_hashed|cate_feature_21_hashed|cate_feature_22_hashed|cate_feature_23_hashed|cate_feature_24_hashed|cate_feature_25_hashed|cate_feature_26_hashed|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|    0|                null|               20820|                null|                 679|               27024|                null|               42737|                9529|               31135|                 null|                28649|                37498|                 2881|                38201|                41359|                19906|                49376|                34640|                25660|                35091|                27171|                36563|                 19215|                 28094|                 37894|                 21219|                 11095|                 40079|                 47102|                 20385|                 40672|                 22179|                 25247|                 36487|                 32721|                 46979|                 26081|                 31686|                 27277|\n",
      "|    0|                null|                null|               19573|               46777|               11705|                null|               42737|               20980|               47212|                 null|                28649|                 null|                12994|                38616|                29633|                19906|                 8694|                34640|                23340|                11202|                37328|                  855|                 25306|                 37462|                 37894|                 18815|                 10735|                 17855|                 47102|                 46193|                 33752|                   618|                 32434|                 36487|                 32721|                 40270|                 34140|                  6937|                 27277|\n",
      "|    0|                null|                4408|               33977|               46072|               27501|                null|               42737|                9529|               49130|                 null|                28649|                37498|                 9629|                49167|                47909|                14873|                32334|                34640|                31272|                16936|                32336|                36563|                 26278|                 44453|                 47519|                 49310|                 33612|                 23164|                 10670|                 48309|                   539|                 22179|                 25247|                 44275|                 32721|                 42773|                 39091|                 31686|                 27277|\n",
      "|    0|                null|                null|                null|                null|               40717|               19038|               43509|               23152|               31135|                 null|                28649|                 null|                 null|                14192|                18153|                19906|                 8694|                34640|                13097|                11202|                32336|                  855|                 25306|                   345|                 37894|                 32169|                 33612|                 17855|                 47102|                  3602|                 12289|                 22179|                 25247|                 36487|                 32721|                 49761|                 34140|                 31686|                 27277|\n",
      "|    0|                null|               39867|                3014|               46475|               45339|                null|               42737|                9529|                3133|                 null|                28649|                 null|                22498|                17641|                36715|                19906|                 8694|                31757|                25660|                11202|                32336|                36563|                 43494|                 37462|                 37894|                 18815|                 11095|                 18700|                 47102|                 46193|                 36234|                 22179|                 32434|                 36487|                 37690|                 46979|                 25014|                 36388|                  7695|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the hashed df\n",
    "train_parquet_hashed.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have preprocessed our training data according with the default values suggested by our EDA. The next step is to train our FFM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM\n",
    "Mathematically, FMM can be expressed as:\n",
    "\n",
    "$ \\phi_{FFM}(w, x) = \\sum\\limits^{n}_{j_1=1} \\sum\\limits^{n}_{j_2=j_1+1}(w_{j_1} \\cdot w_{j_2})x_{j_1}x_{j_2}$\n",
    "\n",
    "However, in the models considered, either all variables are categorical or all integer values are binned effectively making them categorical and the $x_{j_1}$ and $x_{j_2}$ are both equal to 1. This reduces the formula to:\n",
    "\n",
    "$ \\phi_{FFM}(w, x) = \\sum\\limits^{n}_{j_1=1} \\sum\\limits^{n}_{j_2=j_1+1}(w_{j_1} \\cdot w_{j_2})$\n",
    "\n",
    "The optimization function considered for this model is log loss with regularization and the following formula is to be minimized.\n",
    "\n",
    "$\\underset{w}{min}$   $\\dfrac{\\lambda}{2}||w||_2^2 + \\sum\\limits^{m}_{i=1}log(1 + exp(-y_i\\phi_{FFM}(w,x_i)))$\n",
    "\n",
    "Currently, a closed-form solution for minimizing log loss is not known and therefore gradient descent is applied. The gradients for $\\phi_{FFM}(w, x)$ are:\n",
    "\n",
    "$g_{j_1,f_2} = \\triangledown_{w_{j_1,f_2}} f(w) = \\lambda \\cdot w_{j_1,f_2} + \\kappa \\cdot w_{j_2,f_1}$\n",
    "\n",
    "$g_{j_2,f_1} = \\triangledown_{w_{j_2,f_1}} f(w) = \\lambda \\cdot w_{j_2,f_1} + \\kappa \\cdot w_{j_1,f_2}$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\kappa = \\dfrac{\\partial log(1 + exp(-y\\phi_{FFM}(w,x)))}{\\partial \\phi_{FFM}(w, x)} = \\dfrac{-y}{1 + exp(y\\phi_{FFM}(w,x))}$\n",
    "\n",
    "Initially we define two helper function for $\\phi_{FFM}$ and $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x, W):\n",
    "    total = 0\n",
    "    for i in range(len(x) - 1):\n",
    "        if not x[i]:\n",
    "            continue\n",
    "            \n",
    "        for j in range(i + 1, len(x)):\n",
    "            if x[j]:\n",
    "                total += np.dot(W[x[i], j, :], W[x[j], i, :])\n",
    "                            \n",
    "    return total\n",
    "\n",
    "def kappa(y, features, W):\n",
    "    return -int(y)/(1 + np.exp(y*phi(features, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "In addition to these helper functions, an associated cost function to minimize and evaluate the performance of the model during training and validation was required. As noted above, the evaluation method used is the log loss. A helper function is created to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(dataRDD, W):\n",
    "    return dataRDD.map(lambda x: np.log(1 + np.exp(-int(x[0]) * phi(x[1:], W)))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1\n",
    "The first implementation of the algorithm applies batch gradient descent to the model and the algorithm takes the following form for $t$ epochs:\n",
    "\n",
    "1. Map over instance, calculate all subgradients and emit the form ((feature, field) $g_{j_1,f_2}$) and ((feature, field) $g_{j_2,f_1}$) for all field pair combinations\n",
    "2. Reduce by the key to summing values for each key and number of occurances and incrementing a count variable so an average can be calculated. Emit ((feature, field), gradient, count).\n",
    "3. Map over each value, diving the gradient by the count to find an average gradient step.\n",
    "4. Create a tensor of zeroes $R^{n x f x k}$ where $n$ is the number of features, $f$ is the number of fields and $k$ is the size of each latent vector.\n",
    "5. Add each latent vector gradient step to the tensor of zeroes\n",
    "6. Subtract the gradient step multiplied by a learning rate from the original model: $W - eta * gradient$ where W is the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet_hashed = train_parquet_hashed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is first split into train and test for later evaluation before the training set is split into 100 mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train Split\n",
    "train_df, test_df = train_parquet_hashed.randomSplit([0.8, 0.2])\n",
    "batches = train_df.randomSplit([0.01] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7f6a68c8a080>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model parameters\n",
    "k = 2\n",
    "n_features = 50000\n",
    "n_fields = 39\n",
    "eta = 0.3\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(n_fields)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, W):\n",
    "    y = int(x[0])\n",
    "    features = x[1:]\n",
    "    kap = kappa(y, features, W)\n",
    "    \n",
    "    for i in range(len(features) - 1):\n",
    "        if not features[i]:\n",
    "            continue\n",
    "            \n",
    "        for j in range(i+1, len(features)):\n",
    "            if features[j]:\n",
    "                yield ((features[i], j), (kap * W[int(features[j]), i, :], 1))\n",
    "                yield ((features[j], i), (kap * W[int(features[i]), j, :], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_update(dataRDD, W):\n",
    "    grad = dataRDD.flatMap(lambda x: gradient(x, W))\\\n",
    "                .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "                .map(lambda x: ((x[0][0], x[0][1]), x[1][0] / x[1][1]))\\\n",
    "                .collect()\n",
    "    \n",
    "    grad_update = np.zeros(shape=(n_features, n_fields, k))\n",
    "    \n",
    "    for indices, vector in grad:\n",
    "        feature_index = indices[0]\n",
    "        field_index = indices[1]\n",
    "        \n",
    "        grad_update[int(feature_index), field_index, :] += vector\n",
    "    \n",
    "    new_model = W - eta * grad_update\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "Training Loss: 0.5472214583367988\n",
      "Test Loss: 0.5262784148695879\n",
      "----------\n",
      "STEP: 2\n",
      "Training Loss: 0.5647865915673629\n",
      "Test Loss: 0.5028714839356467\n",
      "----------\n",
      "STEP: 3\n",
      "Training Loss: 0.5245438123156343\n",
      "Test Loss: 0.577622650466621\n",
      "----------\n",
      "STEP: 4\n",
      "Training Loss: 0.4733688062360602\n",
      "Test Loss: 0.4455946160742505\n",
      "----------\n",
      "STEP: 5\n",
      "Training Loss: 0.5198603854199589\n",
      "Test Loss: 0.457182182922517\n",
      "----------\n",
      "STEP: 6\n",
      "Training Loss: 0.5545177444479563\n",
      "Test Loss: 0.4219156751234449\n",
      "----------\n",
      "STEP: 7\n",
      "Training Loss: 0.44559461607425044\n",
      "Test Loss: 0.5096670445293715\n",
      "----------\n",
      "STEP: 8\n",
      "Training Loss: 0.5446156418685285\n",
      "Test Loss: 0.4786016246723432\n",
      "----------\n",
      "STEP: 9\n",
      "Training Loss: 0.5986271104835891\n",
      "Test Loss: 0.5406548008367574\n",
      "----------\n",
      "STEP: 10\n",
      "Training Loss: 0.5240868926184952\n",
      "Test Loss: 0.5237112030897364\n",
      "\n",
      "... trained 10 iterations in 88.05562472343445 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def gradient_descent(split_data, w_init, n_steps = 10):\n",
    "    \n",
    "    model = sc.broadcast(w_init)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_steps):\n",
    "        train_rdd = split_data[i].rdd\n",
    "        print(\"----------\")\n",
    "        print(f\"STEP: {i+1}\")\n",
    "        new_model = gd_update(train_rdd, model.value)\n",
    "        model = sc.broadcast(new_model)\n",
    "        train_loss = log_loss(train_rdd, model.value)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        test_loss = log_loss(split_data[i + 50].rdd, model.value)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"\\n... trained {n_steps} iterations in {time.time() - start} seconds\")\n",
    "\n",
    "np.random.seed(1)\n",
    "w_init = np.random.uniform(0, 1/np.sqrt(k), size=(n_features, n_fields, k))\n",
    "gradient_descent(batches, w_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "----------\n",
    "STEP: 1\n",
    "Training Loss: 62.73936057037036\n",
    "Test Loss: 60.10183410156437\n",
    "----------\n",
    "STEP: 2\n",
    "Training Loss: 22.328150602460546\n",
    "Test Loss: 36.45676808231048\n",
    "----------\n",
    "STEP: 3\n",
    "Training Loss: 4.307155265608798\n",
    "Test Loss: 13.901616519936255\n",
    "----------\n",
    "STEP: 4\n",
    "Training Loss: 0.8664094322811013\n",
    "Test Loss: 3.973589613149072\n",
    "----------\n",
    "STEP: 5\n",
    "Training Loss: 1.475896230765925\n",
    "Test Loss: 4.674988641481719\n",
    "----------\n",
    "STEP: 6\n",
    "Training Loss: 2.440370214571568\n",
    "Test Loss: 2.0451115558223574\n",
    "----------\n",
    "STEP: 7\n",
    "Training Loss: 0.882557069689214\n",
    "Test Loss: 5.096213680944408\n",
    "----------\n",
    "STEP: 8\n",
    "Training Loss: 0.018436663839698046\n",
    "Test Loss: 3.3981814430053627\n",
    "----------\n",
    "STEP: 9\n",
    "Training Loss: 1.6968265521501424\n",
    "Test Loss: 2.6162523317611956\n",
    "----------\n",
    "STEP: 10\n",
    "Training Loss: 0.24230267188769136\n",
    "Test Loss: 3.4301517522897385\n",
    "\n",
    "... trained 10 iterations in 31.028324842453003 seconds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1 Evaluation\n",
    "As can be seen from the outputs, the gradient steps are performing correctly with the log loss improving on each iteration. In this case the training occurs over the first 10 batches and testing is performed on batches 50-59. The training loss unsurprisingly converges a lot more than the test loss due to the presence of the most recent gradient step being performed on the training data that was just tested. A suggestion would be to add regularization, which we tried, but it didn't improve performance all that much, which is similar to the conclusions noted in the paper this model is based off of. It should be noted that the losses shown in the saved text above are for a locally run model and performance here is particularly bad because there are as many as 200,000 unique features and only 10000 features are used for the hashing. Moreover, the number of latent vectors is set to 2, which is fairly low. The output from the cell contains 50000 features and performs a lot better.\n",
    "\n",
    "The example above is on a 5000 instance training set, however, the full dataset was evaluated similarly broken into 100 equal blocks. However, training on just one of the blocks (450k instances) took close to 3 hours with the following computational set up:\n",
    " - 1 n1-standard driver \n",
    " - 6 n1-standard workers\n",
    "     - 8 cores each\n",
    "     - 30GB of RAM\n",
    "\n",
    "That said, it was calculating logloss that took close to an hour each time. It was determined that our implementation was too computational inefficient to continue to attempt on the full dataset and we decided to pivot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image (3).png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2\n",
    "A second implementation was devised that would split the data into separate partitions, performing stochastic gradient descent locally before collecting and summing all gradients at the driver. The hope here was that having the model coefficients locally would enable SGD, but also reduce the amount of emitting done by the mappers, improving performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "n_features = 10000\n",
    "n_fields = 39\n",
    "eta = 0.3\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(n_fields)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)\n",
    "\n",
    "def update_gradient(x, w_old, w_new, G, reg_c = 0.1):\n",
    "    \"\"\"Update the gradient of a FFM\n",
    "    \n",
    "    Args:\n",
    "        x: the input row from the RDD, assume that it has been hashed and that \n",
    "            the first entry is the label the update takes values from w_old and \n",
    "            updates w_new. The format of x is [label, h_1, ..., h_F]\n",
    "            where h_i is the hashed value of the field i and F is the total number of fields.\n",
    "        w_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        w_new: the new coefficient matrix, this will be mutated/updated within this function\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \n",
    "    Implements steps from Algorithm 1 (lines 5 - 11) in paper \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "    \"\"\"\n",
    "    eta = 0.3\n",
    "    y = x[0]\n",
    "    features = x[1:]\n",
    "    # Keep a mapping from the hashed value back to the field it came from\n",
    "    w_v = w_old.value\n",
    "#     D = {h:i for i, h in enumerate(H)}\n",
    "    kappa = -int(y)/(1 + np.exp(int(y)*phi(features, w_v)))\n",
    "    # Iterate through the features and update the gradient, use D for lookups\n",
    "    for f1 in range(len(features) - 1):\n",
    "        j1 = features[f1]\n",
    "        if not j1:\n",
    "            continue\n",
    "            \n",
    "        for f2 in range(1, len(features)):\n",
    "            j2 = features[f2]\n",
    "            if not j2:\n",
    "                continue         \n",
    "            \n",
    "            g1 = kappa*w_v[j2,f1,:] + reg_c*w_v[j1, f2, :]\n",
    "            g2 = kappa*w_v[j1,f2,:] + reg_c*w_v[j2, f1, :]\n",
    "#             G[j1,f2, :] += g1**2\n",
    "#             G[j2,f1, :] += g2**2\n",
    "            w_new[j1,f2,:] = w_v[j1,f2,:] - eta * g1 # eta*g1/np.sqrt(G[j1,f2,:])\n",
    "            w_new[j2,f1,:] = w_v[j2,f1,:] - eta * g2 # eta*g2/np.sqrt(G[j2,f1,:])\n",
    "\n",
    "def apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c):\n",
    "    \"\"\"\n",
    "    Implements the sampling part of Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 3 -- 4\n",
    "    \n",
    "    Args:\n",
    "        rdd: The RDD on each partition\n",
    "        W_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    G = np.ones((n_features, n_fields, k))\n",
    "    W_new = np.zeros((n_features, n_fields, k))\n",
    "    for x in rdd: \n",
    "        #= rdd.sample(False, 0.1)\n",
    "        if np.random.uniform() < 0.1:\n",
    "            update_gradient(x, w_old=W_old, w_new=W_new, G = G, reg_c = reg_c)\n",
    "    yield W_new\n",
    "    \n",
    "def run_ffm_train(rdd_hashed, n_features, n_fields, k, n_iters=10, learning_rate=0.1, reg_c=1):\n",
    "    \"\"\"Runs the FFM algorithm and returns the update coefficient matrix. \n",
    "    \n",
    "    \n",
    "    Implements the SGD parallelization in Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 1 - 2\n",
    "    \n",
    "    Args:\n",
    "        rdd_hashed: An RDD of hashed values in the format where each row is of the form\n",
    "            [label, feature_hash_1, feature_hash_2, ..., feature_hash_n]\n",
    "        n_features: The number of features, for example 10**6\n",
    "        n_fields: The number of fields, e.g. 39 for Criteo dataset\n",
    "        k: dimension of the latent vector\n",
    "        n_iters: number of iterations of gradient descent\n",
    "        reg_c: regulization parameter \n",
    "        \n",
    "    Returns:\n",
    "        The returned matrix is of the type W[j,f,k] where the first index is the feature, \n",
    "        the second is the field and the third is the latent component \n",
    "        of the vector. So if there are 1,000,000 features, 39 fields and 10 latent vectors \n",
    "        the return value is a 1,000,000 x 39 x 10 tensor\n",
    "\n",
    "    \"\"\"\n",
    "    W_old = sc.broadcast(np.random.uniform(0, 1/np.sqrt(k), size=(n_features, n_fields, k)))\n",
    "    for i in range(n_iters):\n",
    "        W_new = rdd_hashed.mapPartitions(lambda rdd, w_old=W_old, n_features=n_features, n_fields=n_fields, k=k, reg_c = reg_c: apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c))\\\n",
    "        .sum()\n",
    "        train_loss = log_loss(rdd_hashed, W_new)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        W_old = sc.broadcast(W_new)\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_ffm_train(train_df.rdd , 10000, 39, 2, n_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Training Loss: 51.371761288573545\n",
    "Training Loss: 23.607517955943024\n",
    "Training Loss: 10.84718644190816\n",
    "Training Loss: 5.006273222880837\n",
    "Training Loss: 2.3819632135011095\n",
    "Training Loss: 1.3357208715515054\n",
    "Training Loss: 0.973598711605685\n",
    "Training Loss: 0.8419300960457365\n",
    "Training Loss: 0.7821930373896475\n",
    "Training Loss: 0.7496892202336619\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2 Evaluation\n",
    "The second implementation was similar to the first implementation in that the models tested locally on a subset of 5000 instances worked appropriately with the log loss reducing over each iteration. That said, it struggled with similar constraints as the first in that it simply wasn't performany enough. Even on much smaller subsets of the full dataset (1%) it still took close to an hour when run on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future FFM Work\n",
    "\n",
    "Beyond the two main implementations above, the team read some other literature to better understand how to boost performance of the model. First, the libFFM implementation that won the kaggle competition was compiled and run on a 1% subset of the dataset, which it was able to complete in some minutes. This prompted to the team to consider compiling the C++ library in the cloud and creating a python wrapper to call out to the libFFM implementation, however, it wasn't proving to be feasible in the given time constraints. Further, the team came across another paper from Criteo that stated their distributed solution takes almost 21 hours to run on a much more powerful setup. (Source: https://arxiv.org/pdf/1701.04099.pdf)\n",
    "\n",
    "After noting that performance of numpy arrays and pandas are close to that of C++. Another possible solution, yet to be attempted, is to install pyarrows and enable arrows in the spark environment. This allows writing of pandas UDFs, which allows for vectorization of method across all rows at once that can greatly improve performance. That said, the dataset is too large to hold in memory requiring the data to be chunked into smaller pieces that are run on their own partition. If the team were to continue with the project, this would be our next most likely step to try if an FFM implementation was desired.\n",
    "\n",
    "In realizing our model was too computationally intensive for this task, the team decided to pivot onto using logistic regression on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==============================================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We will use the logistic regression from Spark ML to build a model using the 80% of the training set, and calculate the average log loss on the 20% of the heldout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|          139|         null|            2|         4828|           28|           11|            2|           28|          null|             1|             0|             2|      05db9164|      d833535f|      77f2f2e5|      d16679b9|      4cf72387|      fe6b92e5|      9ea2e0f0|      0b153874|      a73ee510|       43b7a3fa|       2dad6ba2|       9f32b866|       47cb697a|       07d13a8f|       943169c2|       31ca40b6|       e5ba7672|       281769c2|            ***|            ***|       dfcfc3fa|       c9d4222a|       32c7478e|       aee52b6f|            ***|            ***|\n",
      "|    0|         null|            3|            7|            5|        46272|          314|            0|            5|           39|          null|             0|          null|             5|      05db9164|      9b25e48b|           ***|           ***|      25c83c98|           ***|      1401df5c|      0b153874|      7cc72ec2|       a4c398f8|       f697a983|            ***|       e5643e9a|       07d13a8f|       054ebda1|            ***|       e5ba7672|       7d8c03aa|       1d04f4a4|       5840adea|            ***|            ***|       c7dc6720|       48b11258|       ea9a246c|       b4a4615f|\n",
      "|    0|            3|            2|         null|            1|            0|            1|           42|           16|          390|             1|            18|            12|             0|      8cf07265|      38d50e09|      50b60277|      64607668|      25c83c98|      fbad5c96|      97f8984c|      0b153874|      a73ee510|       3b08e48b|       0a41d304|       c65541d1|       11aa1375|       cfef1c29|       71c460b5|       ff8dfb8b|       8efede7f|       582152eb|       21ddcdc9|       5840adea|       0c8b6f3b|            ***|       55dd3565|       51b4beaa|       001f3601|       1d5d3a57|\n",
      "|    0|            0|            0|         null|            7|         2831|         null|            0|           32|            1|             0|             0|             0|            32|      68fd1e64|      7cd19acc|      ad4b77ff|      d16679b9|      25c83c98|      7e0ccccf|      5c6e106b|      0b153874|      a73ee510|       2c443d6f|       5420373c|       a2f4e8b5|       ab160bba|       1adce6ef|       5e3f66c0|       89052618|       e5ba7672|       416587b1|            ***|            ***|       d4703ebd|            ***|       423fab69|       aee52b6f|            ***|            ***|\n",
      "|    0|         null|            4|            4|            1|            8|         null|            0|            1|            1|          null|             0|          null|             1|      05db9164|      bc478804|           ***|      13508380|      30903e74|      7e0ccccf|      038685fb|      5b392875|      a73ee510|       612f8227|       ed04080f|            ***|       5579ddc3|       07d13a8f|       0af7c64c|            ***|       1e88c74f|       65a2ac26|       423fab69|       b1252a9d|            ***|            ***|       32c7478e|       45ab94c8|       001f3601|       c84c4aec|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|            4|         null|            1|            8|            3|            2|            1|            3|          null|             1|             0|             1|      05db9164|      d833535f|      77f2f2e5|      d16679b9|      4cf72387|      fe6b92e5|      9ea2e0f0|      0b153874|      a73ee510|       43b7a3fa|       2dad6ba2|       9f32b866|       47cb697a|       07d13a8f|       943169c2|       31ca40b6|       e5ba7672|       281769c2|            ***|            ***|       dfcfc3fa|       c9d4222a|       32c7478e|       aee52b6f|            ***|            ***|\n",
      "|    0|         null|            1|            2|            1|           10|            5|            0|            1|            3|          null|             0|          null|             1|      05db9164|      9b25e48b|           ***|           ***|      25c83c98|           ***|      1401df5c|      0b153874|      7cc72ec2|       a4c398f8|       f697a983|            ***|       e5643e9a|       07d13a8f|       054ebda1|            ***|       e5ba7672|       7d8c03aa|       1d04f4a4|       5840adea|            ***|            ***|       c7dc6720|       48b11258|       ea9a246c|       b4a4615f|\n",
      "|    0|            1|            1|         null|            1|            0|            1|            3|            2|            5|             1|             2|             2|             0|      8cf07265|      38d50e09|      50b60277|      64607668|      25c83c98|      fbad5c96|      97f8984c|      0b153874|      a73ee510|       3b08e48b|       0a41d304|       c65541d1|       11aa1375|       cfef1c29|       71c460b5|       ff8dfb8b|       8efede7f|       582152eb|       21ddcdc9|       5840adea|       0c8b6f3b|            ***|       55dd3565|       51b4beaa|       001f3601|       1d5d3a57|\n",
      "|    0|            0|            0|         null|            2|            7|         null|            0|            3|            1|             0|             0|             0|             3|      68fd1e64|      7cd19acc|      ad4b77ff|      d16679b9|      25c83c98|      7e0ccccf|      5c6e106b|      0b153874|      a73ee510|       2c443d6f|       5420373c|       a2f4e8b5|       ab160bba|       1adce6ef|       5e3f66c0|       89052618|       e5ba7672|       416587b1|            ***|            ***|       d4703ebd|            ***|       423fab69|       aee52b6f|            ***|            ***|\n",
      "|    0|         null|            1|            1|            1|            2|         null|            0|            1|            1|          null|             0|          null|             1|      05db9164|      bc478804|           ***|      13508380|      30903e74|      7e0ccccf|      038685fb|      5b392875|      a73ee510|       612f8227|       ed04080f|            ***|       5579ddc3|       07d13a8f|       0af7c64c|            ***|       1e88c74f|       65a2ac26|       423fab69|       b1252a9d|            ***|            ***|       32c7478e|       45ab94c8|       001f3601|       c84c4aec|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(1000000,[27171,3...|\n",
      "|    0|(1000000,[27171,5...|\n",
      "|    0|(1000000,[6135,99...|\n",
      "|    0|(1000000,[27171,1...|\n",
      "|    0|(1000000,[36554,7...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#similar feature engineering process to FFM\n",
    "\n",
    "threshold = 10\n",
    "\n",
    "train_parquet_MD = train_parquet\n",
    "\n",
    "for col in cateFieldNames:\n",
    "    valuesToKeep = train_parquet.groupBy(col).count().filter(f\"count >= {threshold}\").select(col)\n",
    "    valuesToKeep = valuesToKeep.withColumn(\"_\"+col, train_parquet[col])\n",
    "    valuesToKeep = valuesToKeep.drop(col)\n",
    "\n",
    "    train_parquet_MD = train_parquet_MD.join(F.broadcast(valuesToKeep), train_parquet_MD[col] == valuesToKeep[\"_\"+col], 'leftouter')\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.when(F.col(\"_\"+col).isNull(), \"***\").otherwise(F.col(\"_\"+col)))\n",
    "    train_parquet_MD = train_parquet_MD.drop(\"_\"+col)\n",
    "\n",
    "train_parquet_MD.show(5)\n",
    "\n",
    "for col in intFieldNames:\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.floor(F.log(F.col(col)+F.lit(2))))\n",
    "\n",
    "train_parquet_MD.show(5)\n",
    "\n",
    "n_features = 1000000 #hash to a set of 1 million feature since it's not field aware and we have a total of 1085739 feature according to EDA\n",
    "\n",
    "\n",
    "hasher = FeatureHasher(inputCols=intFieldNames + cateFieldNames, outputCol=\"features\")\n",
    "hasher.setCategoricalCols(intFieldNames)\n",
    "hasher.setNumFeatures(n_features)\n",
    "\n",
    "df_hashed = hasher.transform(train_parquet_MD).select(\"label\", \"features\").cache()\n",
    "\n",
    "df_hashed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a 20%, 80% split\n",
    "heldOutDF, trainDF = df_hashed.randomSplit([0.2,0.8], seed = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a logistic regression model with L2 regularization and regularization parameter = 1\n",
    "blor = LogisticRegression(regParam=0.1)\n",
    "blorModel = blor.fit(trainDF)\n",
    "result = blorModel.transform(heldOutDF).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateLogLoss(df, probCol='probability', labelCol='label'):\n",
    "    def logloss(probVector, label):\n",
    "        \"\"\"\n",
    "        udf which helps calculate the log loss of test set\n",
    "\n",
    "        Args:\n",
    "            probVector - a dense vector with first number as the predicted probability of label being 0\n",
    "            label      - the true label of an instance\n",
    "        Returns:\n",
    "            the log loss of a instance\n",
    "        \"\"\"\n",
    "        import math\n",
    "        # get the predicted probability of label = 0\n",
    "        probability = probVector[0]\n",
    "        # convert label to a float for calculation\n",
    "        label = float(label)\n",
    "\n",
    "        return -1.0 * (label*math.log(1-probability)+(1.0-label)*math.log(probability))\n",
    "    \n",
    "    # create an udf for dataframe\n",
    "    cal_logloss = F.udf(logloss, types.DoubleType())\n",
    "    \n",
    "    # calculate the average log loss in the input dataframe\n",
    "    meanLoss = df.withColumn(\"loss\", cal_logloss('probability','label')).agg(F.avg(\"loss\")).collect()\n",
    "    \n",
    "    return meanLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|(1000000,[1,63097...|[3.04323408301368...|[0.95448952211045...|       0.0|\n",
      "|    0|(1000000,[13,1510...|[2.82499730621538...|[0.94401177856229...|       0.0|\n",
      "|    0|(1000000,[13,2717...|[3.14050066054781...|[0.95853278536500...|       0.0|\n",
      "|    0|(1000000,[21,1643...|[3.53950764318955...|[0.97179121816449...|       0.0|\n",
      "|    0|(1000000,[29,2717...|[3.30615615511194...|[0.96463940028366...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(avg(loss)=0.08608094434475863)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateLogLoss(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Reduce Paradigm\n",
    "\n",
    "Our FFM algorithm is implemented by iteratively using a series of map-reduce tasks. At the beginning of each iteration, we  broadcast the current weight matrix of latent vectors to all worker nodes. The first map-reduce task then begins, with the mapper reading the broadcast latent vectors to calculate the predicted click-through-rate for each impression. A second map-reduce task is then used to update the weight matrix (using gradient descent methods) for the next iteration of the algorithm. In this second map-reduce task, the mapper is used to compute the sub-gradients on each partition of the data. The reduce phase of then aggregates the sub-gradients and generates a single gradient update for our weight matrix of latent vectors. This is closely related to the approach of using Map-Reduce to run OLS in Homework 4 in parallel.\n",
    "\n",
    "### Gradient Descent - Batch vs Stochastic\n",
    "\n",
    "As previously noted, Juan, Zhuang, Chin, & Lin utilize stochastic gradient descent for their implementation of FFM. They are able to do this by making use of the Hogwild algorithm to parallelize the algorithm across multiple cores with shared memory. This was not possible for our implementation using distributed computation without shared memory in Pyspark. In order to implement stochastic gradient descent, we would have had to repeatedly broadcast a massive array of coefficients and gradients, which is computationally infeasible. As such, we opted instead to implement a parallelized version of batch gradient descent to reduce the need for large shuffles and network communcation. This also prevented potential issues with synchronization of updates.\n",
    "\n",
    "\n",
    "### One-Hot Encoding, Sparse Data\n",
    "\n",
    "Given the raw structure of our data, we were faced with several decisions about how to represent and engineer our features. With many categorical variables, one possibility was to use one-hot encoding, essentially creating indicator variables for every feature within every category. Yet as seen in our EDA, each categorical variable had an enormous number of distinct features. A one-hot encoded representation of the dataset thus would have been very large, and very sparse. Instead, we elected to represent our data using feature hashing, as Juan, Zhuang, Chin, and Lin. With this method, each feature is simply encoded as `(field_index,feature_index)` since all values are one because integer features are all so treated as categorical after log transfomation. Therefore, instead of a size of data insteading of being feature number times number of instances, it is 39 times the number of instances. This leads to a much more efficient use of space for our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
