{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Team 15\n",
    "- Erik Hou, Noah Pflaum, Connor Stern, Anu Yadav\n",
    "- Fall 2019, section 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fp_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>hw5_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fb97d390a20>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t1\t1\t5\t0\t1382\t4\t15\t2\t181\t1\t2\t\t2\t68fd1e64\t80e26c9b\tfb936136\t7b4723c4\t25c83c98\t7e0ccccf\tde7995b8\t1f89b562\ta73ee510\ta8cd5504\tb2cb9c98\t37c9c164\t2824a5f6\t1adce6ef\t8ba8b39a\t891b62e7\te5ba7672\tf54016b9\t21ddcdc9\tb1252a9d\t07b5194c\t\t3a171ecb\tc5c50484\te8b83407\t9727dd16\n"
     ]
    }
   ],
   "source": [
    "!head -1 data/dac/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawRDD = sc.textFile('data/dac/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF = spark.read.text('data/dac/train.txt')\n",
    "#rawDF = rawRDD.toDF(rawRDD, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDF.write.parquet('data/adclicks.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetDF = spark.read.parquet('data/adclicks.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF = spark.read.parquet('data/adclicks.parquet/part-00000-39ed38a6-7cfa-463c-9076-9a79a698c2fc-c000.snappy.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "555493"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(row):\n",
    "    variables = row.split('\\t')\n",
    "    click = variables[0]\n",
    "    yield click\n",
    "#sampleDF.flatMap(parse).take(1)\n",
    "sampleDF.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|               value|\n",
      "+-------+--------------------+\n",
      "|  count|              555493|\n",
      "|   mean|                null|\n",
      "| stddev|                null|\n",
      "|    min|0\t\t-1\t\t\t\t\t\t\t\t\t\t\t\t...|\n",
      "|    max|1\t99\t130\t3\t2\t0\t0\t...|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because an online advertisement can either be clicked ($response = 1$) or not ($response = 0$) , Click-Through-Rate (CTR) Prediction is generally treated as a logistic regression problem. For any set of features, we calculate some value $s$, and perform the logit transformation to yield our CTR prediction:\n",
    "$$CTR = \\frac{1}{1+e^{-s}} $$\n",
    "\n",
    "There are several methods we can use to estimate $s$, each with its own benefits and drawbacks. Typical implementations include a linear model, a degree-2 polynomial mapping, a factorization machine, and a field-aware factorization machine. \n",
    "\n",
    "#### Linear Model\n",
    "In a linear model, the algorithm learns a weight for every given feature. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) =\\textbf{w}^T \\textbf{x}  =\\sum_{j \\epsilon C_1}w_jx_j$$\n",
    "where $\\textbf{w}$ is the learned model, $\\textbf{x}$ is the data observation, and $C_1$ is the non-zero elements in $\\textbf{x}$.\n",
    "\n",
    "As an example, suppose we had an impression where the advertiser is Coca-cola and the publisher is Netflix. The linear model with these features would be: \n",
    "$$s = w_{Coca-cola} + w_{Netflix}$$\n",
    "where $w_{Coca-cola}$ and $w_{Netflix}$ are the learned weights for the corresponding features. (We note that $x_{Coca-cola}$ and $x_{Netflix}$ would be equal to 1 in this case, as they are both indicator variables.)\n",
    "\n",
    "This model is simple and efficient, yet it does not allow for interactive effects between features. For example, Coca-cola may have a higher CTR with Netflix than another publisher. A linear model is unable to learn this type of information, as it essentially learns the \"average effect\" of each feature.\n",
    "\n",
    "#### Degree-2 Polynomial Mapping\n",
    "The simplest way to learn the effect of the \"feature conjunction\" described above (in the case where a particular advertiser may have a higher CTR with one publisher compared to others) is to use a degree-2 polynomial mapping. In this model, the algorithm learns an additional weight for each feature pair. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} w_{j_1,j_2}x_{j_1}x_{j_2}$$\n",
    "where $C_2$ is the pairwise combination of non-zero elements in $\\textbf{x}$.\n",
    "\n",
    "\n",
    "In our example of an impression with the advertiser Coca-cola and the publisher Netflix, the model would be:\n",
    "$$s = w_{Coca-cola} + w_{Netflix} + w_{Coca-cola,Netflix}$$\n",
    "We again note the $x_i$ values are all equal to one in this case, as all the variables in the model are indicator variables.\n",
    "\n",
    "Unfortunately, this model does not handle sparse datasets well. If we have 0 impressions of the advertiser Pepsi with the publisher Pandora, the model prediction will be trivial as no weight was learned for this feature combination. The model is also susceptible to overfitting, as it generates unreliable predictions for feature combinations with a very small number of impressions.\n",
    "\n",
    "#### Factorization Machine\n",
    "Factorization Machines (FM) begin to provide us a solution to the problem of sparse datasets. Here the model learns a latent vector, rather than an explicit weight, for each feature. The model formulation is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1}, \\textbf{w}_{j_2} \\rangle x_{j_1}x_{j_2}$$\n",
    "where $\\textbf{w}_{j_1}$ and $\\textbf{w}_{j_2}$ are two learned latent vectors of length $k$ (some user-defined parameter).\n",
    "\n",
    "Returning to our example of an impression with Coca-cola and Netflix, the FM model would be:\n",
    "$$s = w_{Coca-cola} + w_{Netflix} + v_{Coca-cola} \\cdot v_{Netflix}$$\n",
    "where $v_{Netflix}, v_{Coca-cola} \\epsilon {\\rm I\\!R}^k$.\n",
    "\n",
    "This allows the model to learn the latent vectors for each feature based on all the data points for that feature, and these latent vectors can be used to predict the CTR for unobserved feature combinations (such as the Pepsi-Pandora combination described above). \n",
    "\n",
    "#### Field-Aware Factorization Machine\n",
    "Let us suppose we had an additional piece of information about our impression-- that the gender of the individual whom the ad is served to is male. For ease of communication, we will describe the publisher (P), advertiser (A), and gender (G) as \"fields,\" and their values (or instances) of Netflix, Coca-cola, and Male as \"features.\" \n",
    "Using an FM model, the latent vector for $v_{Netflix}$ will now be used twice: once to calculate the effect of $P \\times A$ (the inner product with $v_{Coca-cola}$ and once to calculate the effect of $P \\times G$ (the inner product with $v_{Male}$).\n",
    "\n",
    "The problem becomes clearer as we begin to write out the first few terms of the traditional FM model formulation, we would have the following:\n",
    "\n",
    "$$s =  v_{Netflix} \\cdot v_{Coca-cola} + v_{Netflix} \\cdot v_{Male} + ... $$\n",
    "\n",
    "The same latent vector $v_{Netflix}$ is used to learn both the latent effect of (Netflix, Coca-cola), or $P \\times A$,  and the latent effect of (Netflix, Male), or $P \\times G$. This is too restrictive and unrealistic, as the latent effect for publisher with advertiser could be very different from the latent vector for publisher and gender.\n",
    "\n",
    "Field-Aware Factorization Machines (FFM) provide a solution to this problem by introducing the flexibility to learn multiple latent vectors for each feature. In the context of the above example, the FFM model will learn two separate latent vectors for Netflix: $v_{Netflix, A}$, which will be used for the calculation of $P \\times A$ (to learn the latent effect of Netflix with a given advertiser), and $v_{Netflix, G}$. $v_{Netflix, A}$, for the calculation of $P \\times G$ (to learn the latent effect of Netflix with a given gender).\n",
    "\n",
    "Writing out the first few terms of the FFM model formulation for this example gives us:\n",
    "\n",
    "$$s = v_{Netflix, A} \\cdot v_{Coca-cola, P} + v_{Netflix, G} \\cdot v_{Male, P} + ... $$\n",
    "\n",
    "Now, the latent effect of (Netflix, Coca-cola), is learned by using the latent vector $v_{Netflix, A}$, since Coca-cola belongs to the advertiser field (A). The latent effect of (Netflix, Male), by contrast, is learned using a different latent vector, $v_{Netflix, G}$, since Male belongs to the gender field (G). In this way, FFM splits the latent factors for $P \\times A$ and $P \\times G$, something the traditional Factorization Machine is unable to do. \n",
    "\n",
    "The full model formulation is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$\n",
    "\n",
    "#### Optimization - Function from papers is different from Log-loss?????\n",
    "\n",
    "As part of a logistic regression problem, the goal is to find the set of parameters that minimize the log-loss function. Specifically, the optimization problem is:\n",
    "\n",
    "$$ \\min_{\\textbf{w}} \\sum_{i=1}^m \\left( log(1+exp(-y_i\\phi(\\textbf{w},\\textbf{x}_i)) + \\frac{\\lambda}{2}\\|\\textbf{w}\\|^2 \\right)$$\n",
    "\n",
    "where $$\\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$\n",
    "\n",
    "$m$ is the number of impressions, and  $\\lambda$ is our regularization parameter.\n",
    "\n",
    "We solve this optimization problem using stochastic gradient methods.\n",
    "\n",
    "*OR*\n",
    "\n",
    "The log-loss function is defined by:\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i} + (1-y_i) \\cdot log_e(1-\\hat{y_i}] $$\n",
    "\n",
    "where $n$ is the number of impressions, $y_i$ is the true CTR of impression $i$, and $\\hat{y_i}$ is the predicted CTR of impression $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example: FFM in action\n",
    "\n",
    "To further demonstrate how FFM works in action, we consider the following example with an artificially created data set:\n",
    "\n",
    "<table>\n",
    "<th>Response</th>\n",
    "<th>Publisher</th>\n",
    "<th>Advertiser</th>\n",
    "<th>Gender</th>\n",
    "<tr><td>1</td><td>Netflix</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>0</td><td>Spotify</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>0</td><td>Facebook</td><td>Gatorade</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Spotify</td><td>Coca-cola</td><td>Male</td></tr>\n",
    "<tr><td>1</td><td>Facebook</td><td>Coca-cola</td><td>Female</td></tr>\n",
    "<tr><td>0</td><td>Facebook</td><td>Pepsi</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Netflix</td><td>Gatorade</td><td>Female</td></tr>\n",
    "</table>\n",
    "\n",
    "Note that we have three fields-- Publisher, Advertiser, and Gender. The Publisher and Advertiser fields each have three different features (Netflix, Spotify, and Facebook for Publisher and Pepsi, Gatorade, and Coca-cola for Advertiser), while the Gender field has two features (Male and Female). \n",
    "\n",
    "1) We define k (the length of our latent vectors) to be 5.\n",
    "\n",
    "2) We randomly initialize our latent vectors $\\textbf{v}$. These latent vectors will be represented as columns below.\n",
    "\n",
    "For the Publisher field, each feature will have two different latent vectors--one corresponding to Advertiser (A), and one corresponding to Gender (G):\n",
    "\n",
    "<table>\n",
    "<th>$\\textbf{v}_{Netflix,A}$</th>\n",
    "<th>$\\textbf{v}_{Netflix,G}$</th>\n",
    "<th>$\\textbf{v}_{Spotify,A}$</th>\n",
    "<th>$\\textbf{v}_{Spotify,G}$</th>\n",
    "<th>$\\textbf{v}_{Facebook,A}$</th>\n",
    "<th>$\\textbf{v}_{Facebook,G}$</th>\n",
    "<tr><td>-0.1</td><td>0.5</td><td>0.7</td><td>0.3</td><td>0.3</td><td>0</td></tr>\n",
    "<tr><td>0.3</td><td>-0.5</td><td>-0.4</td><td>0.2</td><td>-0.1</td><td>0.4</td></tr>\n",
    "<tr><td>0.8</td><td>0.2</td><td>0</td><td>0.6</td><td>0.1</td><td>-0.5</td></tr>\n",
    "<tr><td>0</td><td>-0.1</td><td>0.1</td><td>-0.9</td><td>0.2</td><td>-0.1</td></tr>\n",
    "<tr><td>-0.6</td><td>0.9</td><td>0.1</td><td>0.4</td><td>0.7</td><td>0.9</td></tr>\n",
    "<table>\n",
    "    \n",
    "The Advertiser field will similarly have two different latent vectors for each feature--one for Publisher (P), and one for Gender (G):\n",
    "<table>\n",
    "<th>$\\textbf{v}_{Pepsi,P}$</th>\n",
    "<th>$\\textbf{v}_{Pepsi,G}$</th>\n",
    "<th>$\\textbf{v}_{Gatorade,P}$</th>\n",
    "<th>$\\textbf{v}_{Gatorade,G}$</th>\n",
    "<th>$\\textbf{v}_{Coca-cola,P}$</th>\n",
    "<th>$\\textbf{v}_{Coca-cola,G}$</th>\n",
    "<tr><td>0.2</td><td>-0.4</td><td>0.5</td><td>0.6</td><td>0</td><td>0.7</td></tr>\n",
    "<tr><td>0.1</td><td>0</td><td>0.3</td><td>-0.6</td><td>-0.1</td><td>-0.2</td></tr>\n",
    "<tr><td>-0.7</td><td>0</td><td>0.1</td><td>0.3</td><td>-0.3</td><td>-0.1</td></tr>\n",
    "<tr><td>0.5</td><td>0.3</td><td>0.1</td><td>0.1</td><td>-0.4</td><td>-0.5</td></tr>\n",
    "<tr><td>0.2</td><td>-0.5</td><td>0</td><td>-0.9</td><td>0.8</td><td>0.3</td></tr>\n",
    "<table>\n",
    "    \n",
    "The Gender field will also have two different latent vectors per feature (one for Publisher (P), one for Advertiser (A)): \n",
    "<table>\n",
    "<th>$\\textbf{v}_{Male,P}$</th>\n",
    "<th>$\\textbf{v}_{Male,A}$</th>\n",
    "<th>$\\textbf{v}_{Female,P}$</th>\n",
    "<th>$\\textbf{v}_{Female,A}$</th>\n",
    "<tr><td>0.6</td><td>0.3</td><td>0.0</td><td>-0.5</td></tr>\n",
    "<tr><td>-0.8</td><td>-0.4</td><td>0.3</td><td>-0.4</td></tr>\n",
    "<tr><td>0</td><td>0.6</td><td>0.6</td><td>-0.2</td></tr>\n",
    "<tr><td>0.8</td><td>0.7</td><td>-0.9</td><td>0</td></tr>\n",
    "<tr><td>-0.4</td><td>-0.1</td><td>-0.2</td><td>0.9</td></tr>\n",
    "<table>\n",
    "\n",
    "3) We calculate $s$ for each impression using these weight vectors.\n",
    "\n",
    "For each impression, we would use the formula\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$    \n",
    "to generate our value of $s$. Fully expanded, this would be:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "s =& \\langle\\textbf{v}_{Netflix,A} \\cdot \\textbf{v}_{Pepsi,P}\\rangle x_{Netflix}x_{Pepsi} + \\langle\\textbf{v}_{Netflix,A} \\cdot \\textbf{v}_{Coca-cola,P}\\rangle x_{Netflix}x_{Coca-cola} + \\langle\\textbf{v}_{Netflix,A} \\cdot \\textbf{v}_{Gatorade,P}\\rangle x_{Netflix}x_{Gatorade} \\\\\n",
    "&+ \\langle\\textbf{v}_{Netflix,G} \\cdot \\textbf{v}_{Male,P}\\rangle x_{Netflix}x_{Male} + \\langle\\textbf{v}_{Netflix,G} \\cdot \\textbf{v}_{Female,P}\\rangle x_{Netflix}x_{Female} + \\langle\\textbf{v}_{Spotify,A} \\cdot \\textbf{v}_{Pepsi,P}\\rangle x_{Spotify}x_{Pepsi} \\\\\n",
    "&+ \\langle\\textbf{v}_{Spotify,A} \\cdot \\textbf{v}_{Coca-cola,P}\\rangle x_{Spotify}x_{Coca-cola} + \\langle\\textbf{v}_{Spotify,A} \\cdot \\textbf{v}_{Gatorade,P}\\rangle x_{Spotify}x_{Gatorade} + \\langle\\textbf{v}_{Spotify,G} \\cdot \\textbf{v}_{Male,P}\\rangle x_{Spotify}x_{Male} \\\\\n",
    "&+ \\langle\\textbf{v}_{Spotify,G} \\cdot \\textbf{v}_{Female,P}\\rangle x_{Spotify}x_{Female} + \\langle\\textbf{v}_{Facebook,A} \\cdot \\textbf{v}_{Pepsi,P}\\rangle x_{Facebook}x_{Pepsi} + \\langle\\textbf{v}_{Facebook,A} \\cdot \\textbf{v}_{Coca-cola,P}\\rangle x_{Facebook}x_{Coca-cola} \\\\\n",
    "&+ \\langle\\textbf{v}_{Facebook,A} \\cdot \\textbf{v}_{Gatorade,P}\\rangle x_{Facebook}x_{Gatorade} + \\langle\\textbf{v}_{Facebook,G} \\cdot \\textbf{v}_{Male,P}\\rangle x_{Facebook}x_{Male} + \\langle\\textbf{v}_{Facebook,G} \\cdot \\textbf{v}_{Female,P}\\rangle x_{Facebook}x_{Female} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For each impression, we are only concerned with the terms containing the features represented in the impression, since these $x$ values are equal to one, while all other terms become zero.\n",
    "    \n",
    "Our first impression (Netflix, Pepsi, Male) would therefore give us:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s &= \\langle\\textbf{v}_{Netflix,A} \\cdot \\textbf{v}_{Pepsi,P}\\rangle + \\langle\\textbf{v}_{Netflix,G} \\cdot \\textbf{v}_{Male,P}\\rangle + \\langle\\textbf{v}_{Pepsi,G} \\cdot \\textbf{v}_{Male,A}\\rangle\\\\\n",
    "&= (-0.1*0.2 + 0.3*0.1 + 0.8*-0.7 + 0*0.5 + -0.6*0.2) + (0.5*0.6 + -0.5*-0.8 + 0.2*0 + -0.1*0.8 + 0.9*-0.4) + (-0.4*0.3 + 0*-0.4 + 0*0.6 + 0.3*0.7 + -0.5*-0.1)  \\\\\n",
    "&= -0.67 + 0.26 + 0.14 \\\\\n",
    "&= -0.27\n",
    "\\end{aligned}\n",
    "$$\n",
    "Plugging this value into the logit transformation, we get a predicted CTR of:\n",
    "$$\\frac{1}{1+e^{-s}} = \\frac{1}{1+e^{0.27}} \\approx 0.43$$\n",
    "    \n",
    "Since this value is closer to 0 than 1, we would ultimately predict a non-click (incorrectly). However, this predicted value is vital for our log-loss calculation, which we will see momentarily.\n",
    "    \n",
    "The following table shows the results of the calculations for all seven impressions:\n",
    "\n",
    "<table>\n",
    "<th>Impression</th>\n",
    "<th>$s$</th>\n",
    "<th>Predicted CTR</th>\n",
    "<th>Correct?</th>\n",
    "<tr><td>(Netflix, Pepsi, Male)</td><td>-0.27</td><td>$\\approx 0.43$</td><td>No</td></tr>\n",
    "<tr><td>(Spotify, Pepsi, Male)</td><td>-0.55</td><td>$\\approx 0.37$</td><td>Yes</td></tr>\n",
    "*<tr><td>(Facebook, Gatoriade, Female)</td><td>-1.05</td><td>$\\approx 0.26$</td><td>Yes</td></tr>\n",
    "<tr><td>(Spotify, Coca-cola, Male)</td><td>-0.93</td><td>$\\approx 0.28$</td><td>No</td></tr>\n",
    "<tr><td>(Facebook, Coca-cola, Female)</td><td>0.05</td><td>$\\approx 0.51$</td><td>Yes</td></tr>\n",
    "<tr><td>(Facebook, Pepsi, Female)</td><td>-0.30</td><td>$\\approx 0.43$</td><td>Yes</td></tr>\n",
    "<tr><td>(Netflix, Gatorade, Female)</td><td>-0.93</td><td>$\\approx 0.28$</td><td>No</td></tr>\n",
    "<table>\n",
    "\n",
    "    \n",
    "4) We calculate the log-loss of the model, using the equation:\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i} + (1-y_i) \\cdot log_e(1-\\hat{y_i}] $$\n",
    "Using our values from above, we have:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "LogLoss =& (1 \\cdot log_e(0.43)) + (1 \\cdot log_e(0.63)) + (1 \\cdot log_e(0.74)) + \\\\\n",
    "&+ (1 \\cdot log_e(0.28)) + (1 \\cdot log_e(0.51)) + (1 \\cdot log_e(0.57)) \\\\\n",
    "&+ (1 \\cdot log_e(0.28))\\\\\n",
    "&= 0.769\n",
    "\\end{aligned}\n",
    "$$\n",
    "    \n",
    "    \n",
    "* OR Formula from the paper...\n",
    "    \n",
    "$$\\sum_{i=1}^m \\left( log(1+exp(-y_i\\phi(\\textbf{w},\\textbf{x}_i)) + \\frac{\\lambda}{2}\\|\\textbf{w}\\|^2 \\right)$$\n",
    "\n",
    "For simplicity, we ignore the regularization term $\\frac{\\lambda}{2}\\|\\textbf{w}\\|^2$\n",
    "\n",
    "The log-loss then becomes:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "L =& log(1+exp(-y_1 \\cdot s_1) + log(1+exp(-y_2 \\cdot s_2) \\\\\n",
    "&+ log(1+exp(-y_3 \\cdot s_3) + log(1+exp(-y_4 \\cdot s_4) + log(1+exp(-y_5 \\cdot s_5) \\\\\n",
    "&+ log(1+exp(-y_6 \\cdot s_6) + log(1+exp(-y_7 \\cdot s_7)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Using the values given above, we have:    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "L =& log(1+exp(-1 \\cdot -0.27) + log(1+exp(-0 \\cdot -0.55) \\\\\n",
    "&+ log(1+exp(-0 \\cdot -1.05) + log(1+exp(-1 \\cdot -0.93) + log(1+exp(-1 \\cdot 0.05) \\\\\n",
    "&+ log(1+exp(-0 \\cdot -0.30) + log(1+exp(-1 \\cdot -0.93)\n",
    "\\end{aligned}\n",
    "$$    \n",
    "    \n",
    "5) We then would optimize our weights using gradient descent ...\n",
    "\n",
    "We repeat 3-5..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Sources:\n",
    "- https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "- https://www.youtube.com/watch?v=1cRGpDXTJC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
