{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hashlib import sha256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a small sample to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample.txt\n",
    "1\t10\tESPN\tNike\n",
    "1\t15\tESPN\tNike\n",
    "0\t2\tESPN\tGucci\n",
    "1\t10\tESPN\tAdidas\n",
    "1\t10\tESPN\tAdidas\n",
    "0\t3\tVogue\tNike\n",
    "1\t20\tVogue\tGucci\n",
    "0\t5\tVogue\tAdidas\n",
    "1\t50\tNBC\tNike\n",
    "0\t0\tNBC\tGucci\n",
    "0\t4\tNBC\tAdidas\n",
    "0\t4\tNBC\tAdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_RDD = sc.textFile('sample.txt')\n",
    "split_RDD = sample_RDD.map(lambda line: line.split('\\t')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+\n",
      "| _1| _2|   _3|    _4|\n",
      "+---+---+-----+------+\n",
      "|  1| 10| ESPN|  Nike|\n",
      "|  1| 15| ESPN|  Nike|\n",
      "|  0|  2| ESPN| Gucci|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  0|  3|Vogue|  Nike|\n",
      "|  1| 20|Vogue| Gucci|\n",
      "|  0|  5|Vogue|Adidas|\n",
      "|  1| 50|  NBC|  Nike|\n",
      "|  0|  0|  NBC| Gucci|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "+---+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df = split_RDD.toDF()\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_hash(x, modulo=10**6):\n",
    "    \"\"\"\n",
    "    Now we create a function that can be used to hash the features in each observation in the RDD. \n",
    "    We replace the label with 1, -1 and we hash all other features using sha256 \n",
    "    and then we take modulo some power of 10. \n",
    "    \"\"\"\n",
    "\n",
    "    x[0] = 2*int(x[0]) - 1\n",
    "    for i, value in enumerate(x[1:], 1):\n",
    "        h = sha256(\"{i}-{val}\".format(i=i,val=value).encode('ascii'))\n",
    "        hashed_value = int(h.hexdigest(), base=16) \n",
    "        hashed_value_mod = hashed_value % modulo\n",
    "        x[i] = hashed_value_mod\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_hashed = split_RDD.map(lambda x: feature_hash(x, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 16, 4],\n",
       " [1, 7, 16, 4],\n",
       " [-1, 2, 16, 18],\n",
       " [1, 1, 16, 11],\n",
       " [1, 1, 16, 11],\n",
       " [-1, 2, 9, 4],\n",
       " [1, 14, 9, 18],\n",
       " [-1, 12, 9, 11],\n",
       " [1, 18, 23, 4],\n",
       " [-1, 10, 23, 18],\n",
       " [-1, 22, 23, 11],\n",
       " [-1, 22, 23, 11]]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hashed.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FFM:\n",
    "#     def __init__(self, n_features, k = 10, eta = 0.1, reg_c = 0.1):\n",
    "#         self.n_features = n_features\n",
    "#         self.k = k\n",
    "#         self.eta = eta\n",
    "#         self.reg_c = reg_c\n",
    "        \n",
    "        \n",
    "# ffm = FFM(25, k=3)\n",
    "# ffm.n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.sum([np.dot(W[x[i], :], W[x[j], :]) for i in range(len(x) - 1) for j in range(i + 1, len(x))])\n",
    "\n",
    "def kappa(y, features):\n",
    "    return -y/(1 + np.exp(phi(features)))\n",
    "\n",
    "def gradient(x):\n",
    "    y = x[0]\n",
    "    features = x[1:]\n",
    "    k = kappa(y, features)\n",
    "    \n",
    "    gradients = np.zeros(shape=(25, 3))\n",
    "    \n",
    "    for i in range(len(features) - 1):\n",
    "        for j in range(i+1, len(features)):\n",
    "            gradients[features[i]] += k * W[features[j], :] #+ reg_c * W[features[i], :]\n",
    "            \n",
    "    for i in range(len(features) - 1):\n",
    "        for j in range(i+1, len(features)):\n",
    "            gradients[features[j]] += k * W[features[i], :] #+ reg_c * W[features[j], :]\n",
    "            \n",
    "    return gradients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Initialize first random model\n",
    "k = 3\n",
    "n_features = 25\n",
    "eta = 0.1\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)\n",
    "\n",
    "W = np.random.uniform(0, 1/np.sqrt(k), size=(n_features, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 16, 4]]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hashed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value is the sum of these three: 0.15821071145153956\n"
     ]
    }
   ],
   "source": [
    "wj0f1_wj1f0 = np.dot(W[1, :], W[16, :])\n",
    "wj0f2_wj2f0 = np.dot(W[1, :], W[4, :])\n",
    "wj1f2_wj2f1 = np.dot(W[16, :], W[4, :])\n",
    "total = wj0f1_wj1f0 + wj0f2_wj2f0 + wj1f2_wj2f1\n",
    "print(f\"Expected value is the sum of these three: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15821071145153956"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hashed.map(lambda x: phi(x[1:])).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(dataRDD, W):\n",
    "    return dataRDD.map(lambda x: np.log(1 + np.exp(-x[0] * phi(x[1:])))).mean()\n",
    "\n",
    "def gd_update(dataRDD, W):\n",
    "    grad = dataRDD.map(lambda x: gradient(x)).mean()\n",
    "    \n",
    "    new_model = W - eta * grad\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "Loss: 0.7952802105808737\n",
      "----------\n",
      "STEP: 2\n",
      "Loss: 0.7897749304025715\n",
      "----------\n",
      "STEP: 3\n",
      "Loss: 0.7843003785165544\n",
      "----------\n",
      "STEP: 4\n",
      "Loss: 0.7788562566237199\n",
      "----------\n",
      "STEP: 5\n",
      "Loss: 0.7734422786398728\n",
      "----------\n",
      "STEP: 6\n",
      "Loss: 0.7680581706923254\n",
      "----------\n",
      "STEP: 7\n",
      "Loss: 0.7627036711780439\n",
      "----------\n",
      "STEP: 8\n",
      "Loss: 0.7573785308684284\n",
      "----------\n",
      "STEP: 9\n",
      "Loss: 0.7520825130447141\n",
      "----------\n",
      "STEP: 10\n",
      "Loss: 0.7468153936470999\n",
      "----------\n",
      "STEP: 11\n",
      "Loss: 0.7415769614200466\n",
      "----------\n",
      "STEP: 12\n",
      "Loss: 0.7363670180357952\n",
      "----------\n",
      "STEP: 13\n",
      "Loss: 0.7311853781780426\n",
      "----------\n",
      "STEP: 14\n",
      "Loss: 0.7260318695679198\n",
      "----------\n",
      "STEP: 15\n",
      "Loss: 0.7209063329149344\n",
      "----------\n",
      "STEP: 16\n",
      "Loss: 0.7158086217764081\n",
      "----------\n",
      "STEP: 17\n",
      "Loss: 0.7107386023101187\n",
      "----------\n",
      "STEP: 18\n",
      "Loss: 0.7056961529063783\n",
      "----------\n",
      "STEP: 19\n",
      "Loss: 0.7006811636876074\n",
      "----------\n",
      "STEP: 20\n",
      "Loss: 0.6956935358655668\n",
      "----------\n",
      "STEP: 21\n",
      "Loss: 0.6907331809487837\n",
      "----------\n",
      "STEP: 22\n",
      "Loss: 0.6858000197952805\n",
      "----------\n",
      "STEP: 23\n",
      "Loss: 0.6808939815084608\n",
      "----------\n",
      "STEP: 24\n",
      "Loss: 0.6760150021768646\n",
      "----------\n",
      "STEP: 25\n",
      "Loss: 0.6711630234614206\n",
      "----------\n",
      "STEP: 26\n",
      "Loss: 0.666337991036723\n",
      "----------\n",
      "STEP: 27\n",
      "Loss: 0.6615398528957068\n",
      "----------\n",
      "STEP: 28\n",
      "Loss: 0.6567685575297967\n",
      "----------\n",
      "STEP: 29\n",
      "Loss: 0.6520240519991151\n",
      "----------\n",
      "STEP: 30\n",
      "Loss: 0.6473062799096012\n",
      "----------\n",
      "STEP: 31\n",
      "Loss: 0.6426151793158481\n",
      "----------\n",
      "STEP: 32\n",
      "Loss: 0.6379506805700762\n",
      "----------\n",
      "STEP: 33\n",
      "Loss: 0.6333127041388892\n",
      "----------\n",
      "STEP: 34\n",
      "Loss: 0.6287011584102736\n",
      "----------\n",
      "STEP: 35\n",
      "Loss: 0.6241159375136904\n",
      "----------\n",
      "STEP: 36\n",
      "Loss: 0.6195569191760716\n",
      "----------\n",
      "STEP: 37\n",
      "Loss: 0.6150239626360553\n",
      "----------\n",
      "STEP: 38\n",
      "Loss: 0.6105169066379306\n",
      "----------\n",
      "STEP: 39\n",
      "Loss: 0.606035567525495\n",
      "----------\n",
      "STEP: 40\n",
      "Loss: 0.6015797374544323\n",
      "----------\n",
      "STEP: 41\n",
      "Loss: 0.5971491827399191\n",
      "----------\n",
      "STEP: 42\n",
      "Loss: 0.5927436423540154\n",
      "----------\n",
      "STEP: 43\n",
      "Loss: 0.5883628265850568\n",
      "----------\n",
      "STEP: 44\n",
      "Loss: 0.5840064158687838\n",
      "----------\n",
      "STEP: 45\n",
      "Loss: 0.5796740597983993\n",
      "----------\n",
      "STEP: 46\n",
      "Loss: 0.5753653763181676\n",
      "----------\n",
      "STEP: 47\n",
      "Loss: 0.5710799511026415\n",
      "----------\n",
      "STEP: 48\n",
      "Loss: 0.5668173371211623\n",
      "----------\n",
      "STEP: 49\n",
      "Loss: 0.5625770543849717\n",
      "----------\n",
      "STEP: 50\n",
      "Loss: 0.5583585898721487\n",
      "----------\n",
      "STEP: 51\n",
      "Loss: 0.5541613976236573\n",
      "----------\n",
      "STEP: 52\n",
      "Loss: 0.5499848990021187\n",
      "----------\n",
      "STEP: 53\n",
      "Loss: 0.545828483103472\n",
      "----------\n",
      "STEP: 54\n",
      "Loss: 0.5416915073105341\n",
      "----------\n",
      "STEP: 55\n",
      "Loss: 0.5375732979765493\n",
      "----------\n",
      "STEP: 56\n",
      "Loss: 0.5334731512261898\n",
      "----------\n",
      "STEP: 57\n",
      "Loss: 0.5293903338610719\n",
      "----------\n",
      "STEP: 58\n",
      "Loss: 0.5253240843567097\n",
      "----------\n",
      "STEP: 59\n",
      "Loss: 0.5212736139378998\n",
      "----------\n",
      "STEP: 60\n",
      "Loss: 0.5172381077198075\n",
      "----------\n",
      "STEP: 61\n",
      "Loss: 0.5132167259024798\n",
      "----------\n",
      "STEP: 62\n",
      "Loss: 0.5092086050071177\n",
      "----------\n",
      "STEP: 63\n",
      "Loss: 0.5052128591431795\n",
      "----------\n",
      "STEP: 64\n",
      "Loss: 0.5012285812962265\n",
      "----------\n",
      "STEP: 65\n",
      "Loss: 0.49725484462735553\n",
      "----------\n",
      "STEP: 66\n",
      "Loss: 0.4932907037760337\n",
      "----------\n",
      "STEP: 67\n",
      "Loss: 0.48933519615918336\n",
      "----------\n",
      "STEP: 68\n",
      "Loss: 0.48538734326040484\n",
      "----------\n",
      "STEP: 69\n",
      "Loss: 0.48144615190426626\n",
      "----------\n",
      "STEP: 70\n",
      "Loss: 0.4775106155116345\n",
      "----------\n",
      "STEP: 71\n",
      "Loss: 0.47357971533302984\n",
      "----------\n",
      "STEP: 72\n",
      "Loss: 0.4696524216579734\n",
      "----------\n",
      "STEP: 73\n",
      "Loss: 0.46572769499924377\n",
      "----------\n",
      "STEP: 74\n",
      "Loss: 0.46180448725186113\n",
      "----------\n",
      "STEP: 75\n",
      "Loss: 0.4578817428274749\n",
      "----------\n",
      "STEP: 76\n",
      "Loss: 0.45395839976564106\n",
      "----------\n",
      "STEP: 77\n",
      "Loss: 0.45003339082423616\n",
      "----------\n",
      "STEP: 78\n",
      "Loss: 0.4461056445519718\n",
      "----------\n",
      "STEP: 79\n",
      "Loss: 0.44217408634664085\n",
      "----------\n",
      "STEP: 80\n",
      "Loss: 0.43823763950336153\n",
      "----------\n",
      "STEP: 81\n",
      "Loss: 0.4342952262576736\n",
      "----------\n",
      "STEP: 82\n",
      "Loss: 0.43034576882889963\n",
      "----------\n",
      "STEP: 83\n",
      "Loss: 0.42638819046972093\n",
      "----------\n",
      "STEP: 84\n",
      "Loss: 0.42242141652841614\n",
      "----------\n",
      "STEP: 85\n",
      "Loss: 0.41844437553070607\n",
      "----------\n",
      "STEP: 86\n",
      "Loss: 0.41445600028862123\n",
      "----------\n",
      "STEP: 87\n",
      "Loss: 0.4104552290442718\n",
      "----------\n",
      "STEP: 88\n",
      "Loss: 0.4064410066568678\n",
      "----------\n",
      "STEP: 89\n",
      "Loss: 0.4024122858417927\n",
      "----------\n",
      "STEP: 90\n",
      "Loss: 0.39836802847100766\n",
      "----------\n",
      "STEP: 91\n",
      "Loss: 0.39430720694453164\n",
      "----------\n",
      "STEP: 92\n",
      "Loss: 0.3902288056432341\n",
      "----------\n",
      "STEP: 93\n",
      "Loss: 0.3861318224736703\n",
      "----------\n",
      "STEP: 94\n",
      "Loss: 0.3820152705162103\n",
      "----------\n",
      "STEP: 95\n",
      "Loss: 0.3778781797882344\n",
      "----------\n",
      "STEP: 96\n",
      "Loss: 0.3737195991347227\n",
      "----------\n",
      "STEP: 97\n",
      "Loss: 0.3695385982591184\n",
      "----------\n",
      "STEP: 98\n",
      "Loss: 0.36533426990792384\n",
      "----------\n",
      "STEP: 99\n",
      "Loss: 0.36110573222306225\n",
      "----------\n",
      "STEP: 100\n",
      "Loss: 0.3568521312766237\n"
     ]
    }
   ],
   "source": [
    "n_steps = 100\n",
    "for i in range(n_steps):\n",
    "    print(\"----------\")\n",
    "    print(f\"STEP: {i+1}\")\n",
    "    W = gd_update(sample_hashed, W)\n",
    "    loss = log_loss(sample_hashed, W)\n",
    "    print(f\"Loss: {loss}\")\n",
    "#     print(f\"Model: {[round(w,3) for w in model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
