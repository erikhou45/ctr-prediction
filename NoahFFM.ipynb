{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from hashlib import sha256\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set the seed\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark' has no attribute 'read'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-629eeb3e25e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# sc = pyspark.SparkContext()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_parquet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/mediumTrain.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark' has no attribute 'read'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "# sc = pyspark.SparkContext()\n",
    "train_parquet = pyspark.read.parquet(\"data/mediumTrain.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a small sample to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sample.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sample.txt\n",
    "1\t10\tESPN\tNike\n",
    "1\t15\tESPN\tNike\n",
    "0\t2\tESPN\tGucci\n",
    "1\t10\tESPN\tAdidas\n",
    "1\t10\tESPN\tAdidas\n",
    "0\t3\tVogue\tNike\n",
    "1\t20\tVogue\tGucci\n",
    "0\t5\tVogue\tAdidas\n",
    "1\t50\tNBC\tNike\n",
    "0\t0\tNBC\tGucci\n",
    "0\t4\tNBC\tAdidas\n",
    "0\t4\tNBC\tAdidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_RDD = sc.textFile('sample.txt')\n",
    "split_RDD = sample_RDD.map(lambda line: line.split('\\t')).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+\n",
      "| _1| _2|   _3|    _4|\n",
      "+---+---+-----+------+\n",
      "|  1| 10| ESPN|  Nike|\n",
      "|  1| 15| ESPN|  Nike|\n",
      "|  0|  2| ESPN| Gucci|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  0|  3|Vogue|  Nike|\n",
      "|  1| 20|Vogue| Gucci|\n",
      "|  0|  5|Vogue|Adidas|\n",
      "|  1| 50|  NBC|  Nike|\n",
      "|  0|  0|  NBC| Gucci|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "+---+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df = split_RDD.toDF()\n",
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Only run this if you haven't generated a train.parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path file:/media/notebooks/f19-final-project-f19-team-15/data/train.parquet already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o681.save.\n: org.apache.spark.sql.AnalysisException: path file:/media/notebooks/f19-final-project-f19-team-15/data/train.parquet already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:109)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1cae68e29bfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/train.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path file:/media/notebooks/f19-final-project-f19-team-15/data/train.parquet already exists.;'"
     ]
    }
   ],
   "source": [
    "train_data = spark.read.csv(f\"{PWD}/data/train.txt\", sep=\"\\t\")\n",
    "train_data.write.format(\"parquet\").save(f\"{PWD}/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adjust this value to match the desired level of data to work with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which data to load:\n",
    "# 1->sample.parquet\n",
    "# 2->smallTrain.parquet\n",
    "# 3->mediumTrain.parquet\n",
    "# 4->train.parquet (full dataset)\n",
    "\n",
    "DATA_TO_LOAD = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_TO_LOAD == 1:\n",
    "    train_parquet = spark.read.parquet(f\"{PWD}/data/sample.parquet\")\n",
    "    cate_field_start = 2\n",
    "    cate_field_end = 4\n",
    "else:\n",
    "    if DATA_TO_LOAD == 2:\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/smallTrain.parquet\")\n",
    "    elif DATA_TO_LOAD == 3:\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/mediumTrain.parquet\")\n",
    "    else:\n",
    "#         train_parquet = spark.read.parquet(f\"{PWD}/data/train.parquet\")\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/train.parquet\")\n",
    "    cate_field_start = 14\n",
    "    cate_field_end = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename files and recast integer types on the numeric features\n",
    "\n",
    "oldColNames = train_parquet.schema.names\n",
    "\n",
    "train_parquet = train_parquet.withColumn(\"label\", train_parquet[\"_c0\"])\n",
    "for colNum in range(1,cate_field_start): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"int_feature_\"+ str(colNum), train_parquet[colName].cast(types.IntegerType()))\n",
    "for colNum in range(cate_field_start,cate_field_end): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"cate_feature_\"+ str(colNum-cate_field_start+1), train_parquet[colName])\n",
    "\n",
    "#drop the old columns\n",
    "adjusted_labels_train_parquet = train_parquet.drop(*oldColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "intFieldNames = [colName for colName, dType in adjusted_labels_train_parquet.dtypes if dType == 'int']\n",
    "cateFieldNames = [colName for colName, dType in adjusted_labels_train_parquet.dtypes if dType == 'string' and colName != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|         2015|         null|            2|        11727|         null|            0|           33|            2|          null|             0|             0|             2|      8cf07265|      2c16a946|      4d207a24|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|       d35a0a4d|       3516f6e6|       b28479f6|       3628a186|       e0823772|       3486227d|       e4ca448c|           null|           null|       e35886df|           null|       bcdee96c|       9117a34a|           null|           null|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "adjusted_labels_train_parquet.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 10\n",
    "\n",
    "train_parquet_MD = adjusted_labels_train_parquet\n",
    "\n",
    "for col in cateFieldNames:\n",
    "    valuesToKeep = adjusted_labels_train_parquet.groupBy(col).count().filter(f\"count >= {threshold}\").select(col)\n",
    "    valuesToKeep = valuesToKeep.withColumn(\"_\"+col, adjusted_labels_train_parquet[col])\n",
    "    valuesToKeep = valuesToKeep.drop(col)\n",
    "\n",
    "    train_parquet_MD = train_parquet_MD.join(F.broadcast(valuesToKeep), train_parquet_MD[col] == valuesToKeep[\"_\"+col], 'leftouter')\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.when(F.col(\"_\"+col).isNull(), \"***\").otherwise(F.col(\"_\"+col)))\n",
    "    train_parquet_MD = train_parquet_MD.drop(\"_\"+col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|         2015|         null|            2|        11727|         null|            0|           33|            2|          null|             0|             0|             2|      8cf07265|      2c16a946|           ***|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|            ***|       3516f6e6|       b28479f6|       3628a186|            ***|       3486227d|       e4ca448c|            ***|            ***|            ***|            ***|       bcdee96c|       9117a34a|            ***|            ***|\n",
      "|    0|         null|           -1|           26|            7|        96252|         null|            0|            7|           14|          null|             0|          null|             7|      05db9164|      80e26c9b|           ***|           ***|      25c83c98|           ***|           ***|      64523cfa|      7cc72ec2|            ***|            ***|            ***|            ***|       cfef1c29|            ***|            ***|       e5ba7672|       f54016b9|       21ddcdc9|       b1252a9d|            ***|            ***|       423fab69|            ***|       e8b83407|            ***|\n",
      "|    0|         null|            1|            1|           29|          100|         null|            0|           29|           30|          null|             0|             0|            30|      5a9ed9b0|      8084ee93|      d032c263|      c18be181|      25c83c98|      7e0ccccf|      dda1fed2|      5b392875|      a73ee510|       547c0ffe|       7f8ffe57|       dfbb09fb|       46f42a63|       07d13a8f|       f3996583|       84898b2a|       1e88c74f|       003d4f4f|            ***|            ***|       0014c32a|            ***|       3a171ecb|       3b183c5c|            ***|            ***|\n",
      "|    0|         null|           -1|         null|         null|        30147|            7|            2|            0|            3|          null|             1|          null|          null|           ***|      38a947a1|           ***|           ***|      25c83c98|      6f6d9be8|           ***|      5b392875|      7cc72ec2|            ***|       7a3651f5|            ***|       95bc260c|       07d13a8f|            ***|            ***|       07c540c4|            ***|            ***|            ***|            ***|            ***|       32c7478e|            ***|            ***|            ***|\n",
      "|    0|         null|            5|           11|            1|         1183|         null|            0|           44|          108|          null|             0|          null|             1|      39af2607|      287130e0|           ***|           ***|      30903e74|      fe6b92e5|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|       9efd8b77|            ***|       e5ba7672|       891589e7|            ***|       b1252a9d|            ***|       78e2e389|       bcdee96c|       3fdb382b|       ea9a246c|       49d68486|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "categorical columns processed in 19.424126386642456 seconds.\n"
     ]
    }
   ],
   "source": [
    "# view data after the replacement\n",
    "start = time.time()\n",
    "train_parquet_reduced_dimensions = train_parquet_MD\n",
    "train_parquet_reduced_dimensions.show(5)\n",
    "print(f'categorical columns processed in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numeric Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in intFieldNames:\n",
    "    train_parquet_reduced_dimensions = train_parquet_reduced_dimensions.withColumn(col, F.floor(F.log(F.col(col) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "|    0|         null|            7|         null|            1|            9|         null|            0|            3|            1|          null|             0|             0|             1|      8cf07265|      2c16a946|           ***|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|            ***|       3516f6e6|       b28479f6|       3628a186|            ***|       3486227d|       e4ca448c|            ***|            ***|            ***|            ***|       bcdee96c|       9117a34a|            ***|            ***|\n",
      "|    0|         null|         null|            3|            2|           11|         null|            0|            2|            2|          null|             0|          null|             2|      05db9164|      80e26c9b|           ***|           ***|      25c83c98|           ***|           ***|      64523cfa|      7cc72ec2|            ***|            ***|            ***|            ***|       cfef1c29|            ***|            ***|       e5ba7672|       f54016b9|       21ddcdc9|       b1252a9d|            ***|            ***|       423fab69|            ***|       e8b83407|            ***|\n",
      "|    0|         null|            0|            0|            3|            4|         null|            0|            3|            3|          null|             0|             0|             3|      5a9ed9b0|      8084ee93|      d032c263|      c18be181|      25c83c98|      7e0ccccf|      dda1fed2|      5b392875|      a73ee510|       547c0ffe|       7f8ffe57|       dfbb09fb|       46f42a63|       07d13a8f|       f3996583|       84898b2a|       1e88c74f|       003d4f4f|            ***|            ***|       0014c32a|            ***|       3a171ecb|       3b183c5c|            ***|            ***|\n",
      "|    0|         null|         null|         null|         null|           10|            2|            1|            0|            1|          null|             0|          null|          null|           ***|      38a947a1|           ***|           ***|      25c83c98|      6f6d9be8|           ***|      5b392875|      7cc72ec2|            ***|       7a3651f5|            ***|       95bc260c|       07d13a8f|            ***|            ***|       07c540c4|            ***|            ***|            ***|            ***|            ***|       32c7478e|            ***|            ***|            ***|\n",
      "|    0|         null|            1|            2|            0|            7|         null|            0|            3|            4|          null|             0|          null|             0|      39af2607|      287130e0|           ***|           ***|      30903e74|      fe6b92e5|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|       9efd8b77|            ***|       e5ba7672|       891589e7|            ***|       b1252a9d|            ***|       78e2e389|       bcdee96c|       3fdb382b|       ea9a246c|       49d68486|\n",
      "|    0|         null|            1|         null|            2|            8|            6|            0|            3|            6|          null|             0|          null|             2|      05db9164|           ***|      02cf9876|      c18be181|      25c83c98|      7e0ccccf|           ***|      0b153874|      a73ee510|            ***|            ***|       8fe001f4|            ***|       1adce6ef|            ***|       36103458|       e5ba7672|            ***|            ***|            ***|       e587c466|            ***|       bcdee96c|       3b183c5c|            ***|            ***|\n",
      "|    0|            0|            0|            2|            1|            9|            6|            1|            1|            5|             0|             1|             0|             2|      68fd1e64|      4e8d18ed|           ***|           ***|      4cf72387|      fbad5c96|           ***|      0b153874|      a73ee510|            ***|       a9b84bd5|            ***|       cdc2ccda|       b28479f6|            ***|            ***|       3486227d|       47e4d79e|       a153cea2|       a458ea53|            ***|       c0061c6d|       bcdee96c|       3fdb382b|       e8b83407|       49d68486|\n",
      "|    0|            0|            0|            0|         null|            8|         null|         null|            2|         null|             0|          null|             0|          null|      05db9164|      6887a43c|           ***|           ***|      30903e74|           ***|           ***|      37e4aa92|      a73ee510|       3b08e48b|            ***|            ***|            ***|       64c94865|            ***|            ***|       e5ba7672|            ***|       21ddcdc9|       b1252a9d|            ***|            ***|       32c7478e|            ***|       445bbe3b|            ***|\n",
      "|    0|         null|            5|            2|            1|            9|            5|            2|            2|            4|          null|             0|          null|             1|      05db9164|      b80912da|           ***|           ***|      25c83c98|      fe6b92e5|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|            ***|            ***|       e5ba7672|            ***|            ***|       a458ea53|            ***|            ***|       32c7478e|            ***|       ce62e669|            ***|\n",
      "|    0|            1|            3|            2|            1|            3|            1|            1|            1|            1|             0|             0|          null|             1|      68fd1e64|      0468d672|           ***|           ***|      384874ce|      fbad5c96|           ***|      5b392875|      a73ee510|       3b08e48b|            ***|            ***|            ***|       b28479f6|       234191d3|            ***|       776ce399|       9880032b|       21ddcdc9|       5840adea|            ***|            ***|       3a171ecb|            ***|       ea9a246c|       984e0db0|\n",
      "|    0|         null|            4|            0|            1|            9|         null|            0|            1|            3|          null|             0|          null|             1|      05db9164|      38a947a1|      4470baf4|      8c8a4c47|      25c83c98|      fe6b92e5|           ***|      0b153874|      a73ee510|       3b08e48b|            ***|       bb669e25|            ***|       b28479f6|            ***|       2b2ce127|       776ce399|       ade68c22|            ***|            ***|       2b796e4a|            ***|       be7c41b4|       8d365d3b|            ***|            ***|\n",
      "|    0|         null|            4|            0|            1|            9|            3|            0|            2|            3|          null|             0|          null|             1|      05db9164|      58e67aaf|           ***|           ***|      25c83c98|      fbad5c96|           ***|      062b5529|      a73ee510|            ***|            ***|            ***|            ***|       b28479f6|       62eca3c0|            ***|       07c540c4|       c21c3e4c|       21ddcdc9|       b1252a9d|            ***|       c9d4222a|       3a171ecb|            ***|       b9266ff0|            ***|\n",
      "|    0|         null|            0|            0|            2|           11|         null|         null|            2|         null|          null|          null|          null|             2|      5a9ed9b0|      38a947a1|           ***|           ***|      b0530c50|      7e0ccccf|           ***|      0b153874|      7cc72ec2|            ***|            ***|            ***|            ***|       07d13a8f|            ***|            ***|       e5ba7672|            ***|            ***|            ***|            ***|            ***|       3a171ecb|            ***|            ***|            ***|\n",
      "|    0|            0|            2|         null|            3|            7|            4|            1|            3|            6|             0|             0|          null|             3|      68fd1e64|      08d6d899|           ***|           ***|      384874ce|      fbad5c96|           ***|      0b153874|      a73ee510|            ***|       2bcfb78f|            ***|       e6fc496d|       07d13a8f|       41f10449|            ***|       e5ba7672|       698d1c68|            ***|            ***|            ***|       ad3062eb|       32c7478e|       f96a556f|            ***|            ***|\n",
      "|    0|         null|         null|            1|            2|            8|            3|            2|            2|            4|          null|             1|             0|             2|           ***|      d4be07ad|           ***|           ***|      4cf72387|      fbad5c96|           ***|      0b153874|      a73ee510|            ***|       47db1bd7|            ***|       4004a1be|       f862f261|            ***|            ***|       e5ba7672|       cbae5931|            ***|       b1252a9d|            ***|            ***|       55dd3565|       b2f178a3|       001f3601|       938732a0|\n",
      "|    0|         null|            7|            2|            0|           10|            7|            0|            2|            6|          null|             0|             0|             2|           ***|           ***|      d032c263|      c18be181|           ***|      fbad5c96|           ***|      0b153874|      a73ee510|       3b08e48b|            ***|       dfbb09fb|            ***|       b28479f6|            ***|       84898b2a|       e5ba7672|            ***|            ***|            ***|       0014c32a|            ***|       32c7478e|       3b183c5c|            ***|            ***|\n",
      "|    0|         null|            0|         null|            2|            8|            2|            1|            2|            2|          null|             1|             0|             2|      8cf07265|      d833535f|      77f2f2e5|      d16679b9|      25c83c98|      fbad5c96|           ***|      0b153874|      a73ee510|       3b08e48b|            ***|       9f32b866|            ***|       b28479f6|            ***|       31ca40b6|       27c07bd6|       281769c2|            ***|            ***|       dfcfc3fa|            ***|       32c7478e|       aee52b6f|            ***|            ***|\n",
      "|    0|            0|            4|         null|            0|            1|            0|            0|            0|            0|             0|             0|          null|             0|      8cf07265|      09e68b86|      aa8c1539|      85dd697c|      25c83c98|           ***|           ***|      0b153874|      a73ee510|            ***|            ***|       d8c29807|            ***|       1adce6ef|            ***|       c64d548f|       d4bb7bd8|       63cdbb21|       cf99e5de|       5840adea|       5f957280|            ***|       32c7478e|       1793a828|       e8b83407|       b7d9c3bc|\n",
      "|    0|            0|            4|         null|         null|            8|            5|            3|            0|            4|             0|             0|             0|          null|      68fd1e64|      62e9e9bf|           ***|           ***|      25c83c98|      7e0ccccf|           ***|      64523cfa|      a73ee510|            ***|            ***|            ***|            ***|       07d13a8f|            ***|            ***|       e5ba7672|            ***|            ***|            ***|            ***|            ***|       bcdee96c|            ***|            ***|            ***|\n",
      "|    0|            0|            3|            0|            0|            8|            5|            2|            1|            1|             0|             1|          null|             0|      ae82ea21|      38a947a1|           ***|           ***|      25c83c98|      7e0ccccf|      5d87968e|      0b153874|      a73ee510|            ***|            ***|            ***|       7aab7990|       07d13a8f|            ***|            ***|       e5ba7672|            ***|            ***|            ***|            ***|            ***|       423fab69|            ***|            ***|            ***|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "... completed job in 13.563343524932861 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet_reduced_dimensions.show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10000\n",
    "n_fields = len(intFieldNames) + len(cateFieldNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "hasher = FeatureHasher()\n",
    "hasher.setCategoricalCols(intFieldNames)\n",
    "hasher.setNumFeatures(n_features)\n",
    "\n",
    "for col in intFieldNames + cateFieldNames:\n",
    "    hasher.setInputCols([col])\n",
    "    hasher.setOutputCol(col+\"_hashed\")\n",
    "    train_parquet_reduced_dimensions = hasher.transform(train_parquet_reduced_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|int_feature_1_hashed|int_feature_2_hashed|int_feature_3_hashed|int_feature_4_hashed|int_feature_5_hashed|int_feature_6_hashed|int_feature_7_hashed|int_feature_8_hashed|int_feature_9_hashed|int_feature_10_hashed|int_feature_11_hashed|int_feature_12_hashed|int_feature_13_hashed|cate_feature_1_hashed|cate_feature_2_hashed|cate_feature_3_hashed|cate_feature_4_hashed|cate_feature_5_hashed|cate_feature_6_hashed|cate_feature_7_hashed|cate_feature_8_hashed|cate_feature_9_hashed|cate_feature_10_hashed|cate_feature_11_hashed|cate_feature_12_hashed|cate_feature_13_hashed|cate_feature_14_hashed|cate_feature_15_hashed|cate_feature_16_hashed|cate_feature_17_hashed|cate_feature_18_hashed|cate_feature_19_hashed|cate_feature_20_hashed|cate_feature_21_hashed|cate_feature_22_hashed|cate_feature_23_hashed|cate_feature_24_hashed|cate_feature_25_hashed|cate_feature_26_hashed|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|    0|         null|            7|         null|            1|            9|         null|            0|            3|            1|          null|             0|             0|             1|      8cf07265|      2c16a946|           ***|      9e2401c7|      25c83c98|      fe6b92e5|      dc7659bd|      0b153874|      a73ee510|       03e48276|       e51ddf94|            ***|       3516f6e6|       b28479f6|       3628a186|            ***|       3486227d|       e4ca448c|            ***|            ***|            ***|            ***|       bcdee96c|       9117a34a|            ***|            ***|       (10000,[],[])| (10000,[820],[1.0])|       (10000,[],[])| (10000,[679],[1.0])|(10000,[7024],[1.0])|       (10000,[],[])|(10000,[2737],[1.0])|(10000,[9529],[1.0])|(10000,[1135],[1.0])|        (10000,[],[])| (10000,[8649],[1.0])| (10000,[7498],[1.0])| (10000,[2881],[1.0])| (10000,[8201],[1.0])| (10000,[1359],[1.0])| (10000,[9906],[1.0])| (10000,[9376],[1.0])| (10000,[4640],[1.0])| (10000,[5660],[1.0])| (10000,[5091],[1.0])| (10000,[7171],[1.0])| (10000,[6563],[1.0])|  (10000,[9215],[1.0])|  (10000,[8094],[1.0])|  (10000,[7894],[1.0])|  (10000,[1219],[1.0])|  (10000,[1095],[1.0])|    (10000,[79],[1.0])|  (10000,[7102],[1.0])|   (10000,[385],[1.0])|   (10000,[672],[1.0])|  (10000,[2179],[1.0])|  (10000,[5247],[1.0])|  (10000,[6487],[1.0])|  (10000,[2721],[1.0])|  (10000,[6979],[1.0])|  (10000,[6081],[1.0])|  (10000,[1686],[1.0])|  (10000,[7277],[1.0])|\n",
      "|    0|         null|         null|            3|            2|           11|         null|            0|            2|            2|          null|             0|          null|             2|      05db9164|      80e26c9b|           ***|           ***|      25c83c98|           ***|           ***|      64523cfa|      7cc72ec2|            ***|            ***|            ***|            ***|       cfef1c29|            ***|            ***|       e5ba7672|       f54016b9|       21ddcdc9|       b1252a9d|            ***|            ***|       423fab69|            ***|       e8b83407|            ***|       (10000,[],[])|       (10000,[],[])|(10000,[9573],[1.0])|(10000,[6777],[1.0])|(10000,[1705],[1.0])|       (10000,[],[])|(10000,[2737],[1.0])| (10000,[980],[1.0])|(10000,[7212],[1.0])|        (10000,[],[])| (10000,[8649],[1.0])|        (10000,[],[])| (10000,[2994],[1.0])| (10000,[8616],[1.0])| (10000,[9633],[1.0])| (10000,[9906],[1.0])| (10000,[8694],[1.0])| (10000,[4640],[1.0])| (10000,[3340],[1.0])| (10000,[1202],[1.0])| (10000,[7328],[1.0])|  (10000,[855],[1.0])|  (10000,[5306],[1.0])|  (10000,[7462],[1.0])|  (10000,[7894],[1.0])|  (10000,[8815],[1.0])|   (10000,[735],[1.0])|  (10000,[7855],[1.0])|  (10000,[7102],[1.0])|  (10000,[6193],[1.0])|  (10000,[3752],[1.0])|   (10000,[618],[1.0])|  (10000,[2434],[1.0])|  (10000,[6487],[1.0])|  (10000,[2721],[1.0])|   (10000,[270],[1.0])|  (10000,[4140],[1.0])|  (10000,[6937],[1.0])|  (10000,[7277],[1.0])|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "... completed job in 11.761964559555054 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet_reduced_dimensions.show(2)\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_columns = train_parquet_reduced_dimensions.schema.names[-n_fields:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjust the dataframe to only contain the hashed value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sparse_vectors(vector, field_ind):\n",
    "    if vector.indices.size > 0:\n",
    "        return int(vector.indices[0])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "vector_parser = F.udf(parse_sparse_vectors, types.IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet_hashed = train_parquet_reduced_dimensions\n",
    "for field_ind, col in enumerate(hashed_columns):\n",
    "    \n",
    "    train_parquet_hashed = train_parquet_hashed.withColumn(col, vector_parser(col, F.lit(field_ind)))\n",
    "\n",
    "train_parquet_hashed = train_parquet_hashed.drop(*(intFieldNames + cateFieldNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|label|int_feature_1_hashed|int_feature_2_hashed|int_feature_3_hashed|int_feature_4_hashed|int_feature_5_hashed|int_feature_6_hashed|int_feature_7_hashed|int_feature_8_hashed|int_feature_9_hashed|int_feature_10_hashed|int_feature_11_hashed|int_feature_12_hashed|int_feature_13_hashed|cate_feature_1_hashed|cate_feature_2_hashed|cate_feature_3_hashed|cate_feature_4_hashed|cate_feature_5_hashed|cate_feature_6_hashed|cate_feature_7_hashed|cate_feature_8_hashed|cate_feature_9_hashed|cate_feature_10_hashed|cate_feature_11_hashed|cate_feature_12_hashed|cate_feature_13_hashed|cate_feature_14_hashed|cate_feature_15_hashed|cate_feature_16_hashed|cate_feature_17_hashed|cate_feature_18_hashed|cate_feature_19_hashed|cate_feature_20_hashed|cate_feature_21_hashed|cate_feature_22_hashed|cate_feature_23_hashed|cate_feature_24_hashed|cate_feature_25_hashed|cate_feature_26_hashed|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "|    0|                null|                 820|                null|                 679|                7024|                null|                2737|                9529|                1135|                 null|                 8649|                 7498|                 2881|                 8201|                 1359|                 9906|                 9376|                 4640|                 5660|                 5091|                 7171|                 6563|                  9215|                  8094|                  7894|                  1219|                  1095|                    79|                  7102|                   385|                   672|                  2179|                  5247|                  6487|                  2721|                  6979|                  6081|                  1686|                  7277|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+---------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+----------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "... completed job in 11.699196577072144 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet_hashed.show(1)\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change labels to be -1 and 1\n",
    "train_parquet_hashed = train_parquet_hashed.withColumn(\"label\", F.when(F.col(\"label\") == 0, -1).otherwise(F.col(\"label\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFM\n",
    "Mathematically, FMM can be expressed as:\n",
    "\n",
    "$ \\phi_{FFM}(w, x) = \\sum\\limits^{n}_{j_1=1} \\sum\\limits^{n}_{j_2=j_1+1}(w_{j_1} \\cdot w_{j_2})x_{j_1}x_{j_2}$\n",
    "\n",
    "However, in the models considered, either all variables are categorical or all integer values are binned effectively making them categorical and the $x_{j_1}$ and $x_{j_2}$ are both equal to 1. This reduces the formula to:\n",
    "\n",
    "$ \\phi_{FFM}(w, x) = \\sum\\limits^{n}_{j_1=1} \\sum\\limits^{n}_{j_2=j_1+1}(w_{j_1} \\cdot w_{j_2})$\n",
    "\n",
    "The optimization function considered for this model is log loss with regularization and the following formula is to be minimized.\n",
    "\n",
    "$\\underset{w}{min}$   $\\dfrac{\\lambda}{2}||w||_2^2 + \\sum\\limits^{m}_{i=1}log(1 + exp(-y_i\\phi_{FFM}(w,x_i)))$\n",
    "\n",
    "Currently, a closed-form solution for minimizing log loss is not known and therefore gradient descent is applied. The gradients for $\\phi_{FFM}(w, x)$ are:\n",
    "\n",
    "$g_{j_1,f_2} = \\triangledown_{w_{j_1,f_2}} f(w) = \\lambda \\cdot w_{j_1,f_2} + \\kappa \\cdot w_{j_2,f_1}$\n",
    "\n",
    "$g_{j_2,f_1} = \\triangledown_{w_{j_2,f_1}} f(w) = \\lambda \\cdot w_{j_2,f_1} + \\kappa \\cdot w_{j_1,f_2}$\n",
    "\n",
    "where,\n",
    "\n",
    "$\\kappa = \\dfrac{\\partial log(1 + exp(-y\\phi_{FFM}(w,x)))}{\\partial \\phi_{FFM}(w, x)} = \\dfrac{-y}{1 + exp(y\\phi_{FFM}(w,x))}$\n",
    "\n",
    "Initially we define two helper function for $\\phi_{FFM}$ and $\\kappa$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x, W):\n",
    "    total = 0\n",
    "    for i in range(len(x) - 1):\n",
    "        if not x[i]:\n",
    "            continue\n",
    "            \n",
    "        for j in range(i + 1, len(x)):\n",
    "            if x[j]:\n",
    "                total += np.dot(W[x[i], j, :], W[x[j], i, :])\n",
    "                            \n",
    "    return total\n",
    "\n",
    "def kappa(y, features, W):\n",
    "    return -int(y)/(1 + np.exp(y*phi(features, W)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "In addition to these helper functions, an associated cost function to minimize and evaluate the performance of the model during training and validation was required. As noted above, the evaluation method used is the log loss. A helper function is created to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(dataRDD, W):\n",
    "    return dataRDD.map(lambda x: np.log(1 + np.exp(-int(x[0]) * phi(x[1:], W)))).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1\n",
    "The first implementation of the algorithm applies batch gradient descent to the model and the algorithm takes the following form for $t$ epochs:\n",
    "\n",
    "1. Map over instance, calculate all subgradients and emit the form ((feature, field) $g_{j_1,f_2}$) and ((feature, field) $g_{j_2,f_1}$) for all field pair combinations\n",
    "2. Reduce by the key to summing values for each key and number of occurances and incrementing a count variable so an average can be calculated. Emit ((feature, field), gradient, count).\n",
    "3. Map over each value, diving the gradient by the count to find an average gradient step.\n",
    "4. Create a tensor of zeroes $R^{n x f x k}$ where $n$ is the number of features, $f$ is the number of fields and $k$ is the size of each latent vector.\n",
    "5. Add each latent vector gradient step to the tensor of zeroes\n",
    "6. Subtract the gradient step multiplied by a learning rate from the original model: $W - eta * gradient$ where W is the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet_hashed = train_parquet_hashed.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is first split into train and test for later evaluation before the training set is split into 100 mini batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train Split\n",
    "train_df, test_df = train_parquet_hashed.randomSplit([0.8, 0.2])\n",
    "batches = train_df.randomSplit([0.01] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.broadcast.Broadcast at 0x7fcda50495f8>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize model parameters\n",
    "k = 2\n",
    "n_features = 10000\n",
    "n_fields = 39\n",
    "eta = 0.3\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(n_fields)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient(x, W):\n",
    "    y = int(x[0])\n",
    "    features = x[1:]\n",
    "    kap = kappa(y, features, W)\n",
    "    \n",
    "    for i in range(len(features) - 1):\n",
    "        if not features[i]:\n",
    "            continue\n",
    "            \n",
    "        for j in range(i+1, len(features)):\n",
    "            if features[j]:\n",
    "                yield ((features[i], j), (kap * W[int(features[j]), i, :], 1))\n",
    "                yield ((features[j], i), (kap * W[int(features[i]), j, :], 1))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(dataRDD, W):\n",
    "    return dataRDD.map(lambda x: np.log(1 + np.exp(-int(x[0]) * phi(x[1:], W)))).mean()\n",
    "\n",
    "def gd_update(dataRDD, W):\n",
    "    grad = dataRDD.flatMap(lambda x: gradient(x, W))\\\n",
    "                .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\\\n",
    "                .map(lambda x: ((x[0][0], x[0][1]), x[1][0] / x[1][1]))\\\n",
    "                .collect()\n",
    "    \n",
    "    grad_update = np.zeros(shape=(n_features, n_fields, k))\n",
    "    \n",
    "    for indices, vector in grad:\n",
    "        feature_index = indices[0]\n",
    "        field_index = indices[1]\n",
    "        \n",
    "        grad_update[int(feature_index), field_index, :] += vector\n",
    "    \n",
    "    new_model = W - eta * grad_update\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "STEP: 1\n",
      "Training Loss: 62.73936057037036\n",
      "Test Loss: 60.10183410156437\n",
      "----------\n",
      "STEP: 2\n",
      "Training Loss: 22.328150602460546\n",
      "Test Loss: 36.45676808231048\n",
      "----------\n",
      "STEP: 3\n",
      "Training Loss: 4.307155265608798\n",
      "Test Loss: 13.901616519936255\n",
      "----------\n",
      "STEP: 4\n",
      "Training Loss: 0.8664094322811013\n",
      "Test Loss: 3.973589613149072\n",
      "----------\n",
      "STEP: 5\n",
      "Training Loss: 1.475896230765925\n",
      "Test Loss: 4.674988641481719\n",
      "----------\n",
      "STEP: 6\n",
      "Training Loss: 2.440370214571568\n",
      "Test Loss: 2.0451115558223574\n",
      "----------\n",
      "STEP: 7\n",
      "Training Loss: 0.882557069689214\n",
      "Test Loss: 5.096213680944408\n",
      "----------\n",
      "STEP: 8\n",
      "Training Loss: 0.018436663839698046\n",
      "Test Loss: 3.3981814430053627\n",
      "----------\n",
      "STEP: 9\n",
      "Training Loss: 1.6968265521501424\n",
      "Test Loss: 2.6162523317611956\n",
      "----------\n",
      "STEP: 10\n",
      "Training Loss: 0.24230267188769136\n",
      "Test Loss: 3.4301517522897385\n",
      "\n",
      "... trained 10 iterations in 31.028324842453003 seconds\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(split_data, w_init, n_steps = 10):\n",
    "    \n",
    "    model = sc.broadcast(w_init)\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(n_steps):\n",
    "        train_rdd = split_data[i].rdd\n",
    "        print(\"----------\")\n",
    "        print(f\"STEP: {i+1}\")\n",
    "        new_model = gd_update(train_rdd, model.value)\n",
    "        model = sc.broadcast(new_model)\n",
    "        train_loss = log_loss(train_rdd, model.value)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        test_loss = log_loss(split_data[i + 50].rdd, model.value)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"\\n... trained {n_steps} iterations in {time.time() - start} seconds\")\n",
    "\n",
    "np.random.seed(1)\n",
    "w_init = np.random.uniform(0, 1/np.sqrt(k), size=(n_features, n_fields, k))\n",
    "gradient_descent(batches, w_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 1 Evaluation\n",
    "As can be seen from the outputs, the gradient steps are performing correctly with the log loss improving on each iteration. In this case the training occurs over the first 10 batches and testing is performed on batches 50-59. The training loss unsurprisingly converges a lot more than the test loss due to the presence of the most recent gradient step being performed on the training data that was just tested. A suggestion would be to add regularization, which we tried, but it didn't improve performance all that much, which is similar to the conclusions noted in the paper this model is based off of. It should be noted that the losses shown above are for a locally run model and performance here is particularly bad because there are as many as 200,000 unique features and only 10000 features are used for the hashing. Moreover, the number of latent vectors is set to 2, which is fairly low.\n",
    "\n",
    "The example above is on a 5000 instance training set, however, the full dataset was evaluated similarly broken into 100 equal blocks. However, training on just one of the blocks (450k instances) took close to 3 hours with the following computational set up:\n",
    " - 1 n1-standard driver \n",
    " - 6 n1-standard workers\n",
    "     - 8 cores each\n",
    "     - 30GB of RAM\n",
    "\n",
    "That said, it was calculating logloss that took close to an hour each time. It was determined that our implementation was too computational inefficient to continue to attempt on the full dataset and we decided to pivot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image (3).png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2\n",
    "A second implementation was devised that would split the data into separate partitions, performing stochastic gradient descent locally before collecting and summing all gradients at the driver. The hope here was that having the model coefficients locally would enable SGD, but also reduce the amount of emitting done by the mappers, improving performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "n_features = 10000\n",
    "n_fields = 39\n",
    "eta = 0.3\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(n_fields)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)\n",
    "\n",
    "def update_gradient(x, w_old, w_new, G, reg_c = 0.1):\n",
    "    \"\"\"Update the gradient of a FFM\n",
    "    \n",
    "    Args:\n",
    "        x: the input row from the RDD, assume that it has been hashed and that \n",
    "            the first entry is the label the update takes values from w_old and \n",
    "            updates w_new. The format of x is [label, h_1, ..., h_F]\n",
    "            where h_i is the hashed value of the field i and F is the total number of fields.\n",
    "        w_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        w_new: the new coefficient matrix, this will be mutated/updated within this function\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \n",
    "    Implements steps from Algorithm 1 (lines 5 - 11) in paper \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "    \"\"\"\n",
    "    eta = 0.3\n",
    "    y = x[0]\n",
    "    features = x[1:]\n",
    "    # Keep a mapping from the hashed value back to the field it came from\n",
    "    w_v = w_old.value\n",
    "#     D = {h:i for i, h in enumerate(H)}\n",
    "    kappa = -int(y)/(1 + np.exp(int(y)*phi(features, w_v)))\n",
    "    # Iterate through the features and update the gradient, use D for lookups\n",
    "    for f1 in range(len(features) - 1):\n",
    "        j1 = features[f1]\n",
    "        if not j1:\n",
    "            continue\n",
    "            \n",
    "        for f2 in range(1, len(features)):\n",
    "            j2 = features[f2]\n",
    "            if not j2:\n",
    "                continue         \n",
    "            \n",
    "            g1 = kappa*w_v[j2,f1,:] + reg_c*w_v[j1, f2, :]\n",
    "            g2 = kappa*w_v[j1,f2,:] + reg_c*w_v[j2, f1, :]\n",
    "#             G[j1,f2, :] += g1**2\n",
    "#             G[j2,f1, :] += g2**2\n",
    "            w_new[j1,f2,:] = w_v[j1,f2,:] - eta * g1 # eta*g1/np.sqrt(G[j1,f2,:])\n",
    "            w_new[j2,f1,:] = w_v[j2,f1,:] - eta * g2 # eta*g2/np.sqrt(G[j2,f1,:])\n",
    "\n",
    "def apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c):\n",
    "    \"\"\"\n",
    "    Implements the sampling part of Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 3 -- 4\n",
    "    \n",
    "    Args:\n",
    "        rdd: The RDD on each partition\n",
    "        W_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    G = np.ones((n_features, n_fields, k))\n",
    "    W_new = np.zeros((n_features, n_fields, k))\n",
    "    for x in rdd: \n",
    "        #= rdd.sample(False, 0.1)\n",
    "        if np.random.uniform() < 0.1:\n",
    "            update_gradient(x, w_old=W_old, w_new=W_new, G = G, reg_c = reg_c)\n",
    "    yield W_new\n",
    "    \n",
    "def run_ffm_train(rdd_hashed, n_features, n_fields, k, n_iters=10, learning_rate=0.1, reg_c=1):\n",
    "    \"\"\"Runs the FFM algorithm and returns the update coefficient matrix. \n",
    "    \n",
    "    \n",
    "    Implements the SGD parallelization in Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 1 - 2\n",
    "    \n",
    "    Args:\n",
    "        rdd_hashed: An RDD of hashed values in the format where each row is of the form\n",
    "            [label, feature_hash_1, feature_hash_2, ..., feature_hash_n]\n",
    "        n_features: The number of features, for example 10**6\n",
    "        n_fields: The number of fields, e.g. 39 for Criteo dataset\n",
    "        k: dimension of the latent vector\n",
    "        n_iters: number of iterations of gradient descent\n",
    "        reg_c: regulization parameter \n",
    "        \n",
    "    Returns:\n",
    "        The returned matrix is of the type W[j,f,k] where the first index is the feature, \n",
    "        the second is the field and the third is the latent component \n",
    "        of the vector. So if there are 1,000,000 features, 39 fields and 10 latent vectors \n",
    "        the return value is a 1,000,000 x 39 x 10 tensor\n",
    "\n",
    "    \"\"\"\n",
    "    W_old = sc.broadcast(np.random.uniform(0, 1/np.sqrt(k), size=(n_features, n_fields, k)))\n",
    "    for i in range(n_iters):\n",
    "        W_new = rdd_hashed.mapPartitions(lambda rdd, w_old=W_old, n_features=n_features, n_fields=n_fields, k=k, reg_c = reg_c: apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c))\\\n",
    "        .sum()\n",
    "        train_loss = log_loss(rdd_hashed, W_new)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        W_old = sc.broadcast(W_new)\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 51.371761288573545\n",
      "Training Loss: 23.607517955943024\n",
      "Training Loss: 10.84718644190816\n",
      "Training Loss: 5.006273222880837\n",
      "Training Loss: 2.3819632135011095\n",
      "Training Loss: 1.3357208715515054\n",
      "Training Loss: 0.973598711605685\n",
      "Training Loss: 0.8419300960457365\n",
      "Training Loss: 0.7821930373896475\n",
      "Training Loss: 0.7496892202336619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ffm_train(train_df.rdd , 10000, 39, 2, n_iters=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation 2 Evaluation\n",
    "The second implementation struggled with similar constraints as the first in that it simply wasn't performany enough. Even on much smaller subsets of the full dataset (1%) it still took close to an hour when run on GCP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "Beyond the two main implementations above, the team read some other literature to better understand how to boost performance of the model. First, the libFFM implementation that won the kaggle competition was compiled and run on a 1% subset of the dataset, which it was able to complete in some minutes. This \n",
    "\n",
    "Tried to get it compiled in the cloud and create a python wrapper to call the C++ library, but time constraints and complexity limited this solutions. \n",
    "\n",
    "After noting that performance of numpy arrays and pandas are close to that of C++. Another possible solution, yet to be attempted, is to install pyarrows and enable arrows in the spark environment. This allows writing of pandas UDFs, which allows for vectorization of method across all rows at once that can greatly improve performance. That said, the dataset is too large to hold in memory requiring the data to be chunked into smaller pieces that are run on their own partition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anu's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def phi(W, H):  \n",
    "#     \"\"\"Calculate the factorization function $\\phi_{FMM}\"\"\"\n",
    "#     H = sorted(H)\n",
    "#     D = {h:i for i, h in enumerate(H)}\n",
    "#     return sum(np.dot(W[j,D[l],:], W[l,D[j],:]) for j in H for l in H if l >= j+1)\n",
    "\n",
    "k = 2\n",
    "n_features = 10000\n",
    "n_fields = 39\n",
    "eta = 0.3\n",
    "reg_c = 0.1\n",
    "sc.broadcast(k)\n",
    "sc.broadcast(n_features)\n",
    "sc.broadcast(n_fields)\n",
    "sc.broadcast(reg_c)\n",
    "sc.broadcast(eta)\n",
    "\n",
    "def update_gradient(x, w_old, w_new, G, reg_c = 0.1):\n",
    "    \"\"\"Update the gradient of a FFM\n",
    "    \n",
    "    Args:\n",
    "        x: the input row from the RDD, assume that it has been hashed and that \n",
    "            the first entry is the label the update takes values from w_old and \n",
    "            updates w_new. The format of x is [label, h_1, ..., h_F]\n",
    "            where h_i is the hashed value of the field i and F is the total number of fields.\n",
    "        w_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        w_new: the new coefficient matrix, this will be mutated/updated within this function\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \n",
    "    Implements steps from Algorithm 1 (lines 5 - 11) in paper \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "    \"\"\"\n",
    "    eta = 0.3\n",
    "    y = x[0]\n",
    "    features = x[1:]\n",
    "    # Keep a mapping from the hashed value back to the field it came from\n",
    "    w_v = w_old.value\n",
    "#     D = {h:i for i, h in enumerate(H)}\n",
    "    kappa = -int(y)/(1 + np.exp(int(y)*phi(features, w_v)))\n",
    "    # Iterate through the features and update the gradient, use D for lookups\n",
    "    for f1 in range(len(features) - 1):\n",
    "        j1 = features[f1]\n",
    "        if not j1:\n",
    "            continue\n",
    "            \n",
    "        for f2 in range(1, len(features)):\n",
    "            j2 = features[f2]\n",
    "            if not j2:\n",
    "                continue         \n",
    "            \n",
    "            g1 = kappa*w_v[j2,f1,:] + reg_c*w_v[j1, f2, :]\n",
    "            g2 = kappa*w_v[j1,f2,:] + reg_c*w_v[j2, f1, :]\n",
    "#             G[j1,f2, :] += g1**2\n",
    "#             G[j2,f1, :] += g2**2\n",
    "            w_new[j1,f2,:] = w_v[j1,f2,:] - eta * g1 # eta*g1/np.sqrt(G[j1,f2,:])\n",
    "            w_new[j2,f1,:] = w_v[j2,f1,:] - eta * g2 # eta*g2/np.sqrt(G[j2,f1,:])\n",
    "\n",
    "def apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c):\n",
    "    \"\"\"\n",
    "    Implements the sampling part of Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 3 -- 4\n",
    "    \n",
    "    Args:\n",
    "        rdd: The RDD on each partition\n",
    "        W_old: the old value of the coefficient matrix (this is usually a broadcast variable\n",
    "            from the driver)\n",
    "        n_features: number of features\n",
    "        n_fiels: number of fields\n",
    "        reg_c: regularization parameter\n",
    "    \"\"\"\n",
    "    \n",
    "    G = np.ones((n_features, n_fields, k))\n",
    "    W_new = np.zeros((n_features, n_fields, k))\n",
    "    for x in rdd: \n",
    "        #= rdd.sample(False, 0.1)\n",
    "        if np.random.uniform() < 0.1:\n",
    "            update_gradient(x, w_old=W_old, w_new=W_new, G = G, reg_c = reg_c)\n",
    "    yield W_new\n",
    "    \n",
    "def run_ffm_train(rdd_hashed, n_features, n_fields, k, n_iters=10, learning_rate=0.1, reg_c=1):\n",
    "    \"\"\"Runs the FFM algorithm and returns the update coefficient matrix. \n",
    "    \n",
    "    \n",
    "    Implements the SGD parallelization in Algorithm 1 \n",
    "    https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf, lines 1 - 2\n",
    "    \n",
    "    Args:\n",
    "        rdd_hashed: An RDD of hashed values in the format where each row is of the form\n",
    "            [label, feature_hash_1, feature_hash_2, ..., feature_hash_n]\n",
    "        n_features: The number of features, for example 10**6\n",
    "        n_fields: The number of fields, e.g. 39 for Criteo dataset\n",
    "        k: dimension of the latent vector\n",
    "        n_iters: number of iterations of gradient descent\n",
    "        reg_c: regulization parameter \n",
    "        \n",
    "    Returns:\n",
    "        The returned matrix is of the type W[j,f,k] where the first index is the feature, \n",
    "        the second is the field and the third is the latent component \n",
    "        of the vector. So if there are 1,000,000 features, 39 fields and 10 latent vectors \n",
    "        the return value is a 1,000,000 x 39 x 10 tensor\n",
    "\n",
    "    \"\"\"\n",
    "    W_old = sc.broadcast(np.random.uniform(0, 1/np.sqrt(k), size=(n_features, n_fields, k)))\n",
    "    for i in range(n_iters):\n",
    "        W_new = rdd_hashed.mapPartitions(lambda rdd, w_old=W_old, n_features=n_features, n_fields=n_fields, k=k, reg_c = reg_c: apply_sgd_step(rdd, W_old, n_features, n_fields, k, reg_c))\\\n",
    "        .sum()\n",
    "        train_loss = log_loss(rdd_hashed, W_new)\n",
    "        print(f\"Training Loss: {train_loss}\")\n",
    "        W_old = sc.broadcast(W_new)\n",
    "    return W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 51.32719269429185\n",
      "Training Loss: 23.286917665494684\n",
      "Training Loss: 10.763089486067768\n",
      "Training Loss: 5.004336898513017\n",
      "Training Loss: 2.395190453154146\n",
      "Training Loss: 1.3326793380997923\n",
      "Training Loss: 0.9781263711459354\n",
      "Training Loss: 0.8444198268814129\n",
      "Training Loss: 0.7846071228212534\n",
      "Training Loss: 0.7514901123707874\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]],\n",
       "\n",
       "       [[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        ...,\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]]])"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_ffm_train(train_df.rdd , 10000, 39, 2, n_iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label='-1', int_feature_1_hashed=None, int_feature_2_hashed=None, int_feature_3_hashed=None, int_feature_4_hashed=None, int_feature_5_hashed=None, int_feature_6_hashed=None, int_feature_7_hashed=None, int_feature_8_hashed=None, int_feature_9_hashed=None, int_feature_10_hashed=None, int_feature_11_hashed=None, int_feature_12_hashed=None, int_feature_13_hashed=None, cate_feature_1_hashed=4192, cate_feature_2_hashed=8153, cate_feature_3_hashed=9906, cate_feature_4_hashed=8694, cate_feature_5_hashed=4640, cate_feature_6_hashed=3340, cate_feature_7_hashed=1202, cate_feature_8_hashed=5036, cate_feature_9_hashed=855, cate_feature_10_hashed=3494, cate_feature_11_hashed=7462, cate_feature_12_hashed=7894, cate_feature_13_hashed=8815, cate_feature_14_hashed=3612, cate_feature_15_hashed=7855, cate_feature_16_hashed=7102, cate_feature_17_hashed=6795, cate_feature_18_hashed=2289, cate_feature_19_hashed=2179, cate_feature_20_hashed=5247, cate_feature_21_hashed=6487, cate_feature_22_hashed=2721, cate_feature_23_hashed=9692, cate_feature_24_hashed=4140, cate_feature_25_hashed=1686, cate_feature_26_hashed=7277)]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-457c18bacc84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "sorted([None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 Write up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/77/5865b367a6792da2f811ae49391c1f85c29b29663555aac0a118fe8e153e/pyarrow-0.15.1-cp36-cp36m-manylinux1_x86_64.whl (59.0MB)\n",
      "\u001b[K    100% |################################| 59.0MB 295kB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/anaconda/lib/python3.6/site-packages (from pyarrow) (1.15.1)\n",
      "Requirement already satisfied: six>=1.0.0 in /opt/anaconda/lib/python3.6/site-packages (from pyarrow) (1.11.0)\n",
      "\u001b[31mtwisted 18.7.0 requires PyHamcrest>=1.9.0, which is not installed.\u001b[0m\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-0.15.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_small_df = train_df.limit(10)\n",
    "train_from_pd = spark.createDataFrame(train_small_df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = pandas_udf(multiply_func, returnType=IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    49875300\n",
       "1    15553755\n",
       "2    49970430\n",
       "3    15553755\n",
       "4    15553755\n",
       "5    36808515\n",
       "6    15553755\n",
       "7    15553755\n",
       "8    36808515\n",
       "9    36808515\n",
       "dtype: int32"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_small_pd = train_small_df.toPandas()\n",
    "multiply_func(train_small_pd[\"cate_feature_17_hashed\"], train_small_pd[\"cate_feature_18_hashed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o14813.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1868.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1868.0 (TID 146738, localhost, executor driver): java.lang.NullPointerException\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:256)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:242)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.nextBatch(ArrowConverters.scala:170)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.<init>(ArrowConverters.scala:138)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.fromPayloadIterator(ArrowConverters.scala:135)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:211)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:209)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.GeneratedMethodAccessor382.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:256)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:242)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.nextBatch(ArrowConverters.scala:170)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.<init>(ArrowConverters.scala:138)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.fromPayloadIterator(ArrowConverters.scala:135)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:211)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:209)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-378-eb1907f2efd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_from_pd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cate_feature_17_hashed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cate_feature_18_hashed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o14813.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1868.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1868.0 (TID 146738, localhost, executor driver): java.lang.NullPointerException\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:256)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:242)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.nextBatch(ArrowConverters.scala:170)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.<init>(ArrowConverters.scala:138)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.fromPayloadIterator(ArrowConverters.scala:135)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:211)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:209)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.GeneratedMethodAccessor382.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.NullPointerException\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:256)\n\tat org.apache.arrow.vector.ipc.message.MessageSerializer.deserializeRecordBatch(MessageSerializer.java:242)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.readRecordBatch(ArrowFileReader.java:162)\n\tat org.apache.arrow.vector.ipc.ArrowFileReader.loadNextBatch(ArrowFileReader.java:113)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.nextBatch(ArrowConverters.scala:170)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anon$1.<init>(ArrowConverters.scala:138)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$.fromPayloadIterator(ArrowConverters.scala:135)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:211)\n\tat org.apache.spark.sql.execution.arrow.ArrowConverters$$anonfun$3.apply(ArrowConverters.scala:209)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:800)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:288)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "train_from_pd.select(multiply(col(\"cate_feature_17_hashed\"), col(\"cate_feature_18_hashed\"))).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = spark.read.csv(\"sample.txt\", sep=\"\\t\")\n",
    "sample_data.write.format(\"parquet\").save(\"sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = spark.read.parquet(\"sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+------+\n",
      "|_c0|_c1|  _c2|   _c3|\n",
      "+---+---+-----+------+\n",
      "|  1| 10| ESPN|  Nike|\n",
      "|  1| 15| ESPN|  Nike|\n",
      "|  0|  2| ESPN| Gucci|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  1| 10| ESPN|Adidas|\n",
      "|  0|  3|Vogue|  Nike|\n",
      "|  1| 20|Vogue| Gucci|\n",
      "|  0|  5|Vogue|Adidas|\n",
      "|  1| 50|  NBC|  Nike|\n",
      "|  0|  0|  NBC| Gucci|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "|  0|  4|  NBC|Adidas|\n",
      "+---+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'_c0'>\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "col should be Column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ca73a63cdadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# from pyspark.sql.functions import col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# sample_df.select(*(feature_hash(col(c)).alias(c) for c in sample_df.columns)).show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0msample_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_c0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_c0\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \"\"\"\n\u001b[0;32m-> 1848\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"col should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: col should be Column"
     ]
    }
   ],
   "source": [
    "def feature_hash(x, modulo=10**6):\n",
    "    \"\"\"\n",
    "    A function that can be used to hash the features in each observation in the RDD. \n",
    "    We replace the label with 1, -1 and we hash all other features using sha256 \n",
    "    and then we take modulo some power of 10. \n",
    "    \"\"\"\n",
    "    print(x)\n",
    "#     x[0] = 2*int(x[0]) - 1\n",
    "#     for i, value in enumerate(x[1:], 1):\n",
    "#         h = sha256(\"{i}-{val}\".format(i=i,val=value).encode('ascii'))\n",
    "#         hashed_value = int(h.hexdigest(), base=16) \n",
    "#         hashed_value_mod = hashed_value % modulo\n",
    "#         x[i] = hashed_value_mod\n",
    "#     return x\n",
    "\n",
    "# from pyspark.sql.functions import col\n",
    "# sample_df.select(*(feature_hash(col(c)).alias(c) for c in sample_df.columns)).show()\n",
    "sample_df.withColumn(\"_c0\", feature_hash(sample_df[\"_c0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Full Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = spark.read.csv(\"data/dac/train.txt\", sep=\"\\t\")\n",
    "train_data.write.format(\"parquet\").save(f\"data/dac/train.parquet\")\n",
    "full_rdd = sc.textFile('data/dac/train.txt')\n",
    "train_rdd, test_rdd = full_rdd.randomSplit([0.8,0.2], seed = 2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_parquet = spark.read.parquet(\"data/dac/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "oldColNames = train_parquet.schema.names\n",
    "train_parquet = train_parquet.withColumn(\"label\", train_parquet[\"_c0\"])\n",
    "for colNum in range(1,14): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"int_feature_\"+ str(colNum), train_parquet[colName].cast(types.IntegerType()))\n",
    "for colNum in range(14,40): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"cate_feature_\"+ str(colNum-13), train_parquet[colName])\n",
    "\n",
    "#drop the old columns\n",
    "train_parquet = train_parquet.drop(*oldColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "n_fields = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "intFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'int']\n",
    "cateFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'string' and colName != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "hasher = FeatureHasher()\n",
    "hasher.setCategoricalCols(intFieldNames)\n",
    "hasher.setNumFeatures(n_features)\n",
    "\n",
    "# for col in intFieldNames + cateFieldNames:\n",
    "hasher.setInputCols(intFieldNames + cateFieldNames)\n",
    "hasher.setOutputCol(\"hashed_features\")\n",
    "train_parquet = hasher.transform(train_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "|label|int_feature_1|int_feature_2|int_feature_3|int_feature_4|int_feature_5|int_feature_6|int_feature_7|int_feature_8|int_feature_9|int_feature_10|int_feature_11|int_feature_12|int_feature_13|cate_feature_1|cate_feature_2|cate_feature_3|cate_feature_4|cate_feature_5|cate_feature_6|cate_feature_7|cate_feature_8|cate_feature_9|cate_feature_10|cate_feature_11|cate_feature_12|cate_feature_13|cate_feature_14|cate_feature_15|cate_feature_16|cate_feature_17|cate_feature_18|cate_feature_19|cate_feature_20|cate_feature_21|cate_feature_22|cate_feature_23|cate_feature_24|cate_feature_25|cate_feature_26|     hashed_features|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "|    0|         null|          139|         null|            2|         4828|           28|           11|            2|           28|          null|             1|             0|             2|      05db9164|      d833535f|      77f2f2e5|      d16679b9|      4cf72387|      fe6b92e5|      9ea2e0f0|      0b153874|      a73ee510|       43b7a3fa|       2dad6ba2|       9f32b866|       47cb697a|       07d13a8f|       943169c2|       31ca40b6|       e5ba7672|       281769c2|           null|           null|       dfcfc3fa|       c9d4222a|       32c7478e|       aee52b6f|           null|           null|(100,[12,13,16,19...|\n",
      "+-----+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+---------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_parquet.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2531.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:282)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-a7dfc3150655>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_parquet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"int_feature_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    464\u001b[0m         \"\"\"\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o2531.collectToPython.\n: java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:282)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.next(SparkPlan.scala:276)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.sql.execution.SparkPlan$$anon$1.foreach(SparkPlan.scala:276)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:298)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeCollect$1.apply(SparkPlan.scala:297)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:297)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3195)\n\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3192)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3192)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "train_parquet.select(\"int_feature_1\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cate_feature_1',\n",
       " 'cate_feature_2',\n",
       " 'cate_feature_3',\n",
       " 'cate_feature_4',\n",
       " 'cate_feature_5',\n",
       " 'cate_feature_6',\n",
       " 'cate_feature_7',\n",
       " 'cate_feature_8',\n",
       " 'cate_feature_9',\n",
       " 'cate_feature_10',\n",
       " 'cate_feature_11',\n",
       " 'cate_feature_12',\n",
       " 'cate_feature_13',\n",
       " 'cate_feature_14',\n",
       " 'cate_feature_15',\n",
       " 'cate_feature_16',\n",
       " 'cate_feature_17',\n",
       " 'cate_feature_18',\n",
       " 'cate_feature_19',\n",
       " 'cate_feature_20',\n",
       " 'cate_feature_21',\n",
       " 'cate_feature_22',\n",
       " 'cate_feature_23',\n",
       " 'cate_feature_24',\n",
       " 'cate_feature_25',\n",
       " 'cate_feature_26']"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for col in intFieldNames + cateFieldNames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 776.0 failed 1 times, most recent failure: Lost task 0.0 in stage 776.0 (TID 52515, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-152-eaadc752e283>\", line 1, in <lambda>\nNameError: name 'feature_hash' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-152-eaadc752e283>\", line 1, in <lambda>\nNameError: name 'feature_hash' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-eaadc752e283>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_hashed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfeature_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msample_hashed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/context.py\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1014\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda/lib/python3.6/site-packages/py4j-0.10.7-py3.6.egg/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 776.0 failed 1 times, most recent failure: Lost task 0.0 in stage 776.0 (TID 52515, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-152-eaadc752e283>\", line 1, in <lambda>\nNameError: name 'feature_hash' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 230, in main\n    process()\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 225, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 372, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/anaconda/lib/python3.6/site-packages/pyspark-2.3.1-py3.6.egg/pyspark/rdd.py\", line 1371, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/spark/python/lib/pyspark.zip/pyspark/util.py\", line 55, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-152-eaadc752e283>\", line 1, in <lambda>\nNameError: name 'feature_hash' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:149)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "sample_hashed = train_rdd.map(lambda x: feature_hash(x, 10000))\n",
    "sample_hashed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Assessment\n",
    "We then consider how well the model is performing on the training vs test set to check if the model is tending to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 16, 4]]"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hashed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 16), (1, 41), (16, 41)]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "list(itertools.combinations([1, 16, 41], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected value is the sum of these three: 0.7879988184229724\n"
     ]
    }
   ],
   "source": [
    "wj0f1_wj1f0 = np.dot(W[1, 1, :], W[16, 0, :])\n",
    "wj0f2_wj2f0 = np.dot(W[1, 2, :], W[4, 0, :])\n",
    "wj1f2_wj2f1 = np.dot(W[16, 2, :], W[4, 1, :])\n",
    "total = wj0f1_wj1f0 + wj0f2_wj2f0 + wj1f2_wj2f1\n",
    "print(f\"Expected value is the sum of these three: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7879988184229724"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_hashed.map(lambda x: phi(x[1:])).collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800000112"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(np.random.uniform(0,1,size=(10000000,10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extras to consider\n",
    "## Develop classes for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FFM:\n",
    "#     def __init__(self, n_features, k = 10, eta = 0.1, reg_c = 0.1):\n",
    "#         self.n_features = n_features\n",
    "#         self.k = k\n",
    "#         self.eta = eta\n",
    "#         self.reg_c = reg_c\n",
    "        \n",
    "        \n",
    "# ffm = FFM(25, k=3)\n",
    "# ffm.n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "- We develop the paper this way to mimic homework from throughout the semester and build the model up sequentially\n",
    "- Should we write tests and show them in the presentation for simple function like $\\phi_{FFM}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "a = np.random.randint(5, size=39)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 ms  19.7 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "combs = itertools.combinations(a, r=2)\n",
    "np.sum([np.dot(i, j) for i, j in combs])\n",
    "# print(f'itertools processed in {time.time() - start} seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.63 ms  14.8 s per loop (mean  std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "def phi_test(x):\n",
    "    total = 0\n",
    "    for i in range(len(x) - 1):\n",
    "#         if not x[i]:\n",
    "#             continue\n",
    "            \n",
    "        for j in range(i + 1, len(x)):\n",
    "#             if x[j]:\n",
    "                total += np.dot(int(x[i]), int(x[j]))\n",
    "    return total\n",
    "\n",
    "phi_test(a)\n",
    "# print(f'phi_test processed in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
