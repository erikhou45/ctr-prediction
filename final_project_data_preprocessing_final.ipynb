{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import isnan\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "app_name = \"final_project_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I pushed some parquet files to GitHub to make testing code easier\n",
    "1. Noah's small example: sample.parquet\n",
    "2. 100 randomly selected lines from train.txt: smallTrain.parquet\n",
    "3. 5000 randomly selected lines from train.txt: mediumTrain.parquet\n",
    "\n",
    "The full is not on GitHub due to its size. But the code for opening train.txt and write it to parquet and save is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for opening train.txt, write and save as parquet\n",
    "# train_data = spark.read.csv(f\"{PWD}/data/train.txt\", sep=\"\\t\")\n",
    "# train_data.write.format(\"parquet\").save(f\"{PWD}/data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select which data to load:\n",
    "# 1->sample.parquet\n",
    "# 2->smallTrain.parquet\n",
    "# 3->mediumTrain.parquet\n",
    "# 4->train.parquet (full dataset)\n",
    "\n",
    "DATA_TO_LOAD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_TO_LOAD == 1:\n",
    "    train_parquet = spark.read.parquet(f\"{PWD}/data/sample.parquet\")\n",
    "    cate_field_start = 2\n",
    "    cate_field_end = 4\n",
    "else:\n",
    "    if DATA_TO_LOAD == 2:\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/smallTrain.parquet\")\n",
    "    elif DATA_TO_LOAD == 3:\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/mediumTrain.parquet\")\n",
    "    else:\n",
    "        train_parquet = spark.read.parquet(f\"{PWD}/data/train.parquet\")\n",
    "    cate_field_start = 14\n",
    "    cate_field_end = 40\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use when on wk08 environment\n",
    "\n",
    "# sqlContext = SQLContext(sc)\n",
    "    \n",
    "# sc = spark.sparkContext\n",
    "# train_parquet = spark.read.parquet(\"gs://w261-bucket-hou/final-project/data/train.parquet\")\n",
    "# # train_parquet = spark.read.parquet(\"gs://w261-bucket-hou/final-project/data/mediumTrain.parquet\")\n",
    "# cate_field_start = 14\n",
    "# cate_field_end = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename files and recast integer types on the numeric features\n",
    "\n",
    "oldColNames = train_parquet.schema.names\n",
    "\n",
    "train_parquet = train_parquet.withColumn(\"label\", train_parquet[\"_c0\"])\n",
    "for colNum in range(1,cate_field_start): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"int_feature_\"+ str(colNum), train_parquet[colName].cast(types.IntegerType()))\n",
    "for colNum in range(cate_field_start,cate_field_end): \n",
    "    colName = \"_c\" + str(colNum)\n",
    "    train_parquet = train_parquet.withColumn(\"cate_feature_\"+ str(colNum-cate_field_start+1), train_parquet[colName])\n",
    "\n",
    "#drop the old columns\n",
    "train_parquet = train_parquet.drop(*oldColNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record feature names by feature type\n",
    "intFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'int']\n",
    "cateFieldNames = [colName for colName, dType in train_parquet.dtypes if dType == 'string' and colName != 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------+--------------+\n",
      "|label|int_feature_1|cate_feature_1|cate_feature_2|\n",
      "+-----+-------------+--------------+--------------+\n",
      "|    1|           10|          ESPN|          Nike|\n",
      "|    1|           15|          ESPN|          Nike|\n",
      "|    0|            2|          ESPN|         Gucci|\n",
      "|    1|           10|          ESPN|        Adidas|\n",
      "|    1|           10|          ESPN|        Adidas|\n",
      "|    0|            3|         Vogue|          Nike|\n",
      "|    1|           20|         Vogue|         Gucci|\n",
      "|    0|            5|         Vogue|        Adidas|\n",
      "|    1|           50|           NBC|          Nike|\n",
      "|    0|            0|           NBC|         Gucci|\n",
      "|    0|            4|           NBC|        Adidas|\n",
      "|    0|            4|           NBC|        Adidas|\n",
      "+-----+-------------+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view data before feature engineering\n",
    "train_parquet.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Variables\n",
    "\n",
    "We replace the categorical values having counts less than a certain threshold with a special value `***` to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 4\n",
    "\n",
    "train_parquet_MD = train_parquet\n",
    "\n",
    "for col in cateFieldNames:\n",
    "    valuesToKeep = train_parquet.groupBy(col).count().filter(f\"count >= {threshold}\").select(col)\n",
    "    valuesToKeep = valuesToKeep.withColumn(\"_\"+col, train_parquet[col])\n",
    "    valuesToKeep = valuesToKeep.drop(col)\n",
    "\n",
    "    train_parquet_MD = train_parquet_MD.join(F.broadcast(valuesToKeep), train_parquet_MD[col] == valuesToKeep[\"_\"+col], 'leftouter')\n",
    "    train_parquet_MD = train_parquet_MD.withColumn(col, F.when(F.col(\"_\"+col).isNull(), \"***\").otherwise(F.col(\"_\"+col)))\n",
    "    train_parquet_MD = train_parquet_MD.drop(\"_\"+col)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------+--------------+\n",
      "|label|int_feature_1|cate_feature_1|cate_feature_2|\n",
      "+-----+-------------+--------------+--------------+\n",
      "|    1|           10|          ESPN|          Nike|\n",
      "|    1|           15|          ESPN|          Nike|\n",
      "|    0|            2|          ESPN|           ***|\n",
      "|    1|           10|          ESPN|        Adidas|\n",
      "|    1|           10|          ESPN|        Adidas|\n",
      "|    0|            3|           ***|          Nike|\n",
      "|    1|           20|           ***|           ***|\n",
      "|    0|            5|           ***|        Adidas|\n",
      "|    1|           50|           NBC|          Nike|\n",
      "|    0|            0|           NBC|           ***|\n",
      "|    0|            4|           NBC|        Adidas|\n",
      "|    0|            4|           NBC|        Adidas|\n",
      "+-----+-------------+--------------+--------------+\n",
      "\n",
      "categorical columns processed in 1.1284282207489014 seconds.\n"
     ]
    }
   ],
   "source": [
    "# view data after the replacement\n",
    "start = time.time()\n",
    "train_parquet = train_parquet_MD\n",
    "train_parquet.show()\n",
    "print(f'categorical columns processed in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Variables\n",
    "\n",
    "We take the log of numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in intFieldNames:\n",
    "    train_parquet = train_parquet.withColumn(col, F.floor(F.log(F.col(col))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------+--------------+\n",
      "|label|int_feature_1|cate_feature_1|cate_feature_2|\n",
      "+-----+-------------+--------------+--------------+\n",
      "|    1|            2|          ESPN|          Nike|\n",
      "|    1|            2|          ESPN|          Nike|\n",
      "|    0|            0|          ESPN|           ***|\n",
      "|    1|            2|          ESPN|        Adidas|\n",
      "|    1|            2|          ESPN|        Adidas|\n",
      "|    0|            1|           ***|          Nike|\n",
      "|    1|            2|           ***|           ***|\n",
      "|    0|            1|           ***|        Adidas|\n",
      "|    1|            3|           NBC|          Nike|\n",
      "|    0|         null|           NBC|           ***|\n",
      "|    0|            1|           NBC|        Adidas|\n",
      "|    0|            1|           NBC|        Adidas|\n",
      "+-----+-------------+--------------+--------------+\n",
      "\n",
      "... completed job in 1.0382089614868164 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet.show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 100\n",
    "n_fields = len(intFieldNames) + len(cateFieldNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import FeatureHasher\n",
    "hasher = FeatureHasher()\n",
    "hasher.setCategoricalCols(intFieldNames)\n",
    "hasher.setNumFeatures(n_features)\n",
    "\n",
    "for col in intFieldNames + cateFieldNames:\n",
    "    hasher.setInputCols([col])\n",
    "    hasher.setOutputCol(col+\"_hashed\")\n",
    "    train_parquet = hasher.transform(train_parquet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------+--------------+--------------------+---------------------+---------------------+\n",
      "|label|int_feature_1|cate_feature_1|cate_feature_2|int_feature_1_hashed|cate_feature_1_hashed|cate_feature_2_hashed|\n",
      "+-----+-------------+--------------+--------------+--------------------+---------------------+---------------------+\n",
      "|    1|            2|          ESPN|          Nike|    (100,[15],[1.0])|      (100,[7],[1.0])|     (100,[36],[1.0])|\n",
      "|    1|            2|          ESPN|          Nike|    (100,[15],[1.0])|      (100,[7],[1.0])|     (100,[36],[1.0])|\n",
      "|    0|            0|          ESPN|           ***|    (100,[38],[1.0])|      (100,[7],[1.0])|     (100,[86],[1.0])|\n",
      "|    1|            2|          ESPN|        Adidas|    (100,[15],[1.0])|      (100,[7],[1.0])|     (100,[15],[1.0])|\n",
      "|    1|            2|          ESPN|        Adidas|    (100,[15],[1.0])|      (100,[7],[1.0])|     (100,[15],[1.0])|\n",
      "|    0|            1|           ***|          Nike|    (100,[15],[1.0])|     (100,[92],[1.0])|     (100,[36],[1.0])|\n",
      "|    1|            2|           ***|           ***|    (100,[15],[1.0])|     (100,[92],[1.0])|     (100,[86],[1.0])|\n",
      "|    0|            1|           ***|        Adidas|    (100,[15],[1.0])|     (100,[92],[1.0])|     (100,[15],[1.0])|\n",
      "|    1|            3|           NBC|          Nike|    (100,[79],[1.0])|     (100,[27],[1.0])|     (100,[36],[1.0])|\n",
      "|    0|         null|           NBC|           ***|         (100,[],[])|     (100,[27],[1.0])|     (100,[86],[1.0])|\n",
      "|    0|            1|           NBC|        Adidas|    (100,[15],[1.0])|     (100,[27],[1.0])|     (100,[15],[1.0])|\n",
      "|    0|            1|           NBC|        Adidas|    (100,[15],[1.0])|     (100,[27],[1.0])|     (100,[15],[1.0])|\n",
      "+-----+-------------+--------------+--------------+--------------------+---------------------+---------------------+\n",
      "\n",
      "... completed job in 0.9805564880371094 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet.show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashed_columns = train_parquet.schema.names[-n_fields:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a udf to parse sparse vectors\n",
    "def parse_sparse_vectors(vector, field_ind):\n",
    "    if vector.indices.size > 0:\n",
    "        return f'({field_ind},{vector.indices[0]})'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "vector_parser = F.udf(parse_sparse_vectors, types.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field_ind, col in enumerate(hashed_columns):\n",
    "    \n",
    "    train_parquet = train_parquet.withColumn(col, vector_parser(col, F.lit(field_ind)))\n",
    "\n",
    "train_parquet = train_parquet.drop(*(intFieldNames + cateFieldNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------------+---------------------+\n",
      "|label|int_feature_1_hashed|cate_feature_1_hashed|cate_feature_2_hashed|\n",
      "+-----+--------------------+---------------------+---------------------+\n",
      "|    1|              (0,15)|                (1,7)|               (2,36)|\n",
      "|    1|              (0,15)|                (1,7)|               (2,36)|\n",
      "|    0|              (0,38)|                (1,7)|               (2,86)|\n",
      "|    1|              (0,15)|                (1,7)|               (2,15)|\n",
      "|    1|              (0,15)|                (1,7)|               (2,15)|\n",
      "|    0|              (0,15)|               (1,92)|               (2,36)|\n",
      "|    1|              (0,15)|               (1,92)|               (2,86)|\n",
      "|    0|              (0,15)|               (1,92)|               (2,15)|\n",
      "|    1|              (0,79)|               (1,27)|               (2,36)|\n",
      "|    0|                null|               (1,27)|               (2,86)|\n",
      "|    0|              (0,15)|               (1,27)|               (2,15)|\n",
      "|    0|              (0,15)|               (1,27)|               (2,15)|\n",
      "+-----+--------------------+---------------------+---------------------+\n",
      "\n",
      "... completed job in 1.9337656497955322 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train_parquet.show()\n",
    "print(f'... completed job in {time.time() - start} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
