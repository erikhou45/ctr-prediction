{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# w261 Final Project - Clickthrough Rate Prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Team 15\n",
    "- Erik Hou, Noah Pflaum, Connor Stern, Anu Yadav\n",
    "- Fall 2019, section 3  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "* __Section 1__ - Question Formulation\n",
    "* __Section 2__ - Algorithm Explanation\n",
    "* __Section 3__ - EDA & Challenges\n",
    "* __Section 4__ - Algorithm Implementation\n",
    "* __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store path to notebook\n",
    "PWD = !pwd\n",
    "PWD = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "app_name = \"fp_notebook\"\n",
    "master = \"local[*]\"\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(app_name)\\\n",
    "        .master(master)\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://docker.w261:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>fp_notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5ef2d97e80>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 1__ - Question Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The digital advertising industry is booming. Every day, advertisers publish billions of ads to websites and other online platforms where potential customers can view and click on them. For example, Nike as an advertiser might post an ad for running shoes on the ESPN.com website or in a Google search for shoes. The position of the ad, the audience, and the time that the ad is displayed may all significantly influence a user's decision to click on it or not. \n",
    "\n",
    "Ad posts generate vast amount of user log data representing ad placement and whether or not a user clicked on an ad. This data is a treasure trove for companies seeking to more effectively target their ads to specific consumers. As such 'clickthrough rate' prediction (determining whether or not a customer will click on an ad) has become a core task in the field of digital advertising, and multiple clickthrough rate prediction challenges have been launched in recent years to build and continuously improve models that can predict given the features of the ad, whether or not a user will click on the ad. \n",
    "\n",
    "In this project, we seek to build our own homegrown model to complete such a task, inspired by previous successful models. A baseline for any model would be to measure the average click-through rate (CTR, percent of ads clicked).\n",
    "\n",
    "We therefore begin with the question:\n",
    "\n",
    "-  What are some common approaches to CTR prediction? Which methods allow for more complex interactions among features? Which have been the most successful in previous competitions?\n",
    "    -  After some initial research, we find that CTR prediction is typically treated as a logistic regression problem. However, there are many different methods *within* a logistic regression framework that the problem can be handled. One such method is that of a Field-Aware Factorization Machine (FFM), which was the winning model for several of the previously mentioned CTR prediction competitions.\n",
    "\n",
    "Inspired by the success of Juan, Zhuang, Chin, & Lin (the authors of several papers on FFMs and winners of the CTR challenges), we decide to implement a FFM model of our own. This leads us to several additional questions we seek to answer:\n",
    "\n",
    "-  What resource requirements are necessary for implementing an FFM?\n",
    "\n",
    "-  Can the FFM model be implemented in Pyspark using distributed computing using map-reduce patterns? What level of performance can we expect with this implementation?\n",
    "\n",
    "    -  Juan, Zhuang, Chin, & Lin have built their own custom library called 'LibFFM'. Their papers outline an approach using the Hogwild algorithm that allows for concurrent updates to a sparse matrix of model coefficients, which eliminates some of the issues with the communication and synchronization barriers that generally exist when using stochastic gradient descent. Our goal is to explore how this algorithm might be implemented using the techniques we learned in this class (such as Pyspark and distributed computing using map-reduce patterns, as mentioned above). We do not expect our algorithm to be as performant as a customized C++ implmentation; instead, we highlight the application of the ideas learned in this class for creating predicitve models for big data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 2__ - Algorithm Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because an online advertisement can either be clicked ($response = 1$) or not ($response = 0$) , Click-Through-Rate (CTR) Prediction is generally treated as a logistic regression problem. For any set of features, we calculate some value $s$, and perform the logit transformation to yield our CTR prediction:\n",
    "$$CTR = \\frac{1}{1+e^{-s}} $$\n",
    "\n",
    "There are several methods we can use to estimate $s$, each with its own benefits and drawbacks. Typical implementations include a linear model, a degree-2 polynomial mapping, a factorization machine, and a field-aware factorization machine. \n",
    "\n",
    "We will consider an example with the following dataset as we discuss the different methods of estimating $s$:\n",
    "\n",
    "<table>\n",
    "<th>Response</th>\n",
    "<th>Publisher</th>\n",
    "<th>Advertiser</th>\n",
    "<th>Gender</th>\n",
    "<tr><td>1</td><td>Netflix</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>0</td><td>Spotify</td><td>Pepsi</td><td>Male</td></tr>\n",
    "<tr><td>0</td><td>Facebook</td><td>Gatorade</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Spotify</td><td>Coca-cola</td><td>Male</td></tr>\n",
    "<tr><td>1</td><td>Facebook</td><td>Coca-cola</td><td>Female</td></tr>\n",
    "<tr><td>0</td><td>Facebook</td><td>Pepsi</td><td>Female</td></tr>\n",
    "<tr><td>1</td><td>Netflix</td><td>Gatorade</td><td>Female</td></tr>\n",
    "</table>\n",
    "\n",
    "In this dataset, we refer to the categories Publisher, Advertiser, and Gender as \"fields\" and the labels within each field (Netflix, Spotify, Facebook, Pepsi, Gatorade, Coca-cola, Male, Female) as \"features.\"\n",
    "\n",
    "#### Linear Model\n",
    "In a linear model, the algorithm learns a weight for every given feature. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) =\\textbf{w}^T \\textbf{x}  =\\sum_{j \\epsilon C_1}w_jx_j$$\n",
    "where $\\textbf{w}$ is the learned model, $\\textbf{x}$ is the data observation, and $C_1$ is the non-zero elements in $\\textbf{x}$. \n",
    "\n",
    "In our toy example, our model would learn different weights for the different Publishers (Netflix, Spotify, and Pepsi), Advertisers (Pepsi, Gatorade, and Coca-cola), and Genders (Male, Female). The value $s$ would then be calculated for each impression using these different weights. Thus, for each impression we would have have:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s =& w_{Netflix}\\cdot x_{Netflix} + w_{Spotify}\\cdot x_{Spotify} + w_{Facebook}\\cdot x_{Facebook} \\\\\n",
    "&+ w_{Coca-cola}\\cdot x_{Coca-cola} + w_{Pepsi}\\cdot x_{Pepsi} + w_{Gatorade}\\cdot x_{Gatorade} \\\\\n",
    "&+ + w_{Male}\\cdot x_{Male} + w_{Female}\\cdot x_{Female}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For our first impression of the dataset (Netflix, Pepsi, Male) this becomes:\n",
    "$$s = w_{Netflix} + w_{Coca-cola} + w_{Male}$$\n",
    "since $x_j = 1$ for Netflix, Pepsi, and Male while $x_j = 0$ for all other features. \n",
    "\n",
    "\n",
    "This model is simple and efficient, yet it does not allow for interactive effects between features. For example, Coca-cola may have a higher CTR with Netflix than another publisher. A linear model is unable to learn this type of information, as it essentially learns the \"average effect\" of each feature.\n",
    "\n",
    "#### Degree-2 Polynomial Mapping\n",
    "The simplest way to learn the effect of the \"feature conjunction\" described above (in the case where a particular advertiser may have a higher CTR with one publisher compared to others) is to use a degree-2 polynomial mapping. In this model, the algorithm learns an additional weight for each feature pair. The formulation of the model is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} w_{j_1,j_2} \\cdot x_{j_1}x_{j_2}$$\n",
    "where $C_2$ is the pairwise combination of non-zero elements in $\\textbf{x}$.\n",
    "\n",
    "\n",
    "Returning to our example dataset and the impression with the features Netflix, Pepsi, and Male the model would be:\n",
    "$$s = w_{Netflix} + w_{Pepsi} + w_{Male} + w_{Netflix,Pepsi} + w_{Netflix,Male} + w_{Pepsi,Male}$$\n",
    "\n",
    "While this model improves on the linear model by allowing us to account for interactions between features, it does not handle sparse datasets well. Since we have 0 impressions of the advertiser Gatorade with the publisher Spotify, the model prediction will be trivial as no weight was learned for this feature combination. The model is also susceptible to overfitting, as it generates unreliable predictions for feature combinations with a very small number of impressions.\n",
    "\n",
    "#### Factorization Machine\n",
    "Factorization Machines (FM) begin to provide us a solution to the problem of sparse datasets. Here the model learns a latent vector, rather than an explicit weight, for each feature. The model formulation is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1}, \\textbf{w}_{j_2} \\rangle x_{j_1}x_{j_2}$$\n",
    "where $\\textbf{w}_{j_1}$ and $\\textbf{w}_{j_2}$ are two learned latent vectors of length $k$ (some user-defined parameter).\n",
    "\n",
    "Returning to our example of an impression with the features of Netflix, Pepsi, and Male, the FM model would be:\n",
    "$$s = w_{Netflix} \\cdot w_{Pepsi} + w_{Netflix} \\cdot w_{Male} + w_{Pepsi} \\cdot w_{Male}$$\n",
    "where $w_{Netflix}, w_{Pepsi}, w_{Male} \\epsilon {\\rm I\\!R}^k$.\n",
    "\n",
    "This allows the model to learn the latent vectors for each feature based on all the data points for that feature, and these latent vectors can be used to predict the CTR for unobserved feature combinations (such as Spotify and Gatorade, as previously mentioned), something the degree-2 polynomial mapping method did not allow us to do.\n",
    "\n",
    "We notice in our example above, however, that the latent vector $w_{Netflix}$ is used twice: once to calculate the latent effect of the Publisher Netflix with the Advertiser Pepsi ($w_{Netflix} \\cdot w_{Pepsi}$), and once to calculate the latent effect of the Publisher Netflix with the Gender Male ($w_{Netflix} \\cdot w_{Male}$). Yet the latent effect for publisher with advertiser could be very different from the latent effect for publisher with gender, and as such the Factorization Machine model is too restrictive and unrealistic.\n",
    "\n",
    "\n",
    "#### Field-Aware Factorization Machine\n",
    "Field-Aware Factorization Machines (FFM) provide a solution to this problem by introducing the flexibility to learn multiple latent vectors for each feature. In the context of the above example, the FFM model will learn two separate latent vectors for Netflix: $w_{Netflix, A}$, which will be used for the calculation of $P \\times A$ (to learn the latent effect of Netflix with a given advertiser), and $w_{Netflix, G}$. $w_{Netflix, A}$, for the calculation of $P \\times G$ (to learn the latent effect of Netflix with a given gender).\n",
    "\n",
    "Specifically, the FFM model formulation with the (Netflix, Pepsi, Male) impression gives us:\n",
    "\n",
    "$$s = w_{Netflix, A} \\cdot w_{Pepsi, P} + w_{Netflix, G} \\cdot w_{Male, P} + w_{Pepsi, G} \\cdot w_{Male, A}$$\n",
    "\n",
    "Now, the latent effect of (Netflix, Pepsi), is learned by using the latent vector $w_{Netflix, A}$, since Pepsi belongs to the advertiser field (A). The latent effect of (Netflix, Male), by contrast, is learned using a different latent vector, $w_{Netflix, G}$, since Male belongs to the gender field (G). In this way, FFM splits the latent factors for $P \\times A$ and $P \\times G$, something the traditional Factorization Machine is unable to do. \n",
    "\n",
    "The full model formulation for a Field-Aware Factorization Machine is:\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$\n",
    "\n",
    "where $f_1$ and $f_2$ are the fields of $j_1$ and $j_2$, respectively. \n",
    "#### Optimization\n",
    "\n",
    "As part of a logistic regression problem, the goal is to find the set of parameters that minimize the log-loss function, defined by:\n",
    "\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i}) + (1-y_i) \\cdot log_e(1-\\hat{y_i})] $$\n",
    "\n",
    "\n",
    "where $n$ is the number of impressions, $y_i$ is the true CTR of impression $i$, and $\\hat{y_i}$ is the predicted CTR of impression $i$, $\\frac{1}{1+e^{-s_i}}$.\n",
    "\n",
    "For a given impression $x_i$, we have two cases:\n",
    "* If $y-i = 1$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss &= -log_e(\\hat{y_i}) \\\\\n",
    "&= -log_e \\left( \\frac{1}{1+e^{-s}} \\right) \\\\\n",
    "&= log_e (1+e^{-s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* If $y_i = 0$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "loss &= -log_e(1-\\hat{y_i}) \\\\\n",
    "&= -log_e \\left( 1 - \\frac{1}{1+e^{-s}} \\right)\\\\\n",
    "&= -log_e \\left( \\frac{1 + e^{-s} - 1}{1+e^{-s}} \\right) \\\\\n",
    "&= -log_e \\left( \\frac{e^-s}{1+e^{-s}} \\right) \\\\\n",
    "&= -log_e \\left( \\frac{1}{1+e^{s}} \\right) \\\\\n",
    "&= log_e(1+e^{s})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "As such, the loss for impression $x_i$ can be written as:\n",
    "$$log(1+exp(-\\bar{y_i}\\cdot s_i)$$\n",
    "where \n",
    "$$\\bar{y_i} = \n",
    "\\begin{cases}\n",
    "    1,\\ if\\ y_i=1\\\\\n",
    "    -1,\\ if\\ y_i=0\\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Therefore, we can rewrite the log-loss function as:\n",
    "$$ Loss = \\frac{1}{n} \\sum_{i=1}^n log(1+exp(-\\bar{y_i}\\cdot s_i))$$\n",
    "where \n",
    "$\\bar{y_i} = \n",
    "\\begin{cases}\n",
    "    1,\\ if\\ y_i=1\\\\\n",
    "    -1,\\ if\\ y_i=0\\\\\n",
    "\\end{cases}$\n",
    "\n",
    "\n",
    "and $s = \\phi(\\textbf{w},\\textbf{x}_i) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$\n",
    "\n",
    "\n",
    "Introducing regularization, the optimization problem we have is:\n",
    "\n",
    "$$ \\min_{\\textbf{w}} \\sum_{i=1}^m \\left( log(1+exp(-\\bar{y_i}\\phi(\\textbf{w},\\textbf{x}_i)) + \\frac{\\lambda}{2}\\|\\textbf{w}\\|^2 \\right)$$\n",
    "\n",
    "$m$ is the number of impressions, and  $\\lambda$ is our regularization parameter.\n",
    "\n",
    "We solve this optimization problem using gradient descent methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CTR(s):\n",
    "    return np.divide(1,1+exp(-s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Toy example: FFM in action\n",
    "\n",
    "We will now walk through an entire iteration of the FFM algorithm to learn a CTR prediction model.\n",
    "For our work here, we will utilize a hashed representation of our dataset. In this representation, each field is represented by the first integer in the hash, and the feature within that field is represented by the second integer. The Publisher field is represented by the integer 1, the Advertiser field by the integer 2, and the Gender field by the integer 3. Within the Publisher field, Netflix is represented by the integer 1, Spotify by 2, and Facebook by 3. Within the Advertiser field, Pepsi is represented by the integer 1, Gatorade by 2, and Coca-cola by 3. Within the Gender field, Male is represented by 1 and Female by 2. Using this hash representation, our first impression of (Netflix, Pepsi, Male) has the features 1:1, 2:1, 3:1.\n",
    "The full hashed representation of our dataset is shown below:\n",
    "\n",
    "<table>\n",
    "<th>Impression</th>\n",
    "<th>Response</th>\n",
    "<th>Features</th>\n",
    "<tr><td>1</td><td>1</td><td>1:1, 2:1, 3:1</td></tr>\n",
    "<tr><td>2</td><td>1</td><td>1:2, 2:1, 3:1</td></tr>\n",
    "<tr><td>3</td><td>0</td><td>1:3, 2:2, 3:2</td></tr>\n",
    "<tr><td>4</td><td>0</td><td>1:2, 2:3, 3:1</td></tr>\n",
    "<tr><td>5</td><td>1</td><td>1:3, 2:3, 3:2</td></tr>\n",
    "<tr><td>6</td><td>0</td><td>1:3, 2:1, 3:2</td></tr>\n",
    "<tr><td>7</td><td>1</td><td>1:1, 2:2, 3:2</td></tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/toy_set.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/toy_set.txt\n",
    "1\t1\t1\t1\t1\n",
    "2\t1\t2\t1\t1\n",
    "3\t0\t3\t2\t2\n",
    "4\t0\t2\t3\t1\n",
    "5\t1\t3\t3\t2\n",
    "6\t0\t3\t1\t2\n",
    "7\t1\t1\t2\t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "toyRDD = sc.textFile('data/toy_set.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1)__ We define k (the length of our latent vectors) to be 3.\n",
    "\n",
    "__2)__ We randomly initialize our latent vectors $\\textbf{w}$. For the Publisher field, each feature will have two different latent vectors (represented by columns below)--one corresponding to Advertiser (A), and one corresponding to Gender (G):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.9, -0.2, -0.2],\n",
       "       [-0.6,  0. , -0.3],\n",
       "       [-0.1, -0.5, -1.2],\n",
       "       [-1.1,  0.2,  1.2],\n",
       "       [ 1.4, -0.7,  0.7],\n",
       "       [ 0. ,  0.8,  0.1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W_P = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "#W_P\n",
    "W_P =np.array([0.9, -0.2, -0.2, -0.6, 0, -0.3, -0.1, -0.5, -1.2, -1.1, 0.2, 1.2, 1.4,-0.7, 0.7, 0, 0.8, 0.1]).reshape(6,3)\n",
    "W_P "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Netflix,A}$</th>\n",
    "<th>$\\textbf{w}_{Netflix,G}$</th>\n",
    "<th>$\\textbf{w}_{Spotify,A}$</th>\n",
    "<th>$\\textbf{w}_{Spotify,G}$</th>\n",
    "<th>$\\textbf{w}_{Facebook,A}$</th>\n",
    "<th>$\\textbf{w}_{Facebook,G}$</th>\n",
    "<tr><td>0.9</td><td>-0.6</td><td>-0.1</td><td>-1.1</td><td>1.4</td><td>0</td></tr>\n",
    "<tr><td>-0.2</td><td>0</td><td>-0.5</td><td>0.2</td><td>-0.7</td><td>0.8</td></tr>\n",
    "<tr><td>-0.2</td><td>-0.3</td><td>-1.2</td><td>1.2</td><td>0.7</td><td>0.1</td></tr>\n",
    "<table>\n",
    "The Advertiser field will similarly have two different latent vectors for each feature--one for Publisher (P), and one for Gender (G):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.3, -0.1],\n",
       "       [-0.4, -1. ,  0. ],\n",
       "       [-0.2, -1. , -1.7],\n",
       "       [ 0.7, -0.3, -1.1],\n",
       "       [ 0.1,  1.3, -0.2],\n",
       "       [ 0.5,  0.2,  0.5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W_A = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "#W_A\n",
    "W_A =np.array([1, -0.3, -0.1, -0.4, -1, 0, -0.2, -1, -1.7, 0.7, -0.3, -1.1, 0.1, 1.3, -0.2, 0.5, 0.2, 0.5]).reshape(6,3)\n",
    "W_A "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Pepsi,P}$</th>\n",
    "<th>$\\textbf{w}_{Pepsi,G}$</th>\n",
    "<th>$\\textbf{w}_{Gatorade,P}$</th>\n",
    "<th>$\\textbf{w}_{Gatorade,G}$</th>\n",
    "<th>$\\textbf{w}_{Coca-cola,P}$</th>\n",
    "<th>$\\textbf{w}_{Coca-cola,G}$</th>\n",
    "<tr><td>1</td><td>-0.4</td><td>-0.2</td><td>0.7</td><td>0.1</td><td>0.5</td></tr>\n",
    "<tr><td>-0.3</td><td>-1</td><td>-1</td><td>-0.3</td><td>-1.3</td><td>0.2</td></tr>\n",
    "<tr><td>-0.1</td><td>0</td><td>-1.7</td><td>-1.1</td><td>-0.2</td><td>0.5</td></tr>\n",
    "<table>\n",
    "\n",
    "The Gender field will also have two different latent vectors per feature (one for Publisher (P), one for Advertiser (A)): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.4,  0.8,  1.5],\n",
       "       [-0.9,  1.3, -1. ],\n",
       "       [ 0.8, -1.5,  0.3],\n",
       "       [ 0.4,  0.7,  0.5]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#W_G = np.random.uniform(-1.7,1.7, size=(4,3)).round(1)\n",
    "#W_G\n",
    "W_G =np.array([-0.4, 0.8, 1.5, -0.9, 1.3, -1, 0.8, -1.5, 0.3, 0.4, 0.7, 0.5]).reshape(4,3)\n",
    "W_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "<th>$\\textbf{w}_{Male,P}$</th>\n",
    "<th>$\\textbf{w}_{Male,A}$</th>\n",
    "<th>$\\textbf{w}_{Female,P}$</th>\n",
    "<th>$\\textbf{w}_{Female,A}$</th>\n",
    "<tr><td>-0.4</td><td>-0.9</td><td>0.8</td><td>0.4</td></tr>\n",
    "<tr><td>0.8</td><td>1.3</td><td>-1.5</td><td>0.7</td></tr>\n",
    "<tr><td>1.5</td><td>-1</td><td>0.3</td><td>0.5</td></tr>\n",
    "<table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3)__ We calculate $s$ for each impression using these weight vectors.\n",
    "\n",
    "For each impression, we would use the formula\n",
    "$$s = \\phi(\\textbf{w},\\textbf{x}) = \\sum_{j_1, j_2 \\epsilon C_2} \\langle \\textbf{w}_{j_1, f_2}, \\textbf{w}_{j_2, f_1} \\rangle x_{j_1}x_{j_2}$$    \n",
    "to generate our value of $s$. Fully expanded, this would be:\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "s =& \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Netflix}x_{Pepsi} + \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Netflix}x_{Coca-cola} + \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Netflix}x_{Gatorade} \\\\\n",
    "&+ \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Netflix}x_{Male} + \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Netflix}x_{Female} + \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Spotify}x_{Pepsi} \\\\\n",
    "&+ \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Spotify}x_{Coca-cola} + \\langle\\textbf{w}_{Spotify,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Spotify}x_{Gatorade} + \\langle\\textbf{w}_{Spotify,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Spotify}x_{Male} \\\\\n",
    "&+ \\langle\\textbf{w}_{Spotify,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Spotify}x_{Female} + \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle x_{Facebook}x_{Pepsi} + \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Coca-cola,P}\\rangle x_{Facebook}x_{Coca-cola} \\\\\n",
    "&+ \\langle\\textbf{w}_{Facebook,A} \\cdot \\textbf{w}_{Gatorade,P}\\rangle x_{Facebook}x_{Gatorade} + \\langle\\textbf{w}_{Facebook,G} \\cdot \\textbf{w}_{Male,P}\\rangle x_{Facebook}x_{Male} + \\langle\\textbf{w}_{Facebook,G} \\cdot \\textbf{w}_{Female,P}\\rangle x_{Facebook}x_{Female} \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For each impression, we are only concerned with the terms containing the features represented in the impression, since these $x$ values are equal to one, while all other terms become zero.\n",
    "    \n",
    "Our first impression (Netflix, Pepsi, Male) would therefore give us:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "s &= \\langle\\textbf{w}_{Netflix,A} \\cdot \\textbf{w}_{Pepsi,P}\\rangle + \\langle\\textbf{w}_{Netflix,G} \\cdot \\textbf{w}_{Male,P}\\rangle + \\langle\\textbf{w}_{Pepsi,G} \\cdot \\textbf{w}_{Male,A}\\rangle\\\\\n",
    "&= (0.9*1 + -0.2*-0.3 + -0.2*-0.1) + (-0.6*-0.4 + 0*0.8 + -0.3*1.5) + (-0.4*-0.9 + -1*1.3 + 0*-1)  \\\\\n",
    "&= 0.98 + -0.21 + -0.94 \\\\\n",
    "&= -0.17\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Plugging this value into the logit transformation, we get a predicted CTR of:\n",
    "$$\\frac{1}{1+e^{-s}} = \\frac{1}{1+e^{0.17}} \\approx 0.46$$\n",
    "    \n",
    "Since this value is closer to 0 than 1, we would ultimately predict a non-click (incorrectly). However, this predicted value is vital for our log-loss calculation, which we will see momentarily.\n",
    "\n",
    "Below we calculate the value of $s$ and predicted CTR for each impression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_p = sc.broadcast(W_P)\n",
    "W_a = sc.broadcast(W_A)\n",
    "W_g = sc.broadcast(W_G)\n",
    "\n",
    "def predict(x):\n",
    "    imp = x[0]\n",
    "    res = x[2]\n",
    "    pub = x[4]\n",
    "    adv = x[6]\n",
    "    gen = x[8]\n",
    "    #define appropriate latent vectors to use in calculations\n",
    "    if pub == '1':\n",
    "        w_pa = W_p.value[0]\n",
    "        w_pg = W_p.value[1]\n",
    "    elif pub == '2':\n",
    "        w_pa = W_p.value[2]\n",
    "        w_pg = W_p.value[3]\n",
    "    else:\n",
    "        w_pa = W_p.value[4]\n",
    "        w_pg = W_p.value[5]\n",
    "    if adv == '1':\n",
    "        w_ap = W_a.value[0]\n",
    "        w_ag = W_a.value[1]\n",
    "    elif adv == '2':\n",
    "        w_ap = W_a.value[2]\n",
    "        w_ag = W_a.value[3]\n",
    "    else:\n",
    "        w_ap = W_a.value[4]\n",
    "        w_ag = W_a.value[5]\n",
    "    if gen == '1':\n",
    "        w_gp = W_g.value[0]\n",
    "        w_ga = W_g.value[1]\n",
    "    else:\n",
    "        w_gp = W_g.value[2]\n",
    "        w_ga = W_g.value[3]\n",
    "    #calculate s using latent vectors\n",
    "    s = (np.dot(w_pa, w_ap) + np.dot(w_pg, w_gp) + np.dot(w_ag, w_ga)).round(2)\n",
    "    #calculate CTR\n",
    "    CTR = np.divide(1,1+np.exp(-s)).round(2)\n",
    "    yield imp,res, pub, adv, gen, s, CTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '1', '1', '1', '1', -0.17, 0.46),\n",
       " ('2', '1', '2', '1', '1', 1.63, 0.84),\n",
       " ('3', '0', '3', '2', '2', -2.42, 0.08),\n",
       " ('4', '0', '2', '3', '1', 1.29, 0.78),\n",
       " ('5', '1', '3', '3', '2', -1.49, 0.18),\n",
       " ('6', '0', '3', '1', '2', -0.49, 0.38),\n",
       " ('7', '1', '1', '2', '2', -0.69, 0.33)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultRDD = toyRDD.flatMap(predict).cache()\n",
    "resultRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarized in a table, we have:\n",
    "\n",
    "<table>\n",
    "<th>Impression</th>\n",
    "<th>$s$</th>\n",
    "<th>Predicted CTR</th>\n",
    "<th>True Response</th>\n",
    "<th>Correct?</th>\n",
    "<tr><td>(Netflix, Pepsi, Male)</td><td>-0.17</td><td>$\\approx 0.46$</td><td>1</td><td>No</td></tr>\n",
    "<tr><td>(Spotify, Pepsi, Male)</td><td>1.63</td><td>$\\approx 0.84$</td><td>1</td><td>Yes</td></tr>\n",
    "<tr><td>(Facebook, Gatoriade, Female)</td><td>-2.42</td><td>$\\approx 0.08$</td><td>0</td><td>Yes</td></tr>\n",
    "<tr><td>(Spotify, Coca-cola, Male)</td><td>1.29</td><td>$\\approx 0.78$</td><td>0</td><td>No</td></tr>\n",
    "<tr><td>(Facebook, Coca-cola, Female)</td><td>-1.49</td><td>$\\approx 0.18$</td><td>1</td><td>No</td></tr>\n",
    "<tr><td>(Facebook, Pepsi, Female)</td><td>-0.49</td><td>$\\approx 0.38$</td><td>0</td><td>Yes</td></tr>\n",
    "<tr><td>(Netflix, Gatorade, Female)</td><td>-0.69</td><td>$\\approx 0.33$</td><td>1</td><td>No</td></tr>\n",
    "<table>\n",
    "\n",
    "    \n",
    "__4)__ We can now calculate the log-loss of the model. While this is not necessary in order to find the optimum latent vectors for our model, we show the calculation here using the equation:\n",
    "$$ LogLoss = - \\frac{1}{n} \\sum_{i=1}^n [y_i \\cdot log_e(\\hat{y_i}) + (1-y_i) \\cdot log_e(1-\\hat{y_i})] $$\n",
    "\n",
    "Simply put, if our True Response ($y_i$) for an impression is 0, we add the log-value of `1-Predicted CTR` to our sum; if our True Response ($y_i$) for an impression is 1, we add the log-value of `CTR`.    \n",
    "\n",
    "Using our values from our table above, we have:\n",
    "\n",
    "$$ \\begin{aligned}\n",
    "LogLoss =& - \\frac{1}{7}[log_e(0.46) + log_e(0.84) + log_e(0.92) + log_e(0.22) + log_e(0.18) + log_e(0.62)+ log_e(0.33)]\\\\\n",
    "=& 0.8356\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Our hand calculation is confirmed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8356983388241626]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def log_loss(imp):\n",
    "    #helper function to calculate log(CTR) or log(1-CTR) depending on true response\n",
    "    resp = imp[1]\n",
    "    pred = imp[6]\n",
    "    if resp == '0':\n",
    "        yield 1, (np.log(1-pred), 1)\n",
    "    else:\n",
    "        yield 1, (np.log(pred), 1)\n",
    "\n",
    "def sum_loss(x, y):\n",
    "    #function to add log-losses, counts\n",
    "    return (x[0]+y[0],x[1]+y[1])\n",
    "\n",
    "loss = resultRDD.flatMap(log_loss).reduceByKey(lambda x, y: sum_loss(x, y))\\\n",
    "    .map(lambda x: -np.divide(x[1][0], x[1][1])).collect()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__5)__ We then would optimize our weights using gradient descent. In their implementation outlined in their paper, Juan, Zhuang, Chin, & Lin update the weight matrix of latent vectors using stochastic gradient descent. We will replicate this method with our toy sample here.\n",
    "\n",
    "At each step, we choose a data point and update $\\textbf{w}_{j_1,f_2}$ and $\\textbf{w}_{j_2, f_1}$ for each non-zero pair of features $f_1 \\epsilon j_1$ and $f_2 \\epsilon j_2$. We first calculate the subgradients: \n",
    "$$\\textbf{g}_{j_1,f_2} = \\nabla \\textbf{w}_{j_1,f_2} f(\\textbf{w}) = \\lambda \\cdot \\textbf{w}_{j_1,f_2} + \\kappa \\cdot \\textbf{w}_{j_2,f_1}x_{j_1}x_{j_2} $$\n",
    "$$\\textbf{g}_{j_2,f_1} = \\nabla \\textbf{w}_{j_2,f_1} f(\\textbf{w}) = \\lambda \\cdot \\textbf{w}_{j_2,f_1} + \\kappa \\cdot \\textbf{w}_{j_1,f_2}x_{j_1}x_{j_2} $$\n",
    "where $$\\kappa = \\frac{-\\bar{y}}{1+exp(\\bar{y}\\phi(\\textbf{wx}))}$$\n",
    "\n",
    "with $\\bar{y}=1$ for a positive click response and $\\bar{y}=-1$ for a negative click response, as before.\n",
    "\n",
    "For simplicity here, we ignore the regularization term and consider the first datapoint- the impression of (Netflix, Pepsi, Male). We calculate $\\kappa = -0.54$, and calculate the first pair of subgradients:\n",
    "    $$\\textbf{g}_{Pepsi, P} = \\kappa\\textbf{w}_{Netflix, A}$$ \n",
    "    $$\\textbf{g}_{Netflix, A} = \\kappa\\textbf{w}_{Pepsi, P}$$\n",
    "    \n",
    "(Note: below we only continue to demonstrate how to update the latent vector $\\textbf{w}_{Pepsi,P}$. Updating all other latent vectors is analagous.)\n",
    "    \n",
    "Plugging in our known and calculated values, we have:\n",
    "    \n",
    "$$\\textbf{g}_{Pepsi, P}= -0.54 \\times \\begin{bmatrix}0.9\\\\-0.2\\\\-0.2 \\end{bmatrix} = \\begin{bmatrix}-0.488\\\\0.108\\\\0.108 \\end{bmatrix} $$\n",
    "\n",
    "Next, for each coordinate $d = {1,...,k}$, the sum of squared gradient is accumulated:\n",
    "$$ (G_{j_1,f_2})_d \\leftarrow (G_{j_1,f_2})_d + (\\textbf{g}_{j_1,f_2})_d^2$$ where $(G_{j_1,f_2})_d$ are initialized as all 1s. Thus, we have:\n",
    "$$ (G_{Pepsi,P}) = \\begin{bmatrix}1\\\\1\\\\1\\end{bmatrix} + \\begin{bmatrix}(-0.488)^2\\\\(0.108)^2\\\\(0.108)^2\\end{bmatrix} = \\begin{bmatrix}1.2383\\\\1.0117\\\\1.0117\\end{bmatrix}  $$\n",
    "    \n",
    "Finally, we update our latent vector $\\textbf{w}_{j_1,f_2}$ by:\n",
    "    $$(\\textbf{w}_{j_1,f_2})_d \\leftarrow (\\textbf{w}_{j_1,f_2})_d - \\frac{\\eta}{\\sqrt{(G_{j_1,f_2})_d}} \\textbf{g}_{j_1,f_2} $$\n",
    "where $\\eta$ is a specified learning rate. For this example, we use $\\eta = 0.1$, and get the following update to our latent vector, $\\textbf{w}_{Pepsi,P}$:\n",
    "\n",
    "$$\\textbf{w}_{Pepsi,P} = \\begin{bmatrix}1\\\\-0.3\\\\-0.1 \\end{bmatrix} - \\begin{bmatrix}\\frac{0.1}{\\sqrt{1.2383}} \\cdot -0.488\\\\\\frac{0.1}{\\sqrt{1.0117}} \\cdot 0.108\\\\\\frac{0.1}{\\sqrt{1.0117}} \\cdot 0.108 \\end{bmatrix} = \\begin{bmatrix}1.04\\\\-0.31\\\\-0.11 \\end{bmatrix}$$\n",
    "\n",
    "Using the same impression, we similarly update the latent vectors $\\textbf{w}_{Netflix,A}$, $\\textbf{w}_{Netflix,G}$, $\\textbf{w}_{Male,P}$, $\\textbf{w}_{Male,A}$, and $\\textbf{w}_{Pepsi,G}$.\n",
    "These newly updated latent vectors are calculated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1',\n",
       "  '1',\n",
       "  '1',\n",
       "  array([ 0.94767801, -0.2160607 , -0.20541602]),\n",
       "  array([-0.62120264,  0.03980592, -0.2368895 ]),\n",
       "  array([ 1.04386801, -0.31078469, -0.11078469]),\n",
       "  array([-0.44386801, -0.94237342, -0.04767801]),\n",
       "  array([-0.43094634,  0.8       ,  1.4839393 ]),\n",
       "  array([-0.92120264,  1.25232199, -1.        ]))]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def kappa(x):\n",
    "    #function to calculate kappa value based on true response and calculated s value\n",
    "    if x[1]=='1':\n",
    "        y=1\n",
    "    else:\n",
    "        y=-1\n",
    "    return -np.divide(y,1 + np.exp(y*x[5])) #kappa calculation\n",
    "\n",
    "def gradient(x):\n",
    "    #function to calculate gradient update for latent vectors of an impression\n",
    "    #need response variables to be +1 or -1\n",
    "    k = kappa(x)\n",
    "    pub = x[2]\n",
    "    adv = x[3]\n",
    "    gen = x[4]\n",
    "    #extract appropriate latent vectors to update, given the impression\n",
    "    if pub == '1':\n",
    "        w_pa = W_p.value[0]\n",
    "        w_pg = W_p.value[1]\n",
    "    elif pub == '2':\n",
    "        w_pa = W_p.value[2]\n",
    "        w_pg = W_p.value[3]\n",
    "    else:\n",
    "        w_pa = W_p.value[4]\n",
    "        w_pg = W_p.value[5]\n",
    "    if adv == '1':\n",
    "        w_ap = W_a.value[0]\n",
    "        w_ag = W_a.value[1]\n",
    "    elif adv == '2':\n",
    "        w_ap = W_a.value[2]\n",
    "        w_ag = W_a.value[3]\n",
    "    else:\n",
    "        w_ap = W_a.value[4]\n",
    "        w_ag = W_a.value[5]\n",
    "    if gen == '1':\n",
    "        w_gp = W_g.value[0]\n",
    "        w_ga = W_g.value[1]\n",
    "    else:\n",
    "        w_gp = W_g.value[2]\n",
    "        w_ga = W_g.value[3]\n",
    "    #calculate subgradients\n",
    "    g_pa = k*w_ap\n",
    "    g_pg = k*w_gp\n",
    "    g_ap = k*w_pa\n",
    "    g_ag = k*w_ga\n",
    "    g_gp = k*w_pg\n",
    "    g_ga = k*w_ag\n",
    "    #accumulate sum of squared gradients, add to G vector (initialized as ones)\n",
    "    G_pa = np.ones(3) + g_pa**2\n",
    "    G_pg = np.ones(3) + g_pg**2\n",
    "    G_ap = np.ones(3) + g_ap**2\n",
    "    G_ag = np.ones(3) + g_ag**2\n",
    "    G_gp = np.ones(3) + g_gp**2\n",
    "    G_ga = np.ones(3) + g_ga**2\n",
    "    \n",
    "    #update latent vectors\n",
    "    w_pa = w_pa - np.divide(0.1,np.sqrt(G_pa))*g_pa\n",
    "    w_pg = w_pg - np.divide(0.1,np.sqrt(G_pg))*g_pg\n",
    "    w_ap = w_ap - np.divide(0.1,np.sqrt(G_ap))*g_ap\n",
    "    w_ag = w_ag - np.divide(0.1,np.sqrt(G_ag))*g_ag\n",
    "    w_gp = w_gp - np.divide(0.1,np.sqrt(G_gp))*g_gp\n",
    "    w_ga = w_ga - np.divide(0.1,np.sqrt(G_ga))*g_ga\n",
    "    #yield features (to know which vectors to update in weight matrix) and updated latent vectors\n",
    "    yield  pub, adv, gen, w_pa, w_pg, w_ap, w_ag, w_gp, w_ga\n",
    "    \n",
    "# demonstrate, specifying first impression to check hand calculations\n",
    "update = sc.parallelize(resultRDD.takeOrdered(1)).flatMap(gradient).collect()\n",
    "update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the third vector here, representing the updated `PepsiP` latent vector,  matches our hand calculation (which rounded to the second decimal place). We must also pass along the features in order to ensure we update the correct latent vectors in the weight matrix. In the case of the first impression, we emit '1', '1', '1' for the features, so we know to update NetflixA (`W_P[0]`), Netflix G (`W_P[1]`), PepsiP(`W_A[0]`), PepsiG (`W_A[1]`), MaleP(`W_G[0]`), and MaleA (`W_G[1]`) in the weight matrix:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateWeights(W_P, W_A, W_G, update):\n",
    "# Function that takes current weight matrices, and updated vectors emitted after SGD\n",
    "# and updates Weight Matrices\n",
    "    #take first feature (Publisher), and update W_P latent vectors accordingly\n",
    "    if update[0][0] == '1':\n",
    "        W_P[0] = update[0][3]\n",
    "        W_P[1] = update[0][4]\n",
    "    elif update[0][0] == '2':\n",
    "        W_P[2] = update[0][3]\n",
    "        W_P[3] = update[0][4]\n",
    "    else:\n",
    "        W_P[4] = update[0][3]\n",
    "        W_P[5] = update[0][4]\n",
    "    #take second feature (Advertiser), and update W_A latent vectors accordingly\n",
    "    if update[0][1] == '1':\n",
    "        W_A[0] = update[0][5]\n",
    "        W_A[1] = update[0][6]\n",
    "    elif update[0][1] == '2':\n",
    "        W_A[2] = update[0][5]\n",
    "        W_A[3] = update[0][6]\n",
    "    else:\n",
    "        W_A[4] = update[0][5]\n",
    "        W_A[5] = update[0][6]\n",
    "    #take third feature (Gender), and update W_G latent vectors accordingly\n",
    "    if update[0][2] == '1':\n",
    "        W_G[0] = update[0][7]\n",
    "        W_G[1] = update[0][8]\n",
    "    else:\n",
    "        W_G[2] = update[0][7]\n",
    "        W_G[3] = update[0][8]\n",
    "    return W_P, W_A, W_G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we print the updated matrices of latent vectors that would be broadcast back out for the next iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.94767801, -0.2160607 , -0.20541602],\n",
       "        [-0.62120264,  0.03980592, -0.2368895 ],\n",
       "        [-0.1       , -0.5       , -1.2       ],\n",
       "        [-1.1       ,  0.2       ,  1.2       ],\n",
       "        [ 1.4       , -0.7       ,  0.7       ],\n",
       "        [ 0.        ,  0.8       ,  0.1       ]]),\n",
       " array([[ 1.04386801, -0.31078469, -0.11078469],\n",
       "        [-0.44386801, -0.94237342, -0.04767801],\n",
       "        [-0.2       , -1.        , -1.7       ],\n",
       "        [ 0.7       , -0.3       , -1.1       ],\n",
       "        [ 0.1       ,  1.3       , -0.2       ],\n",
       "        [ 0.5       ,  0.2       ,  0.5       ]]),\n",
       " array([[-0.43094634,  0.8       ,  1.4839393 ],\n",
       "        [-0.92120264,  1.25232199, -1.        ],\n",
       "        [ 0.8       , -1.5       ,  0.3       ],\n",
       "        [ 0.4       ,  0.7       ,  0.5       ]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updateWeights(W_P, W_A, W_G, update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__6)__ We repeat 3-5 for a specified number of iterations or until some convergence criteria is met for the latent vectors. In each iteration we would use a different impression to update the latent vectors. Therefore, our code from above must be slightly modified (to choose an impression at random, and not explicitly the first impression. Below is the code we would run for 10 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Loss: 0.6674505942985505\n",
      "Iteration: 2\n",
      "Loss: 0.6494112620568574\n",
      "Iteration: 3\n",
      "Loss: 0.655285193107078\n",
      "Iteration: 4\n",
      "Loss: 0.6289630447180298\n",
      "Iteration: 5\n",
      "Loss: 0.6460073554561067\n",
      "Iteration: 6\n",
      "Loss: 0.6310762578752105\n",
      "Iteration: 7\n",
      "Loss: 0.6310762578752105\n",
      "Iteration: 8\n",
      "Loss: 0.6269998104577843\n",
      "Iteration: 9\n",
      "Loss: 0.6314429931312981\n",
      "Iteration: 10\n",
      "Loss: 0.6278931538743446\n",
      "Final Weight Matrices:\n",
      "W_P:\n",
      " [[ 0.43168523  0.74967893  0.34789988]\n",
      " [ 0.7024423   0.99953272  0.52493794]\n",
      " [-0.9         1.4        -1.5       ]\n",
      " [-1.6        -0.4        -1.4       ]\n",
      " [-0.04794223  1.54197082 -1.46481178]\n",
      " [-0.4055017   0.06581357  1.12199341]] \n",
      "W_A:\n",
      " [[ 0.40556162 -0.44751152  0.65177044]\n",
      " [ 0.89036241 -1.50447862  0.95611675]\n",
      " [ 1.45286369 -0.16237191  0.25864327]\n",
      " [ 1.27069659 -0.86144194  1.32753492]\n",
      " [-0.3         0.4         1.6       ]\n",
      " [-0.6        -0.7        -0.7       ]] \n",
      "W_G:\n",
      " [[-1.29278909 -0.38949718  1.60583298]\n",
      " [ 0.30907984 -1.41524401  1.00969589]\n",
      " [ 0.17304778 -0.42320007 -1.10260394]\n",
      " [ 1.08750227 -0.97697814 -0.2830387 ]]\n"
     ]
    }
   ],
   "source": [
    "W_P = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "W_A = np.random.uniform(-1.7,1.7, size=(6,3)).round(1)\n",
    "W_G = np.random.uniform(-1.7,1.7, size=(4,3)).round(1)\n",
    "for i in range(10):\n",
    "    print('Iteration:', i+1)\n",
    "    W_p = sc.broadcast(W_P)\n",
    "    W_a = sc.broadcast(W_A)\n",
    "    W_g = sc.broadcast(W_G)\n",
    "    resultRDD = toyRDD.flatMap(predict).cache()\n",
    "    loss = resultRDD.flatMap(log_loss).reduceByKey(lambda x, y: sum_loss(x, y))\\\n",
    "        .map(lambda x: -np.divide(x[1][0], x[1][1])).collect()\n",
    "    print('Loss:', loss[0])\n",
    "    update = sc.parallelize(resultRDD.takeSample(True,1)).flatMap(gradient).collect()\n",
    "    updateWeights(W_P, W_A, W_G, update)\n",
    "    \n",
    "print('Final Weight Matrices:')\n",
    "print('W_P:\\n', W_P,'\\nW_A:\\n', W_A,'\\nW_G:\\n', W_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that stochastic gradient descent does not always improve the loss from iteration to iteration, as only a single impression is used to update after each iteration. For this and other reasons, we consider using other methods of gradient descent moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sources:\n",
    "- https://www.csie.ntu.edu.tw/~r01922136/slides/ffm.pdf\n",
    "- https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf\n",
    "- https://www.youtube.com/watch?v=1cRGpDXTJC8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 3__ - EDA & Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 4__ - Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Section 5__ - Course Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
